{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ba4e86",
   "metadata": {},
   "source": [
    "# 예제: CNN으로 성씨 분류하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbb0140",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "962715ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nationality</th>\n",
       "      <th>nationality_index</th>\n",
       "      <th>split</th>\n",
       "      <th>surname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Totah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Abboud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Fakhoury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Srour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Sayegh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10975</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>11</td>\n",
       "      <td>test</td>\n",
       "      <td>Dinh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10976</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>11</td>\n",
       "      <td>test</td>\n",
       "      <td>Phung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10977</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>11</td>\n",
       "      <td>test</td>\n",
       "      <td>Quang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10978</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>11</td>\n",
       "      <td>test</td>\n",
       "      <td>Vu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10979</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>11</td>\n",
       "      <td>test</td>\n",
       "      <td>Ha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10980 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      nationality  nationality_index  split   surname\n",
       "0          Arabic                 15  train     Totah\n",
       "1          Arabic                 15  train    Abboud\n",
       "2          Arabic                 15  train  Fakhoury\n",
       "3          Arabic                 15  train     Srour\n",
       "4          Arabic                 15  train    Sayegh\n",
       "...           ...                ...    ...       ...\n",
       "10975  Vietnamese                 11   test      Dinh\n",
       "10976  Vietnamese                 11   test     Phung\n",
       "10977  Vietnamese                 11   test     Quang\n",
       "10978  Vietnamese                 11   test        Vu\n",
       "10979  Vietnamese                 11   test        Ha\n",
       "\n",
       "[10980 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../data/surnames_with_splits.csv\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b1afc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nationality\n",
       "English       2972\n",
       "Russian       2373\n",
       "Arabic        1603\n",
       "Japanese       775\n",
       "Italian        600\n",
       "German         576\n",
       "Czech          414\n",
       "Spanish        258\n",
       "Dutch          236\n",
       "French         229\n",
       "Chinese        220\n",
       "Irish          183\n",
       "Greek          156\n",
       "Polish         120\n",
       "Korean          77\n",
       "Scottish        75\n",
       "Vietnamese      58\n",
       "Portuguese      55\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['nationality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f706d137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split\n",
       "train    7680\n",
       "test     1660\n",
       "val      1640\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['split'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f63e53",
   "metadata": {},
   "source": [
    "### 데이터 split(train/valid/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2795b1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 다시 train/valid/test로 나눠줌\n",
    "\n",
    "# train 데이터 \n",
    "train_df = df[df.split=='train']\n",
    "train_size = len(train_df)\n",
    "\n",
    "# valid 데이터 \n",
    "val_df = df[df.split=='val']\n",
    "val_size = len(val_df)\n",
    "\n",
    "# test 데이터 \n",
    "test_df = df[df.split=='test']\n",
    "test_size = len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8000220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dict = {'train': (train_df, train_size), \n",
    "              'val': (val_df, val_size), \n",
    "              'test': (test_df, test_size)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf85816",
   "metadata": {},
   "source": [
    "## 2. Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1141fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, token_to_idx=None):\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self.token_to_idx = token_to_idx\n",
    "\n",
    "        self.idx_to_token = {idx: token \n",
    "                              for token, idx in self.token_to_idx.items()}\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \n",
    "#       만약 해당 토큰이 있으면 토큰 idx만 return\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "            \n",
    "#       만약 해당 토큰이 없으면 새로운 토큰 만들어줌\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6934b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "443a9bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab 생성\n",
    "char_vocab = SequenceVocabulary()\n",
    "nationality_vocab = Vocabulary()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    for char in row.surname:\n",
    "        char_vocab.add_token(char)\n",
    "    nationality_vocab.add_token(row.nationality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dfc4c5",
   "metadata": {},
   "source": [
    "### Char Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d51f439e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<MASK>': 0, '<UNK>': 1, '<BEGIN>': 2, '<END>': 3, 'T': 4, 'o': 5, 't': 6, 'a': 7, 'h': 8, 'A': 9}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(char_vocab.token_to_idx.items())[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4659251",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<MASK>', 1: '<UNK>', 2: '<BEGIN>', 3: '<END>', 4: 'T', 5: 'o', 6: 't', 7: 'a', 8: 'h', 9: 'A'}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(char_vocab.idx_to_token.items())[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e172eee",
   "metadata": {},
   "source": [
    "### 국적 Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bb67a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Arabic': 0, 'Chinese': 1, 'Czech': 2, 'Dutch': 3, 'English': 4, 'French': 5, 'German': 6, 'Greek': 7, 'Irish': 8, 'Italian': 9}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(nationality_vocab.token_to_idx.items())[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caf7746c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Arabic', 1: 'Chinese', 2: 'Czech', 3: 'Dutch', 4: 'English', 5: 'French', 6: 'German', 7: 'Greek', 8: 'Irish', 9: 'Italian'}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(nationality_vocab.idx_to_token.items())[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd30e80",
   "metadata": {},
   "source": [
    "## 3. Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0720a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 토큰에 대응하는 인덱스 반환\n",
    "\n",
    "def lookup_token(vocabulary_class,token):\n",
    "    return vocabulary_class.token_to_idx[token]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d025329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 인덱스에 대응하는 토큰 반환\n",
    "\n",
    "def lookup_index(vocabulary_class, index):\n",
    "        if index not in vocabulary_class.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return vocabulary_class.idx_to_token[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e09e6111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰의 수: 88\n"
     ]
    }
   ],
   "source": [
    "vocab_length = len(char_vocab.token_to_idx)\n",
    "print(\"토큰의 수:\", vocab_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11824f7",
   "metadata": {},
   "source": [
    "### 텍스트(surname)에 대한 원 핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e375f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예시\n",
      "(array([ 2, 20,  8,  5, 23,  0,  0,  0,  0,  0]), array([20,  8,  5, 23,  3,  0,  0,  0,  0,  0]))\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# vector_length (int): 인덱스 벡터의 길이를 맞추기 위한 매개변수\n",
    "def vectorize(surname, vector_length=-1):\n",
    "\n",
    "    indices = [char_vocab.begin_seq_index]\n",
    "    indices.extend(lookup_token(char_vocab,token) \n",
    "                   for token in surname)\n",
    "    indices.append(char_vocab.end_seq_index)\n",
    "\n",
    "    if vector_length < 0:\n",
    "        vector_length = len(indices)\n",
    "    \n",
    "    from_vector = np.empty(vector_length, dtype=np.int64)         \n",
    "    from_indices = indices[:-1]\n",
    "    from_vector[:len(from_indices)] = from_indices\n",
    "    from_vector[len(from_indices):] = char_vocab.mask_index\n",
    "\n",
    "    to_vector = np.empty(vector_length, dtype=np.int64)\n",
    "    to_indices = indices[1:]\n",
    "    to_vector[:len(to_indices)] = to_indices\n",
    "    to_vector[len(to_indices):] = char_vocab.mask_index\n",
    "        \n",
    "    return from_vector, to_vector\n",
    "\n",
    "print(\"예시\")\n",
    "example = vectorize(\"Choi\", 10)\n",
    "print(example)\n",
    "print(len(example[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b49a42",
   "metadata": {},
   "source": [
    "### SurnameDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7320e89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, surname_df):\n",
    "        self.surname_df = surname_df\n",
    "        self.max_seq_length = max(map(len, df.surname)) + 2\n",
    "        \n",
    "     \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.surname_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.surname_df.iloc[index]\n",
    "        \n",
    "        from_vector, to_vector = vectorize(row.surname, self.max_seq_length)\n",
    "        \n",
    "        nationality_index = lookup_token(nationality_vocab,row.nationality)\n",
    "\n",
    "        return {'x_data': from_vector, \n",
    "                'y_target': to_vector, \n",
    "                'class_index': nationality_index}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cae239",
   "metadata": {},
   "source": [
    "### 데이터셋 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14c6c27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SurnameDataset at 0x7f8ae0902f10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋을 인스턴스화 해주어야 로더에 넣어줄 수 있다. \n",
    "\n",
    "train_dataset = SurnameDataset(train_df)\n",
    "train_dataset\n",
    "\n",
    "valid_dataset = SurnameDataset(val_df)\n",
    "valid_dataset\n",
    "\n",
    "test_dataset = SurnameDataset(test_df)\n",
    "test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b097a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "19\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.max_seq_length)\n",
    "print(valid_dataset.max_seq_length)\n",
    "print(test_dataset.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9514135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더 설정\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# drop_last=True -> 배치 사이즈보다 over하면 drop\n",
    "\n",
    "Traindataloader = DataLoader(dataset=train_dataset, batch_size=512,\n",
    "                            shuffle=True, drop_last=True)\n",
    "\n",
    "Validdataloader = DataLoader(dataset=valid_dataset, batch_size=512,\n",
    "                            shuffle=True, drop_last=True)\n",
    "\n",
    "Testdataloader = DataLoader(dataset=test_dataset, batch_size=512,\n",
    "                            shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e8a9d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7680 15\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset),len(Traindataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "daefd63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x_data': tensor([[ 2, 45,  7,  ...,  0,  0,  0],\n",
      "        [ 2, 29,  5,  ...,  0,  0,  0],\n",
      "        [ 2, 30, 11,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 2,  4, 43,  ...,  0,  0,  0],\n",
      "        [ 2, 40, 25,  ...,  0,  0,  0],\n",
      "        [ 2, 37,  7,  ...,  0,  0,  0]]), 'y_target': tensor([[45,  7, 25,  ...,  0,  0,  0],\n",
      "        [29,  5, 15,  ...,  0,  0,  0],\n",
      "        [30, 11, 23,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 4, 43,  8,  ...,  0,  0,  0],\n",
      "        [40, 25, 43,  ...,  0,  0,  0],\n",
      "        [37,  7, 21,  ...,  0,  0,  0]]), 'class_index': tensor([14, 14,  4, 14,  4,  4,  4,  6, 14,  4,  4,  6,  0, 14,  6,  4,  0,  4,\n",
      "        14,  4, 14, 14,  0, 14,  0,  3,  4,  7, 14,  5,  0,  4,  4,  0,  5,  4,\n",
      "         4, 10,  6,  4,  6,  0, 14,  4, 14, 14,  4, 14, 14, 15,  4,  0,  4,  0,\n",
      "        10, 14,  4,  4,  4, 14, 14,  0,  9,  9,  4, 14, 10,  0, 14, 16,  4, 16,\n",
      "         4,  4, 14,  4,  1,  4, 14,  4,  7,  0,  4,  0, 10, 14,  0,  4,  4,  4,\n",
      "        14,  1,  4, 14,  0, 14, 14,  4,  0,  9,  4,  6,  1,  4, 14,  0,  4,  4,\n",
      "        14, 14, 14, 10,  4,  4,  4, 14,  0, 14, 14,  4,  0, 14,  5,  0,  4,  0,\n",
      "        14,  9,  6, 14,  0, 14,  9,  9,  0, 10, 14, 10,  0,  4,  4,  9, 10, 10,\n",
      "         0,  4,  2,  4, 16,  0,  7,  8,  9,  6,  6, 14,  7,  4,  3,  4, 10,  4,\n",
      "         4,  4,  0,  3, 14,  9, 14,  0, 16,  0, 14,  4,  0,  4, 14, 10, 14,  4,\n",
      "        14,  4, 10,  9,  0, 14, 12,  4,  9,  0,  6,  8, 14,  4,  7,  9,  4, 14,\n",
      "        16,  0, 14,  9,  4, 10, 11,  0,  4,  4,  0,  4, 14,  0,  4,  4, 14,  4,\n",
      "         6,  6, 14, 16, 14,  0, 14, 14,  4,  0,  4,  6,  4, 17, 14,  4,  4,  0,\n",
      "         4, 13,  5,  3, 14,  9,  0,  5,  4,  0, 13, 12, 14, 10,  7,  4,  5,  3,\n",
      "         5, 10,  9,  4, 14,  4,  4, 14, 14, 10,  7, 10, 14, 14, 14,  4, 17,  0,\n",
      "        10, 14,  4,  0,  0,  4, 14,  4, 14, 14,  4,  7,  2,  0,  3, 10,  0,  0,\n",
      "         4,  4, 14,  4, 14, 10,  0, 14, 17,  1,  4,  4, 14,  0,  9, 14,  4,  0,\n",
      "         4, 14,  4, 10,  4, 14, 14, 10, 10, 14,  4, 14, 14,  6,  4,  0,  4,  4,\n",
      "         4,  0,  9, 10,  4,  2,  4,  6, 10,  4,  7, 14, 16,  6,  4,  4, 14,  5,\n",
      "         4,  6,  8,  9,  0, 14, 16, 15,  9,  4,  0,  2, 14,  9,  0, 16,  3,  4,\n",
      "         4, 14,  6, 14,  6, 14,  4,  4,  9,  4,  3,  0,  6,  8,  4,  6,  0,  9,\n",
      "        10, 14,  9,  0,  0,  4,  4,  2,  6, 14,  9, 14, 10, 14, 14,  2, 14,  0,\n",
      "         0,  0,  4,  6,  4,  3,  3, 11,  4,  6,  9,  1,  4, 14,  6, 14,  0,  2,\n",
      "        14, 14, 14,  4, 16, 14,  0,  8,  6,  2,  2, 14,  6, 16,  0,  9,  4, 13,\n",
      "        15,  4, 14, 14,  2,  1,  0, 10, 14,  8,  7, 14,  4, 14,  1, 14,  0,  4,\n",
      "        10,  6, 14, 10,  0,  6,  0,  4,  0, 10,  4, 14,  0, 12,  0,  4,  0,  3,\n",
      "         4,  6,  4,  1,  4,  1, 16, 11, 10, 14,  4,  4,  0, 14, 14,  4,  4,  4,\n",
      "         4, 10, 10,  4,  0, 14, 12,  4,  9,  0,  7, 14,  4, 12,  0,  4,  4,  6,\n",
      "         6, 14, 10, 14,  0, 14, 14,  8])}\n"
     ]
    }
   ],
   "source": [
    "for batch_index, batch_dict in enumerate(Traindataloader):\n",
    "#     print(batch_index)\n",
    "    print(batch_dict)\n",
    "    \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ecfdb1",
   "metadata": {},
   "source": [
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39bf2607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 정의 함수\n",
    "# 주어진 배치의 각 데이터 포인트에 대해 시퀀스의 마지막 벡터를 추출\n",
    "# => y_out에 있는 각 데이터 포인트에서 마지막 벡터 추출\n",
    "def column_gather(y_out, x_lengths):\n",
    "    \n",
    "#     x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
    "    x_lengths = x_lengths-1\n",
    "    out = []\n",
    "    for batch_index, length in enumerate(x_lengths):\n",
    "        out.append(y_out[batch_index, length])\n",
    "\n",
    "    return torch.stack(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c45938",
   "metadata": {},
   "source": [
    "### column_gather 출력예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f303587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7,  8,  9],\n",
      "        [16, 17, 18]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 가상의 입력 데이터\n",
    "y_out = torch.tensor([\n",
    "    [[1, 2, 3], [4, 5, 6], [7, 8, 9]],  # 첫 번째 시퀀스: 길이 3\n",
    "    [[10, 11, 12], [13, 14, 15], [16, 17, 18]],  # 두 번째 시퀀스: 길이 3 (길이를 맞춤)\n",
    "])\n",
    "\n",
    "x_lengths = torch.tensor([3, 3])  # 각 시퀀스의 길이 (동일하게 맞춤)\n",
    "# column_gather 함수 호출\n",
    "result = column_gather(y_out, x_lengths)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f9f13",
   "metadata": {},
   "source": [
    "### 조건이 없는 성씨 생성 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0278b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "import torch.nn as nn\n",
    "\n",
    "class SurnameGenerationModel(nn.Module):\n",
    "    def __init__(self, char_embedding_size, char_vocab_size, rnn_hidden_size, \n",
    "                 batch_first=True, padding_idx=0, dropout_p=0.5):\n",
    "        \n",
    "        super(SurnameGenerationModel, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(num_embeddings = char_vocab_size,\n",
    "                               embedding_dim = char_embedding_size,\n",
    "                               padding_idx = padding_idx)\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=char_embedding_size, \n",
    "                          hidden_size=rnn_hidden_size,\n",
    "                          batch_first=batch_first)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=rnn_hidden_size, \n",
    "                            out_features=char_vocab_size)\n",
    "        \n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        x_embedded = self.emb(x_in)\n",
    "\n",
    "        y_out, _ = self.rnn(x_embedded)\n",
    "\n",
    "        batch_size, seq_size, feat_size = y_out.shape\n",
    "        y_out = y_out.contiguous().view(batch_size * seq_size, feat_size)\n",
    "\n",
    "        y_out = self.fc(F.dropout(y_out, p=self.dropout_p))\n",
    "                         \n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "            \n",
    "        new_feat_size = y_out.shape[-1]\n",
    "        y_out = y_out.view(batch_size, seq_size, new_feat_size)\n",
    "            \n",
    "        return y_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf8c0292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char_vocab.token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3521990e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nationality_vocab.token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37fcc4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SurnameGenerationModel(\n",
       "  (emb): Embedding(88, 32, padding_idx=0)\n",
       "  (rnn): GRU(32, 32, batch_first=True)\n",
       "  (fc): Linear(in_features=32, out_features=88, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_embedding_size = 32\n",
    "rnn_hidden_size = 32\n",
    "\n",
    "model = SurnameGenerationModel(char_embedding_size=char_embedding_size,\n",
    "                               char_vocab_size=len(char_vocab.token_to_idx),\n",
    "                               rnn_hidden_size=rnn_hidden_size,\n",
    "                               padding_idx=char_vocab.mask_index)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0d622c",
   "metadata": {},
   "source": [
    "### 옵티마이저, loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "330f6821",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59b5517c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 옵티마이저\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "065a7109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nationality\n",
       "English       2972\n",
       "Russian       2373\n",
       "Arabic        1603\n",
       "Japanese       775\n",
       "Italian        600\n",
       "German         576\n",
       "Czech          414\n",
       "Spanish        258\n",
       "Dutch          236\n",
       "French         229\n",
       "Chinese        220\n",
       "Irish          183\n",
       "Greek          156\n",
       "Polish         120\n",
       "Korean          77\n",
       "Scottish        75\n",
       "Vietnamese      58\n",
       "Portuguese      55\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['nationality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0def7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numSample_list = df['nationality'].value_counts().tolist()\n",
    "# numSample_list\n",
    "# # weights 계산\n",
    "# weights = [1 - (x / sum(numSample_list)) for x in numSample_list]\n",
    "\n",
    "# # weights를 torch.FloatTensor로 변환\n",
    "# weights = torch.FloatTensor(weights)\n",
    "# weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c7f4831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sizes(y_pred, y_true):\n",
    "    \"\"\"텐서 크기 정규화\n",
    "    \n",
    "    매개변수:\n",
    "        y_pred (torch.Tensor): 모델의 출력\n",
    "            3차원 텐서이면 행렬로 변환합니다.\n",
    "        y_true (torch.Tensor): 타깃 예측\n",
    "            행렬이면 벡터로 변환합니다.\n",
    "    \"\"\"\n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b56190b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3781e8c8",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e515293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    \n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    \n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8427c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train state 초기화 \n",
    "def make_train_state():\n",
    "    return {\n",
    "        'stop_early':False,\n",
    "        'early_stopping_step':0,\n",
    "        'early_stopping_best_val':1e8,\n",
    "        'early_stopping_criteria' : 10,\n",
    "        'epoch_index' : 0,\n",
    "        'train_loss': [], \n",
    "        'train_acc' :[], \n",
    "        'val_loss' : [],\n",
    "        'val_acc' : [], \n",
    "        'test_loss' : [],\n",
    "        'test_acc' : [],\n",
    "         \n",
    "#       모델 저장파일\n",
    "        'model_filename' : 'model.pth'\n",
    "    } \n",
    "\n",
    "\n",
    "# Train update \n",
    "def update_train_state(model, train_state):\n",
    "    \n",
    "#   학습시작하면 초기에 모델 저장하기 \n",
    "    \n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(),train_state['model_filename'])\n",
    "        \n",
    "#   모델 성능이 향상되면 모델 저장(valid loss가 더 낮아지면)\n",
    "    elif train_state['epoch_index'] >=1 :\n",
    "        loss_t = train_state['val_loss'][-1]\n",
    "#        loss가 나빠지면 early stop step 업데이트\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            train_state['early_stopping_step']+=1\n",
    "            \n",
    "#        loss가 좋아지면   \n",
    "        else:\n",
    "#            early stop step 0으로 다시 초기화        \n",
    "            train_state['early_stopping_step']=0\n",
    "    \n",
    "#           최저 loss이면 모델 저장 \n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "                torch.save(model.state_dict(),train_state['model_filename'])\n",
    "\n",
    "#       기준점 넘으면 early stop \n",
    "        if train_state['early_stopping_step'] >= train_state['early_stopping_criteria']:\n",
    "            train_state['stop_early'] = True\n",
    "        \n",
    "        return train_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87458bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop_early': False,\n",
       " 'early_stopping_step': 0,\n",
       " 'early_stopping_best_val': 100000000.0,\n",
       " 'early_stopping_criteria': 10,\n",
       " 'epoch_index': 0,\n",
       " 'train_loss': [],\n",
       " 'train_acc': [],\n",
       " 'val_loss': [],\n",
       " 'val_acc': [],\n",
       " 'test_loss': [],\n",
       " 'test_acc': [],\n",
       " 'model_filename': 'model.pth'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 진행 상황 함수 초기화\n",
    "train_state = make_train_state()\n",
    "train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "550eb7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                          | 1/100 [00:03<05:02,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 4.371586958567302\n",
      "val_acc 4.5338280686133805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▊                                          | 2/100 [00:05<04:26,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 4.1335768699646\n",
      "val_acc 9.210562744860738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|█▎                                         | 3/100 [00:07<03:45,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.7114296754201255\n",
      "val_acc 10.713930249351195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|█▋                                         | 4/100 [00:09<03:16,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.3486293156941733\n",
      "val_acc 12.832106259635193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|██▏                                        | 5/100 [00:10<02:59,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.190239906311035\n",
      "val_acc 15.025515695310691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|██▌                                        | 6/100 [00:12<02:59,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.123265345891317\n",
      "val_acc 16.53255833183104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|███                                        | 7/100 [00:14<02:50,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.0531802972157798\n",
      "val_acc 17.52826504710769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|███▍                                       | 8/100 [00:15<02:44,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.022765318552653\n",
      "val_acc 17.91532824044064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███▊                                       | 9/100 [00:17<02:41,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.9727914333343506\n",
      "val_acc 18.183255007401975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▏                                     | 10/100 [00:19<02:37,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.958892504374186\n",
      "val_acc 18.390078172670172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████▌                                     | 11/100 [00:21<02:32,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.9337037404378257\n",
      "val_acc 19.00673104415413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████                                     | 12/100 [00:22<02:29,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.916237990061442\n",
      "val_acc 18.78534056276807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█████▍                                    | 13/100 [00:24<02:25,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.896769126256307\n",
      "val_acc 19.045606446731853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█████▉                                    | 14/100 [00:25<02:22,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.88787833849589\n",
      "val_acc 18.918670029134436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|██████▎                                   | 15/100 [00:27<02:19,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.883155902226766\n",
      "val_acc 19.12320328532728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|██████▋                                   | 16/100 [00:29<02:16,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.8685765266418457\n",
      "val_acc 19.288503417685632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████▏                                  | 17/100 [00:30<02:14,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.8631296157836914\n",
      "val_acc 19.075272284281862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|███████▌                                  | 18/100 [00:32<02:12,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.858396132787069\n",
      "val_acc 19.258894623114184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████▉                                  | 19/100 [00:33<02:09,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.850728432337443\n",
      "val_acc 19.484739166814368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████▍                                 | 20/100 [00:35<02:07,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.8450376192728677\n",
      "val_acc 19.723560441311168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|████████▊                                 | 21/100 [00:37<02:06,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.8420214653015137\n",
      "val_acc 19.40906483625827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████████▏                                | 22/100 [00:38<02:04,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.8391596476236978\n",
      "val_acc 19.549606188027294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|█████████▋                                | 23/100 [00:40<02:02,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.829118251800537\n",
      "val_acc 19.78057852117794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██████████                                | 24/100 [00:41<02:01,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.8381129105885825\n",
      "val_acc 19.586309917298454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██████████▌                               | 25/100 [00:43<02:00,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.829327424367269\n",
      "val_acc 19.498473526073433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|██████████▉                               | 26/100 [00:45<01:58,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.8263725439707437\n",
      "val_acc 19.46207101463547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|███████████▎                              | 27/100 [00:46<01:58,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.8221234480539956\n",
      "val_acc 19.837305495624538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|███████████▊                              | 28/100 [00:48<01:57,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.8253773053487143\n",
      "val_acc 19.586282959464338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|████████████▏                             | 29/100 [00:50<01:55,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.8185150623321533\n",
      "val_acc 19.435228836164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████▌                             | 30/100 [00:51<01:53,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.8203254540761313\n",
      "val_acc 19.28231656223044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|█████████████                             | 31/100 [00:53<01:51,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.8084877332051597\n",
      "val_acc 19.802302813929156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|█████████████▍                            | 32/100 [00:54<01:48,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.8178109327952066\n",
      "val_acc 19.552524493293376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|█████████████▊                            | 33/100 [00:56<01:47,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.808476765950521\n",
      "val_acc 19.796634684773046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|██████████████▎                           | 34/100 [00:57<01:45,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.811448017756144\n",
      "val_acc 19.847259554178752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|██████████████▋                           | 35/100 [00:59<01:43,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.810793161392212\n",
      "val_acc 19.48083755941952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███████████████                           | 36/100 [01:01<01:41,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.800457795461019\n",
      "val_acc 19.727502206095267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███████████████▌                          | 37/100 [01:02<01:41,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.8075652917226157\n",
      "val_acc 19.90372202406522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███████████████▉                          | 38/100 [01:04<01:42,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.8020026683807373\n",
      "val_acc 19.841288439906577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|████████████████▍                         | 39/100 [01:06<01:41,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.8036988576253257\n",
      "val_acc 19.64059573531469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████▊                         | 40/100 [01:07<01:38,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.8030402660369873\n",
      "val_acc 20.03275340976394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|█████████████████▏                        | 41/100 [01:09<01:36,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.8009393215179443\n",
      "val_acc 19.79637791396655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|█████████████████▋                        | 42/100 [01:11<01:34,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.7992873986562095\n",
      "val_acc 19.80497768081722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|██████████████████                        | 43/100 [01:12<01:33,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.7975541750590005\n",
      "val_acc 19.75468209231582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|██████████████████▍                       | 44/100 [01:14<01:30,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.7965813477834067\n",
      "val_acc 19.9672979850907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|██████████████████▉                       | 45/100 [01:15<01:29,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.7958484490712485\n",
      "val_acc 20.008969343824802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|███████████████████▎                      | 46/100 [01:17<01:28,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.794985214869181\n",
      "val_acc 19.791685710816104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|███████████████████▋                      | 47/100 [01:19<01:26,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.795844078063965\n",
      "val_acc 19.677495081226514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████████████████████▏                     | 48/100 [01:20<01:23,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.7936272621154785\n",
      "val_acc 20.047904543744316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|████████████████████▌                     | 49/100 [01:22<01:22,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.7915585041046143\n",
      "val_acc 20.0420528822765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████                     | 50/100 [01:24<01:20,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.7933963934580484\n",
      "val_acc 19.774537412839898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|█████████████████████▍                    | 51/100 [01:25<01:18,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.7910653750101724\n",
      "val_acc 19.712296929447767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|█████████████████████▊                    | 52/100 [01:27<01:17,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.7933882077534995\n",
      "val_acc 19.865518715060375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|██████████████████████▎                   | 53/100 [01:28<01:16,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.793864091237386\n",
      "val_acc 19.784379602800183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|██████████████████████▋                   | 54/100 [01:30<01:14,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.795107285181681\n",
      "val_acc 19.868655573282982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|███████████████████████                   | 55/100 [01:32<01:13,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.7876636187235513\n",
      "val_acc 19.914197171538206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|███████████████████████▌                  | 56/100 [01:33<01:11,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.789115826288859\n",
      "val_acc 19.869675478662447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|███████████████████████▉                  | 57/100 [01:35<01:09,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.78832213083903\n",
      "val_acc 19.765876517988747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|████████████████████████▎                 | 58/100 [01:37<01:08,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.792834758758545\n",
      "val_acc 19.685286130812774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|████████████████████████▊                 | 59/100 [01:38<01:06,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.7841528256734214\n",
      "val_acc 19.841875951340796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████▏                | 60/100 [01:40<01:04,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.785991827646891\n",
      "val_acc 19.91310280684384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|█████████████████████████▌                | 61/100 [01:41<01:03,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.786848545074463\n",
      "val_acc 19.878133944572323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████████████████████████                | 62/100 [01:43<01:01,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.7887869675954184\n",
      "val_acc 19.83070484389385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|██████████████████████████▍               | 63/100 [01:45<00:59,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.7879863580067954\n",
      "val_acc 19.847825263441717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|██████████████████████████▉               | 64/100 [01:46<00:57,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.7836272716522217\n",
      "val_acc 19.703591179757673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|███████████████████████████▎              | 65/100 [01:48<00:56,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.784733215967814\n",
      "val_acc 20.01200646433569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|███████████████████████████▋              | 66/100 [01:49<00:54,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.7780936559041343\n",
      "val_acc 19.800835206892632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|████████████████████████████▏             | 67/100 [01:51<00:53,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.783039093017578\n",
      "val_acc 19.775765744635375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|████████████████████████████▌             | 68/100 [01:53<00:52,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.787869135538737\n",
      "val_acc 19.90638948511476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|████████████████████████████▉             | 69/100 [01:54<00:50,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.7849205334981284\n",
      "val_acc 19.65123604539941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████▍            | 70/100 [01:56<00:48,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.778562545776367\n",
      "val_acc 19.879846159342893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|█████████████████████████████▊            | 71/100 [01:57<00:46,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.7813854217529297\n",
      "val_acc 19.805967269014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|██████████████████████████████▏           | 72/100 [01:59<00:45,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.78399928410848\n",
      "val_acc 19.81870972675603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|██████████████████████████████▋           | 73/100 [02:01<00:43,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.7800671259562173\n",
      "val_acc 19.881977450707442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████████████████████████████           | 74/100 [02:02<00:41,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.78303329149882\n",
      "val_acc 19.602702172872757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████████████████████████████▌          | 75/100 [02:04<00:40,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.78485377629598\n",
      "val_acc 19.864646774174187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████████████████████████████▌          | 75/100 [02:06<00:42,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.784484624862671\n",
      "val_acc 19.472807831836885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "mask_index = char_vocab.mask_index\n",
    "\n",
    "# 에포크만큼\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "\n",
    "#     print('epoch',epoch)\n",
    "#     print(train_state['epoch_index']) \n",
    "    train_state['epoch_index'] +=1 \n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "\n",
    "#     모델을 학습 모드로 설정 -> 드롭아웃 및 배치 정규화와 같은 학습 중에만 적용되는 기법들이 활성화\n",
    "#     모델을 평가 모드로 전환하려면 model.eval()을 사용\n",
    "    model.train()\n",
    "# 배치 만큼\n",
    "    for batch_idx, batch_data in enumerate(Traindataloader):\n",
    "        \n",
    "\n",
    "#       1. 옵티마이저 그레디언트 0으로 초기화\n",
    "        optimizer.zero_grad()\n",
    "#       2. 모델에 데이터 넣어서 출력받기\n",
    "        y_pred = model(x_in=batch_data['x_data'])\n",
    "#       3. loss 계산하기\n",
    "        loss =  sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "    \n",
    "#       4. gradient 계산하기\n",
    "        loss.backward()\n",
    "\n",
    "#       5. 옵티마이저 가중치 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "#       Accuracy 계산\n",
    "        # 이동 손실과 이동 정확도를 계산\n",
    "        running_loss += (loss.item() - running_loss) / (batch_idx + 1)\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "        running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "\n",
    "\n",
    "#   valid에 대한 계산\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "    model.eval() # 모델 파라미터를 수정하지 못 하게 비활성화\n",
    "\n",
    "    for batch_idx, batch_data in enumerate(Validdataloader):\n",
    "\n",
    "#       1. 모델의 출력값(y_pred)계산\n",
    "        y_pred = model(x_in=batch_data['x_data'])\n",
    "\n",
    "#       2. loss 계산\n",
    "        loss_t = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "        running_loss += (loss_t.item() - running_loss) / (batch_idx + 1)\n",
    "\n",
    "#       3. Accuracy 계산\n",
    "        acc_t = compute_accuracy(y_pred,batch_data['y_target'],mask_index)\n",
    "        running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "    \n",
    "    print(\"val_loss\",running_loss)\n",
    "    print(\"val_acc\",running_acc)\n",
    "\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "    \n",
    "\n",
    "#   전체 loss, acc 저장\n",
    "    train_state = update_train_state(model=model,\n",
    "                                     train_state=train_state)\n",
    "#   early stop해라고 했으면 학습 멈추기    \n",
    "    if train_state['stop_early']:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81148c1c",
   "metadata": {},
   "source": [
    "### Test 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c570a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 좋은 모델을 사용해 테스트 세트의 손실과 정확도를 계산합니다\n",
    "\n",
    "model.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "\n",
    "# 가중치 업데이트 하지 못 하게\n",
    "model.eval()\n",
    "\n",
    "for batch_idx, batch_data in enumerate(Testdataloader):\n",
    "    \n",
    "    y_pred = model(x_in=batch_data['x_data'])\n",
    "    loss = sequence_loss(y_pred,batch_data['y_target'],mask_index)\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "    \n",
    "    acc_t = compute_accuracy(y_pred, batch_data['y_target'],mask_index)\n",
    "    running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4576ed1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 2.841\n",
      "테스트 정확도: 20.11\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 손실: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {:.2f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a501556c",
   "metadata": {},
   "source": [
    "### 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "86fbe523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_samples(sampled_indices):\n",
    "    \"\"\"인덱스를 성씨 문자열로 변환합니다\n",
    "    \n",
    "    매개변수:\n",
    "        sampled_indices (torch.Tensor): `sample_from_model` 함수에서 얻은 인덱스\n",
    "    \"\"\"\n",
    "    decoded_surnames = []\n",
    "    vocab = char_vocab\n",
    "    \n",
    "    for sample_index in range(sampled_indices.shape[0]):\n",
    "        surname = \"\"\n",
    "        for time_step in range(sampled_indices.shape[1]):\n",
    "            sample_item = sampled_indices[sample_index, time_step].item()\n",
    "            if sample_item == vocab.begin_seq_index:\n",
    "                continue\n",
    "            elif sample_item == vocab.end_seq_index:\n",
    "                break\n",
    "            else:\n",
    "                surname += lookup_index(vocab,sample_item)\n",
    "        decoded_surnames.append(surname)\n",
    "    return decoded_surnames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2d0426c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(model, num_samples=1, sample_size=20, \n",
    "                      temperature=1.0):\n",
    "    \"\"\"모델이 만든 인덱스 시퀀스를 샘플링합니다.\n",
    "    \n",
    "    매개변수:\n",
    "        model (SurnameGenerationModel): 훈련 모델\n",
    "        num_samples (int): 샘플 개수\n",
    "        sample_size (int): 샘플의 최대 길이\n",
    "        temperature (float): 무작위성 정도\n",
    "            0.0 < temperature < 1.0 이면 최대 값을 선택할 가능성이 높습니다\n",
    "            temperature > 1.0 이면 균등 분포에 가깝습니다\n",
    "    반환값:\n",
    "        indices (torch.Tensor): 인덱스 행렬\n",
    "        shape = (num_samples, sample_size)\n",
    "    \"\"\"\n",
    "    begin_seq_index = [char_vocab.begin_seq_index \n",
    "                       for _ in range(num_samples)]\n",
    "    begin_seq_index = torch.tensor(begin_seq_index, \n",
    "                                   dtype=torch.int64).unsqueeze(dim=1)\n",
    "    indices = [begin_seq_index]\n",
    "    h_t = None\n",
    "    \n",
    "    for time_step in range(sample_size):\n",
    "        x_t = indices[time_step]\n",
    "        x_emb_t = model.emb(x_t)\n",
    "        rnn_out_t, h_t = model.rnn(x_emb_t, h_t)\n",
    "        prediction_vector = model.fc(rnn_out_t.squeeze(dim=1))\n",
    "        probability_vector = F.softmax(prediction_vector / temperature, dim=1)\n",
    "        indices.append(torch.multinomial(probability_vector, num_samples=1))\n",
    "    indices = torch.stack(indices).squeeze().permute(1, 0)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e4dc3563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Bimektvv\n",
      "Aavcvg\n",
      "Ksygm\n",
      "Minilg\n",
      "Lklalue\n",
      "Aoiceyvrk\n",
      "Ketlr\n",
      "Giinmllhrdh\n",
      "Mnpaejn\n",
      "Shneol\n"
     ]
    }
   ],
   "source": [
    "# 생성할 이름 개수\n",
    "num_names = 10\n",
    "model = model.cpu()\n",
    "# 이름 생성\n",
    "sampled_surnames = decode_samples(\n",
    "    sample_from_model(model, num_samples=num_names))\n",
    "# 결과 출력\n",
    "print (\"-\"*15)\n",
    "for i in range(num_names):\n",
    "    print (sampled_surnames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62057cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27698821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
