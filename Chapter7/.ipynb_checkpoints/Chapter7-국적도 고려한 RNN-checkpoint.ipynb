{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ba4e86",
   "metadata": {},
   "source": [
    "# 예제: 국적도 고려한 RNN으로 성씨 생성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbb0140",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "962715ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nationality</th>\n",
       "      <th>nationality_index</th>\n",
       "      <th>split</th>\n",
       "      <th>surname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Totah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Abboud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Fakhoury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Srour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Sayegh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10975</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>11</td>\n",
       "      <td>test</td>\n",
       "      <td>Dinh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10976</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>11</td>\n",
       "      <td>test</td>\n",
       "      <td>Phung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10977</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>11</td>\n",
       "      <td>test</td>\n",
       "      <td>Quang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10978</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>11</td>\n",
       "      <td>test</td>\n",
       "      <td>Vu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10979</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>11</td>\n",
       "      <td>test</td>\n",
       "      <td>Ha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10980 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      nationality  nationality_index  split   surname\n",
       "0          Arabic                 15  train     Totah\n",
       "1          Arabic                 15  train    Abboud\n",
       "2          Arabic                 15  train  Fakhoury\n",
       "3          Arabic                 15  train     Srour\n",
       "4          Arabic                 15  train    Sayegh\n",
       "...           ...                ...    ...       ...\n",
       "10975  Vietnamese                 11   test      Dinh\n",
       "10976  Vietnamese                 11   test     Phung\n",
       "10977  Vietnamese                 11   test     Quang\n",
       "10978  Vietnamese                 11   test        Vu\n",
       "10979  Vietnamese                 11   test        Ha\n",
       "\n",
       "[10980 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../data/surnames_with_splits.csv\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29b1afc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nationality\n",
       "English       2972\n",
       "Russian       2373\n",
       "Arabic        1603\n",
       "Japanese       775\n",
       "Italian        600\n",
       "German         576\n",
       "Czech          414\n",
       "Spanish        258\n",
       "Dutch          236\n",
       "French         229\n",
       "Chinese        220\n",
       "Irish          183\n",
       "Greek          156\n",
       "Polish         120\n",
       "Korean          77\n",
       "Scottish        75\n",
       "Vietnamese      58\n",
       "Portuguese      55\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['nationality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f706d137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split\n",
       "train    7680\n",
       "test     1660\n",
       "val      1640\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['split'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f63e53",
   "metadata": {},
   "source": [
    "### 데이터 split(train/valid/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2795b1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 다시 train/valid/test로 나눠줌\n",
    "\n",
    "# train 데이터 \n",
    "train_df = df[df.split=='train']\n",
    "train_size = len(train_df)\n",
    "\n",
    "# valid 데이터 \n",
    "val_df = df[df.split=='val']\n",
    "val_size = len(val_df)\n",
    "\n",
    "# test 데이터 \n",
    "test_df = df[df.split=='test']\n",
    "test_size = len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8000220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dict = {'train': (train_df, train_size), \n",
    "              'val': (val_df, val_size), \n",
    "              'test': (test_df, test_size)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf85816",
   "metadata": {},
   "source": [
    "## 2. Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1141fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, token_to_idx=None):\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self.token_to_idx = token_to_idx\n",
    "\n",
    "        self.idx_to_token = {idx: token \n",
    "                              for token, idx in self.token_to_idx.items()}\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \n",
    "#       만약 해당 토큰이 있으면 토큰 idx만 return\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "            \n",
    "#       만약 해당 토큰이 없으면 새로운 토큰 만들어줌\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6934b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "443a9bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab 생성\n",
    "char_vocab = SequenceVocabulary()\n",
    "nationality_vocab = Vocabulary()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    for char in row.surname:\n",
    "        char_vocab.add_token(char)\n",
    "    nationality_vocab.add_token(row.nationality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dfc4c5",
   "metadata": {},
   "source": [
    "### Char Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d51f439e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<MASK>': 0, '<UNK>': 1, '<BEGIN>': 2, '<END>': 3, 'T': 4, 'o': 5, 't': 6, 'a': 7, 'h': 8, 'A': 9}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(char_vocab.token_to_idx.items())[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4659251",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<MASK>', 1: '<UNK>', 2: '<BEGIN>', 3: '<END>', 4: 'T', 5: 'o', 6: 't', 7: 'a', 8: 'h', 9: 'A'}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(char_vocab.idx_to_token.items())[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e172eee",
   "metadata": {},
   "source": [
    "### 국적 Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bb67a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Arabic': 0, 'Chinese': 1, 'Czech': 2, 'Dutch': 3, 'English': 4, 'French': 5, 'German': 6, 'Greek': 7, 'Irish': 8, 'Italian': 9}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(nationality_vocab.token_to_idx.items())[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caf7746c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Arabic', 1: 'Chinese', 2: 'Czech', 3: 'Dutch', 4: 'English', 5: 'French', 6: 'German', 7: 'Greek', 8: 'Irish', 9: 'Italian'}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(nationality_vocab.idx_to_token.items())[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd30e80",
   "metadata": {},
   "source": [
    "## 3. Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0720a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 토큰에 대응하는 인덱스 반환\n",
    "\n",
    "def lookup_token(vocabulary_class,token):\n",
    "    return vocabulary_class.token_to_idx[token]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d025329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 인덱스에 대응하는 토큰 반환\n",
    "\n",
    "def lookup_index(vocabulary_class, index):\n",
    "        if index not in vocabulary_class.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return vocabulary_class.idx_to_token[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e09e6111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰의 수: 18\n"
     ]
    }
   ],
   "source": [
    "vocab_length = len(nationality_vocab.token_to_idx)\n",
    "print(\"토큰의 수:\", vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b203ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰의 수: 88\n"
     ]
    }
   ],
   "source": [
    "vocab_length = len(char_vocab.token_to_idx)\n",
    "print(\"토큰의 수:\", vocab_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11824f7",
   "metadata": {},
   "source": [
    "### 텍스트(surname)에 대한 원 핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e375f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예시\n",
      "(array([ 2, 20,  8,  5, 23,  0,  0,  0,  0,  0]), array([20,  8,  5, 23,  3,  0,  0,  0,  0,  0]))\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# vector_length (int): 인덱스 벡터의 길이를 맞추기 위한 매개변수\n",
    "def vectorize(surname, vector_length=-1):\n",
    "\n",
    "    indices = [char_vocab.begin_seq_index]\n",
    "    indices.extend(lookup_token(char_vocab,token) \n",
    "                   for token in surname)\n",
    "    indices.append(char_vocab.end_seq_index)\n",
    "\n",
    "    if vector_length < 0:\n",
    "        vector_length = len(indices)\n",
    "    \n",
    "    from_vector = np.empty(vector_length, dtype=np.int64)         \n",
    "    from_indices = indices[:-1]\n",
    "    from_vector[:len(from_indices)] = from_indices\n",
    "    from_vector[len(from_indices):] = char_vocab.mask_index\n",
    "\n",
    "    to_vector = np.empty(vector_length, dtype=np.int64)\n",
    "    to_indices = indices[1:]\n",
    "    to_vector[:len(to_indices)] = to_indices\n",
    "    to_vector[len(to_indices):] = char_vocab.mask_index\n",
    "        \n",
    "    return from_vector, to_vector\n",
    "\n",
    "print(\"예시\")\n",
    "example = vectorize(\"Choi\", 10)\n",
    "print(example)\n",
    "print(len(example[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b49a42",
   "metadata": {},
   "source": [
    "### SurnameDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7320e89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, surname_df):\n",
    "        self.surname_df = surname_df\n",
    "        self.max_seq_length = max(map(len, df.surname)) + 2\n",
    "        \n",
    "     \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.surname_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.surname_df.iloc[index]\n",
    "        \n",
    "        from_vector, to_vector = vectorize(row.surname, self.max_seq_length)\n",
    "        \n",
    "        nationality_index = lookup_token(nationality_vocab,row.nationality)\n",
    "\n",
    "        return {'x_data': from_vector, \n",
    "                'y_target': to_vector, \n",
    "                'class_index': nationality_index}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cae239",
   "metadata": {},
   "source": [
    "### 데이터셋 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14c6c27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SurnameDataset at 0x7f9ac1d8fdf0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋을 인스턴스화 해주어야 로더에 넣어줄 수 있다. \n",
    "\n",
    "train_dataset = SurnameDataset(train_df)\n",
    "train_dataset\n",
    "\n",
    "valid_dataset = SurnameDataset(val_df)\n",
    "valid_dataset\n",
    "\n",
    "test_dataset = SurnameDataset(test_df)\n",
    "test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b097a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "19\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.max_seq_length)\n",
    "print(valid_dataset.max_seq_length)\n",
    "print(test_dataset.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9514135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더 설정\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# drop_last=True -> 배치 사이즈보다 over하면 drop\n",
    "\n",
    "Traindataloader = DataLoader(dataset=train_dataset, batch_size=512,\n",
    "                            shuffle=True, drop_last=True)\n",
    "\n",
    "Validdataloader = DataLoader(dataset=valid_dataset, batch_size=512,\n",
    "                            shuffle=True, drop_last=True)\n",
    "\n",
    "Testdataloader = DataLoader(dataset=test_dataset, batch_size=512,\n",
    "                            shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e8a9d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7680 15\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset),len(Traindataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "daefd63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x_data': tensor([[ 2, 22,  7,  ...,  0,  0,  0],\n",
      "        [ 2, 17,  8,  ...,  0,  0,  0],\n",
      "        [ 2, 24,  7,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 2, 34,  7,  ...,  0,  0,  0],\n",
      "        [ 2, 35,  8,  ...,  0,  0,  0],\n",
      "        [ 2, 45, 18,  ...,  0,  0,  0]]), 'y_target': tensor([[22,  7, 31,  ...,  0,  0,  0],\n",
      "        [17,  8, 23,  ...,  0,  0,  0],\n",
      "        [24,  7, 25,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [34,  7, 15,  ...,  0,  0,  0],\n",
      "        [35,  8,  7,  ...,  0,  0,  0],\n",
      "        [45, 18, 12,  ...,  0,  0,  0]]), 'class_index': tensor([ 0,  4,  4,  2, 14,  3,  2, 14, 13,  2, 14, 14,  4,  4,  9,  0,  7,  4,\n",
      "         9,  9,  0, 12, 10,  9, 17,  4,  0,  4,  4,  8,  4,  0,  4,  4,  0, 10,\n",
      "         4,  4,  0,  9,  0, 10, 14,  6,  4,  0, 14, 14,  0,  3, 14, 16,  4,  0,\n",
      "        14,  4,  1, 17, 10, 14, 14,  0, 10,  5, 14, 16,  4,  4,  7, 14,  0,  0,\n",
      "        14, 10, 14, 10,  3, 10,  9,  6,  4,  9,  6,  4, 14,  4,  4,  0,  4, 14,\n",
      "        14,  4, 14,  4,  9, 10,  4, 14,  2,  2, 10,  4,  0,  0,  9, 10, 15,  9,\n",
      "         0,  0, 14, 16, 14, 14, 14,  2,  0,  4,  6, 14,  4, 14,  5,  8,  4,  4,\n",
      "         2,  6,  1,  0,  0,  4,  4,  4,  7, 14, 13,  1, 14,  4, 16,  6, 14,  4,\n",
      "         4,  4,  0,  4,  4,  9, 12,  0,  0,  4, 14,  3,  4, 14,  6, 14,  4,  0,\n",
      "         4,  9, 17, 13, 17,  0, 14,  4,  0,  4, 10, 14,  6, 14,  4,  4,  9,  9,\n",
      "        14, 14,  9, 10, 14, 14,  0, 16,  4,  7,  7,  5,  4, 14, 17,  4,  6,  4,\n",
      "         0,  4, 14,  4, 10, 14, 14, 14,  4, 14, 14, 11,  0,  4, 16,  4,  2,  4,\n",
      "        14,  4, 14,  7,  0,  4, 14,  2,  6,  4,  8, 14,  4,  1,  4, 14,  4, 14,\n",
      "         0,  4,  6, 10, 10,  4,  3, 14,  4,  6, 14,  7,  4,  0, 14, 14,  1,  0,\n",
      "         9,  6, 12,  5,  4, 14, 14,  1, 14,  0,  0,  9,  0,  4,  4, 14, 14, 14,\n",
      "         6, 12, 14, 14,  0, 10, 14,  0,  6,  4,  5, 14, 16,  0,  4, 14,  0,  2,\n",
      "        16, 14,  4, 14, 14,  5,  7, 14, 10,  0,  4, 10,  3,  4,  3,  5, 14,  6,\n",
      "        14,  4,  4, 13, 15,  4,  4,  4,  0,  4,  8,  0, 14, 12,  9,  0, 14,  8,\n",
      "        10, 14,  2,  4,  6, 14,  4,  4,  4,  1,  0, 10,  4,  4, 10,  0,  6,  4,\n",
      "         9,  9,  0, 14, 15, 14,  2,  1,  9, 14,  7,  4,  4,  0,  8, 10, 16,  7,\n",
      "        15,  4, 14,  0,  2,  4,  4, 10,  3,  4, 14,  4, 12,  2,  2, 10,  0,  9,\n",
      "        14,  4, 10,  4, 16,  0,  0,  4,  4,  6,  3,  0, 14, 14,  4,  9,  4,  4,\n",
      "         4, 14,  4, 16,  4,  4, 14, 14,  0, 14,  1, 10,  4, 14,  2,  0, 14,  2,\n",
      "         6,  0,  4,  2,  4,  4, 14,  7,  4,  0,  1, 10,  6,  9,  0, 14,  9,  9,\n",
      "         9, 14,  0,  4,  4,  4, 10, 14,  2, 14,  9,  4,  7,  0,  4, 10,  0,  5,\n",
      "        10,  4,  4,  0,  1, 14, 14, 12,  4, 14,  4,  4,  4, 11, 13, 10, 14,  8,\n",
      "         0, 14, 14, 10,  4,  4,  0,  4,  0,  9,  6,  4, 14,  4, 10,  4,  5,  0,\n",
      "        14,  0,  0,  5,  0, 14, 10, 14,  7,  0,  0,  0,  4, 14,  8,  6, 10,  9,\n",
      "        14,  6,  4,  1,  7,  9, 10,  2])}\n"
     ]
    }
   ],
   "source": [
    "for batch_index, batch_dict in enumerate(Traindataloader):\n",
    "#     print(batch_index)\n",
    "    print(batch_dict)\n",
    "    \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ecfdb1",
   "metadata": {},
   "source": [
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39bf2607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 정의 함수\n",
    "# 주어진 배치의 각 데이터 포인트에 대해 시퀀스의 마지막 벡터를 추출\n",
    "# => y_out에 있는 각 데이터 포인트에서 마지막 벡터 추출\n",
    "def column_gather(y_out, x_lengths):\n",
    "    \n",
    "#     x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
    "    x_lengths = x_lengths-1\n",
    "    out = []\n",
    "    for batch_index, length in enumerate(x_lengths):\n",
    "        out.append(y_out[batch_index, length])\n",
    "\n",
    "    return torch.stack(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c45938",
   "metadata": {},
   "source": [
    "### column_gather 출력예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f303587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7,  8,  9],\n",
      "        [16, 17, 18]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 가상의 입력 데이터\n",
    "y_out = torch.tensor([\n",
    "    [[1, 2, 3], [4, 5, 6], [7, 8, 9]],  # 첫 번째 시퀀스: 길이 3\n",
    "    [[10, 11, 12], [13, 14, 15], [16, 17, 18]],  # 두 번째 시퀀스: 길이 3 (길이를 맞춤)\n",
    "])\n",
    "\n",
    "x_lengths = torch.tensor([3, 3])  # 각 시퀀스의 길이 (동일하게 맞춤)\n",
    "# column_gather 함수 호출\n",
    "result = column_gather(y_out, x_lengths)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f9f13",
   "metadata": {},
   "source": [
    "### 조건이 없는 성씨 생성 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0278b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "import torch.nn as nn\n",
    "\n",
    "class SurnameGenerationModel(nn.Module):\n",
    "    def __init__(self, char_embedding_size, char_vocab_size, num_nationalities, rnn_hidden_size, \n",
    "                 batch_first=True, padding_idx=0, dropout_p=0.5):\n",
    "        \n",
    "        super(SurnameGenerationModel, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(num_embeddings = char_vocab_size,\n",
    "                               embedding_dim = char_embedding_size,\n",
    "                               padding_idx = padding_idx)\n",
    "        \n",
    "        self.nation_emb = nn.Embedding(embedding_dim=rnn_hidden_size,\n",
    "                                            num_embeddings=num_nationalities)\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=char_embedding_size, \n",
    "                          hidden_size=rnn_hidden_size,\n",
    "                          batch_first=batch_first)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=rnn_hidden_size, \n",
    "                            out_features=char_vocab_size)\n",
    "        \n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "    def forward(self, x_in, nationality_index, apply_softmax=False):\n",
    "        x_embedded = self.emb(x_in)\n",
    "        nationality_embedded = self.nation_emb(nationality_index).unsqueeze(0)\n",
    "        \n",
    "        y_out, _ = self.rnn(x_embedded,nationality_embedded)\n",
    "\n",
    "        batch_size, seq_size, feat_size = y_out.shape\n",
    "        y_out = y_out.contiguous().view(batch_size * seq_size, feat_size)\n",
    "\n",
    "        y_out = self.fc(F.dropout(y_out, p=self.dropout_p))\n",
    "                         \n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "            \n",
    "        new_feat_size = y_out.shape[-1]\n",
    "        y_out = y_out.view(batch_size, seq_size, new_feat_size)\n",
    "            \n",
    "        return y_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf8c0292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char_vocab.token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3521990e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nationality_vocab.token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "37fcc4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SurnameGenerationModel(\n",
       "  (emb): Embedding(88, 32, padding_idx=0)\n",
       "  (nation_emb): Embedding(18, 32)\n",
       "  (rnn): GRU(32, 32, batch_first=True)\n",
       "  (fc): Linear(in_features=32, out_features=88, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_embedding_size = 32\n",
    "rnn_hidden_size = 32\n",
    "\n",
    "model = SurnameGenerationModel(char_embedding_size=char_embedding_size,\n",
    "                               char_vocab_size=len(char_vocab.token_to_idx),\n",
    "                               num_nationalities=len(nationality_vocab.token_to_idx),\n",
    "                               rnn_hidden_size=rnn_hidden_size,\n",
    "                               padding_idx=char_vocab.mask_index)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0d622c",
   "metadata": {},
   "source": [
    "### 옵티마이저, loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "330f6821",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "59b5517c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 옵티마이저\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "065a7109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nationality\n",
       "English       2972\n",
       "Russian       2373\n",
       "Arabic        1603\n",
       "Japanese       775\n",
       "Italian        600\n",
       "German         576\n",
       "Czech          414\n",
       "Spanish        258\n",
       "Dutch          236\n",
       "French         229\n",
       "Chinese        220\n",
       "Irish          183\n",
       "Greek          156\n",
       "Polish         120\n",
       "Korean          77\n",
       "Scottish        75\n",
       "Vietnamese      58\n",
       "Portuguese      55\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['nationality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0def7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numSample_list = df['nationality'].value_counts().tolist()\n",
    "# numSample_list\n",
    "# # weights 계산\n",
    "# weights = [1 - (x / sum(numSample_list)) for x in numSample_list]\n",
    "\n",
    "# # weights를 torch.FloatTensor로 변환\n",
    "# weights = torch.FloatTensor(weights)\n",
    "# weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7c7f4831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sizes(y_pred, y_true):\n",
    "    \"\"\"텐서 크기 정규화\n",
    "    \n",
    "    매개변수:\n",
    "        y_pred (torch.Tensor): 모델의 출력\n",
    "            3차원 텐서이면 행렬로 변환합니다.\n",
    "        y_true (torch.Tensor): 타깃 예측\n",
    "            행렬이면 벡터로 변환합니다.\n",
    "    \"\"\"\n",
    "    if len(y_pred.size()) == 3:\n",
    "        y_pred = y_pred.contiguous().view(-1, y_pred.size(2))\n",
    "    if len(y_true.size()) == 2:\n",
    "        y_true = y_true.contiguous().view(-1)\n",
    "    return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b56190b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "\n",
    "def sequence_loss(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "    return F.cross_entropy(y_pred, y_true, ignore_index=mask_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3781e8c8",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e515293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_true, mask_index):\n",
    "    y_pred, y_true = normalize_sizes(y_pred, y_true)\n",
    "\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    \n",
    "    correct_indices = torch.eq(y_pred_indices, y_true).float()\n",
    "    valid_indices = torch.ne(y_true, mask_index).float()\n",
    "    \n",
    "    n_correct = (correct_indices * valid_indices).sum().item()\n",
    "    n_valid = valid_indices.sum().item()\n",
    "\n",
    "    return n_correct / n_valid * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8427c454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train state 초기화 \n",
    "def make_train_state():\n",
    "    return {\n",
    "        'stop_early':False,\n",
    "        'early_stopping_step':0,\n",
    "        'early_stopping_best_val':1e8,\n",
    "        'early_stopping_criteria' : 10,\n",
    "        'epoch_index' : 0,\n",
    "        'train_loss': [], \n",
    "        'train_acc' :[], \n",
    "        'val_loss' : [],\n",
    "        'val_acc' : [], \n",
    "        'test_loss' : [],\n",
    "        'test_acc' : [],\n",
    "         \n",
    "#       모델 저장파일\n",
    "        'model_filename' : 'model.pth'\n",
    "    } \n",
    "\n",
    "\n",
    "# Train update \n",
    "def update_train_state(model, train_state):\n",
    "    \n",
    "#   학습시작하면 초기에 모델 저장하기 \n",
    "    \n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(),train_state['model_filename'])\n",
    "        \n",
    "#   모델 성능이 향상되면 모델 저장(valid loss가 더 낮아지면)\n",
    "    elif train_state['epoch_index'] >=1 :\n",
    "        loss_t = train_state['val_loss'][-1]\n",
    "#        loss가 나빠지면 early stop step 업데이트\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            train_state['early_stopping_step']+=1\n",
    "            \n",
    "#        loss가 좋아지면   \n",
    "        else:\n",
    "#            early stop step 0으로 다시 초기화        \n",
    "            train_state['early_stopping_step']=0\n",
    "    \n",
    "#           최저 loss이면 모델 저장 \n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "                torch.save(model.state_dict(),train_state['model_filename'])\n",
    "\n",
    "#       기준점 넘으면 early stop \n",
    "        if train_state['early_stopping_step'] >= train_state['early_stopping_criteria']:\n",
    "            train_state['stop_early'] = True\n",
    "        \n",
    "        return train_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87458bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop_early': False,\n",
       " 'early_stopping_step': 0,\n",
       " 'early_stopping_best_val': 100000000.0,\n",
       " 'early_stopping_criteria': 10,\n",
       " 'epoch_index': 0,\n",
       " 'train_loss': [],\n",
       " 'train_acc': [],\n",
       " 'val_loss': [],\n",
       " 'val_acc': [],\n",
       " 'test_loss': [],\n",
       " 'test_acc': [],\n",
       " 'model_filename': 'model.pth'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 진행 상황 함수 초기화\n",
    "train_state = make_train_state()\n",
    "train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "550eb7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                          | 1/100 [00:02<04:02,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 4.38468599319458\n",
      "val_acc 4.240601526834645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▊                                          | 2/100 [00:04<03:40,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 4.146158854166667\n",
      "val_acc 11.143917595981465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|█▎                                         | 3/100 [00:07<04:06,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.77731990814209\n",
      "val_acc 14.358069765971136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|█▋                                         | 4/100 [00:09<03:46,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.4539973735809326\n",
      "val_acc 15.687638372598947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|██▏                                        | 5/100 [00:11<03:30,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.300968647003174\n",
      "val_acc 16.837265709518334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|██▌                                        | 6/100 [00:13<03:23,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.212869723637899\n",
      "val_acc 17.67955726524717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|███                                        | 7/100 [00:15<03:14,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.1570770740509033\n",
      "val_acc 18.236864825511372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|███▍                                       | 8/100 [00:17<03:06,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.105630954106649\n",
      "val_acc 19.420657010840685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███▊                                       | 9/100 [00:19<03:05,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.071368932723999\n",
      "val_acc 19.177238303592617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▏                                     | 10/100 [00:21<02:59,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.038522958755493\n",
      "val_acc 20.05096690361011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████▌                                     | 11/100 [00:23<03:04,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.0228164196014404\n",
      "val_acc 20.319826482831836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████                                     | 12/100 [00:25<02:57,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.012853225072225\n",
      "val_acc 20.686607354735553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█████▍                                    | 13/100 [00:27<02:52,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.0072545210520425\n",
      "val_acc 20.941600808262283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█████▉                                    | 14/100 [00:29<02:48,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.991237163543701\n",
      "val_acc 21.040790431445316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|██████▎                                   | 15/100 [00:31<02:53,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.994898478190104\n",
      "val_acc 21.22007312225309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|██████▋                                   | 16/100 [00:33<02:51,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.0061072508494058\n",
      "val_acc 21.993889433352848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████▏                                  | 17/100 [00:35<02:45,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.0020174980163574\n",
      "val_acc 21.75070365270316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|███████▌                                  | 18/100 [00:37<02:39,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.011943260828654\n",
      "val_acc 21.86098003256209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████▉                                  | 19/100 [00:39<02:37,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.017106533050537\n",
      "val_acc 22.767459311378456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████▍                                 | 20/100 [00:41<02:35,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.023940324783325\n",
      "val_acc 22.939004006695402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|████████▊                                 | 21/100 [00:43<02:34,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.0361363887786865\n",
      "val_acc 23.14873753498164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████████▏                                | 22/100 [00:45<02:34,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.02583114306132\n",
      "val_acc 23.72851730122807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|█████████▋                                | 23/100 [00:47<02:32,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.032292445500692\n",
      "val_acc 23.606516535284776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|█████████▋                                | 23/100 [00:49<02:44,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 3.0546260674794516\n",
      "val_acc 23.887859490414918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "mask_index = char_vocab.mask_index\n",
    "\n",
    "# 에포크만큼\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "\n",
    "#     print('epoch',epoch)\n",
    "#     print(train_state['epoch_index']) \n",
    "    train_state['epoch_index'] +=1 \n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "\n",
    "#     모델을 학습 모드로 설정 -> 드롭아웃 및 배치 정규화와 같은 학습 중에만 적용되는 기법들이 활성화\n",
    "#     모델을 평가 모드로 전환하려면 model.eval()을 사용\n",
    "    model.train()\n",
    "# 배치 만큼\n",
    "    for batch_idx, batch_data in enumerate(Traindataloader):\n",
    "        \n",
    "\n",
    "#       1. 옵티마이저 그레디언트 0으로 초기화\n",
    "        optimizer.zero_grad()\n",
    "#       2. 모델에 데이터 넣어서 출력받기\n",
    "        y_pred = model(x_in=batch_data['x_data'], \n",
    "                           nationality_index=batch_data['class_index'])\n",
    "#       3. loss 계산하기\n",
    "        loss =  sequence_loss(y_pred, batch_data['y_target'], mask_index)\n",
    "\n",
    "    \n",
    "#       4. gradient 계산하기\n",
    "        loss.backward()\n",
    "\n",
    "#       5. 옵티마이저 가중치 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "#       Accuracy 계산\n",
    "        # 이동 손실과 이동 정확도를 계산\n",
    "        running_loss += (loss.item() - running_loss) / (batch_idx + 1)\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "        running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "\n",
    "\n",
    "#   valid에 대한 계산\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "    model.eval() # 모델 파라미터를 수정하지 못 하게 비활성화\n",
    "\n",
    "    for batch_idx, batch_data in enumerate(Validdataloader):\n",
    "\n",
    "#       1. 모델의 출력값(y_pred)계산\n",
    "        y_pred = model(x_in=batch_data['x_data'], \n",
    "                           nationality_index=batch_data['class_index'])\n",
    "\n",
    "#       2. loss 계산\n",
    "        loss_t = sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "        running_loss += (loss_t.item() - running_loss) / (batch_idx + 1)\n",
    "\n",
    "#       3. Accuracy 계산\n",
    "        acc_t = compute_accuracy(y_pred,batch_data['y_target'],mask_index)\n",
    "        running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "    \n",
    "    print(\"val_loss\",running_loss)\n",
    "    print(\"val_acc\",running_acc)\n",
    "\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "    \n",
    "\n",
    "#   전체 loss, acc 저장\n",
    "    train_state = update_train_state(model=model,\n",
    "                                     train_state=train_state)\n",
    "#   early stop해라고 했으면 학습 멈추기    \n",
    "    if train_state['stop_early']:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81148c1c",
   "metadata": {},
   "source": [
    "### Test 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c570a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 좋은 모델을 사용해 테스트 세트의 손실과 정확도를 계산합니다\n",
    "\n",
    "model.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "\n",
    "# 가중치 업데이트 하지 못 하게\n",
    "model.eval()\n",
    "\n",
    "for batch_idx, batch_data in enumerate(Testdataloader):\n",
    "    \n",
    "    y_pred = model(x_in=batch_data['x_data'], \n",
    "                   nationality_index=batch_data['class_index'])\n",
    "    \n",
    "    loss = sequence_loss(y_pred,batch_data['y_target'],mask_index)\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "    \n",
    "    acc_t = compute_accuracy(y_pred, batch_data['y_target'],mask_index)\n",
    "    running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4576ed1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 2.794\n",
      "테스트 정확도: 21.25\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 손실: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {:.2f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a501556c",
   "metadata": {},
   "source": [
    "### 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "86fbe523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_samples(sampled_indices):\n",
    "    \"\"\"인덱스를 성씨 문자열로 변환합니다\n",
    "    \n",
    "    매개변수:\n",
    "        sampled_indices (torch.Tensor): `sample_from_model` 함수에서 얻은 인덱스\n",
    "    \"\"\"\n",
    "    decoded_surnames = []\n",
    "    vocab = char_vocab\n",
    "    \n",
    "    for sample_index in range(sampled_indices.shape[0]):\n",
    "        surname = \"\"\n",
    "        for time_step in range(sampled_indices.shape[1]):\n",
    "            sample_item = sampled_indices[sample_index, time_step].item()\n",
    "            if sample_item == vocab.begin_seq_index:\n",
    "                continue\n",
    "            elif sample_item == vocab.end_seq_index:\n",
    "                break\n",
    "            else:\n",
    "                surname += lookup_index(vocab,sample_item)\n",
    "        decoded_surnames.append(surname)\n",
    "    return decoded_surnames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2d0426c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(model, nationalities, sample_size=20, \n",
    "                      temperature=1.0):\n",
    "    \"\"\"모델이 만든 인덱스 시퀀스를 샘플링합니다.\n",
    "    \n",
    "    매개변수:\n",
    "        model (SurnameGenerationModel): 훈련 모델\n",
    "        num_samples (int): 샘플 개수\n",
    "        sample_size (int): 샘플의 최대 길이\n",
    "        temperature (float): 무작위성 정도\n",
    "            0.0 < temperature < 1.0 이면 최대 값을 선택할 가능성이 높습니다\n",
    "            temperature > 1.0 이면 균등 분포에 가깝습니다\n",
    "    반환값:\n",
    "        indices (torch.Tensor): 인덱스 행렬\n",
    "        shape = (num_samples, sample_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    num_samples = len(nationalities)\n",
    "    begin_seq_index = [char_vocab.begin_seq_index \n",
    "                       for _ in range(num_samples)]\n",
    "    begin_seq_index = torch.tensor(begin_seq_index, \n",
    "                                   dtype=torch.int64).unsqueeze(dim=1)\n",
    "    indices = [begin_seq_index]\n",
    "    \n",
    "    nationality_indices = torch.tensor(nationalities, dtype=torch.int64).unsqueeze(dim=0)\n",
    "    h_t = model.nation_emb(nationality_indices)\n",
    "    \n",
    "    h_t = None\n",
    "    \n",
    "    for time_step in range(sample_size):\n",
    "        x_t = indices[time_step]\n",
    "        x_emb_t = model.emb(x_t)\n",
    "        rnn_out_t, h_t = model.rnn(x_emb_t, h_t)\n",
    "        prediction_vector = model.fc(rnn_out_t.squeeze(dim=1))\n",
    "        probability_vector = F.softmax(prediction_vector / temperature, dim=1)\n",
    "        indices.append(torch.multinomial(probability_vector, num_samples=1))\n",
    "    indices = torch.stack(indices).squeeze().permute(1, 0)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c62057cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic 샘플: \n",
      "-  raut\n",
      "-  Talev\n",
      "-  Jasovt\n",
      "Chinese 샘플: \n",
      "-  Belar\n",
      "-  Tatam\n",
      "-  Bsan\n",
      "Czech 샘플: \n",
      "-  Gal\n",
      "-  Van\n",
      "-  Biernenb\n",
      "Dutch 샘플: \n",
      "-  Bafr\n",
      "-  Dnadsa\n",
      "-  Aarst\n",
      "English 샘플: \n",
      "-  Foekot\n",
      "-  Tnirun\n",
      "-  Mar\n",
      "French 샘플: \n",
      "-  Minie\n",
      "-  Pcuikir\n",
      "-  Krinens\n",
      "German 샘플: \n",
      "-  gane\n",
      "-  Jeson\n",
      "-  Mgaer\n",
      "Greek 샘플: \n",
      "-  Cáetonibad\n",
      "-  Mhana\n",
      "-  Klav\n",
      "Irish 샘플: \n",
      "-  Hbeevsn\n",
      "-  Atblee\n",
      "-  Srnoin\n",
      "Italian 샘플: \n",
      "-  Tlorren\n",
      "-  Tuidun\n",
      "-  Bitol\n",
      "Japanese 샘플: \n",
      "-  Glemeo\n",
      "-  Gaska\n",
      "-  Aeenica\n",
      "Korean 샘플: \n",
      "-  Dltsrime\n",
      "-  Bkizery\n",
      "-  Drni\n",
      "Polish 샘플: \n",
      "-  Gaba\n",
      "-  Seagi\n",
      "-  Glillern\n",
      "Portuguese 샘플: \n",
      "-  Hene\n",
      "-  Caelami\n",
      "-  Var\n",
      "Russian 샘플: \n",
      "-  Yter\n",
      "-  Sgioin\n",
      "-  Hhaorbm\n",
      "Scottish 샘플: \n",
      "-  Tani\n",
      "-  Cekrir\n",
      "-  Qanale\n",
      "Spanish 샘플: \n",
      "-  Dalsov\n",
      "-  Grceut\n",
      "-  YMa\n",
      "Vietnamese 샘플: \n",
      "-  Khenikae\n",
      "-  Heada\n",
      "-  Cetene\n"
     ]
    }
   ],
   "source": [
    "model = model.cpu()\n",
    "for index in range(len(nationality_vocab.token_to_idx)):\n",
    "    nationality = lookup_index(nationality_vocab,index)\n",
    "    print(\"{} 샘플: \".format(nationality))\n",
    "    sampled_indices = sample_from_model(model,  \n",
    "                                        nationalities=[index] * 3, \n",
    "                                        temperature=0.7)\n",
    "    for sampled_surname in decode_samples(sampled_indices):\n",
    "        print(\"-  \" + sampled_surname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27698821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ddc8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
