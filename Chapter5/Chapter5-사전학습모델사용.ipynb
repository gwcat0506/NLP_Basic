{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9abe6618",
   "metadata": {},
   "source": [
    "## 예제: 문서 분류에 사전 훈련된 임베딩을 사용한 전이 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ad1892f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>Jobs, tax cuts key issues for Bush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>Jarden Buying Mr. Coffee #39;s Maker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>Retail sales show festive fervour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>Intervoice's Customers Come Calling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>Boeing Expects Air Force Contract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>World</td>\n",
       "      <td>test</td>\n",
       "      <td>Genesis Space Capsule Crashes Into Desert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>World</td>\n",
       "      <td>test</td>\n",
       "      <td>U.S.: Too Early to Tell Iraq Unit's Fate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>World</td>\n",
       "      <td>test</td>\n",
       "      <td>AFGHAN OPIUM GROWING UP TWO THIRDS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>World</td>\n",
       "      <td>test</td>\n",
       "      <td>At least one Saudi policeman killed in clashes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>World</td>\n",
       "      <td>test</td>\n",
       "      <td>U.S. Forces Claim Most of Fallujah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        category  split                                              title\n",
       "0       Business  train                 Jobs, tax cuts key issues for Bush\n",
       "1       Business  train               Jarden Buying Mr. Coffee #39;s Maker\n",
       "2       Business  train                  Retail sales show festive fervour\n",
       "3       Business  train                Intervoice's Customers Come Calling\n",
       "4       Business  train                  Boeing Expects Air Force Contract\n",
       "...          ...    ...                                                ...\n",
       "119995     World   test          Genesis Space Capsule Crashes Into Desert\n",
       "119996     World   test           U.S.: Too Early to Tell Iraq Unit's Fate\n",
       "119997     World   test                 AFGHAN OPIUM GROWING UP TWO THIRDS\n",
       "119998     World   test  At least one Saudi policeman killed in clashes...\n",
       "119999     World   test                 U.S. Forces Claim Most of Fallujah\n",
       "\n",
       "[120000 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"news_with_splits.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa05145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 다시 train/valid/test로 나눠줌\n",
    "\n",
    "# train 데이터 \n",
    "train_df = df[df.split=='train']\n",
    "train_size = len(train_df)\n",
    "\n",
    "# valid 데이터 \n",
    "val_df = df[df.split=='val']\n",
    "val_size = len(val_df)\n",
    "\n",
    "# test 데이터 \n",
    "test_df = df[df.split=='test']\n",
    "test_size = len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e066c664",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dict = {'train': (train_df, train_size),\n",
    "                             'val': (val_df, val_size),\n",
    "                             'test': (test_df, test_size)}\n",
    "# lookup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5109dd35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>Jobs, tax cuts key issues for Bush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>Jarden Buying Mr. Coffee #39;s Maker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>Retail sales show festive fervour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>Intervoice's Customers Come Calling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>Boeing Expects Air Force Contract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110995</th>\n",
       "      <td>World</td>\n",
       "      <td>train</td>\n",
       "      <td>UAE president dies at 86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110996</th>\n",
       "      <td>World</td>\n",
       "      <td>train</td>\n",
       "      <td>Iran says it won #39;t halt nuclear technology...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110997</th>\n",
       "      <td>World</td>\n",
       "      <td>train</td>\n",
       "      <td>Republicans Assail Kerry at Convention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110998</th>\n",
       "      <td>World</td>\n",
       "      <td>train</td>\n",
       "      <td>Muslim envoys off to Baghdad in bid to free Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110999</th>\n",
       "      <td>World</td>\n",
       "      <td>train</td>\n",
       "      <td>UN staff union mulls no-confidence motion agai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        category  split                                              title\n",
       "0       Business  train                 Jobs, tax cuts key issues for Bush\n",
       "1       Business  train               Jarden Buying Mr. Coffee #39;s Maker\n",
       "2       Business  train                  Retail sales show festive fervour\n",
       "3       Business  train                Intervoice's Customers Come Calling\n",
       "4       Business  train                  Boeing Expects Air Force Contract\n",
       "...          ...    ...                                                ...\n",
       "110995     World  train                           UAE president dies at 86\n",
       "110996     World  train  Iran says it won #39;t halt nuclear technology...\n",
       "110997     World  train             Republicans Assail Kerry at Convention\n",
       "110998     World  train  Muslim envoys off to Baghdad in bid to free Br...\n",
       "110999     World  train  UN staff union mulls no-confidence motion agai...\n",
       "\n",
       "[84000 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d30142",
   "metadata": {},
   "source": [
    "### 2. Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fae986f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_unk=True를 하면 '<UNK>': 0 토큰을 추가해줌 !\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", \n",
    "                 begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "        \n",
    "        self.token_to_idx = {}\n",
    "        self.idx_to_token = {}\n",
    "        \n",
    "        self.mask_token = mask_token\n",
    "        self.unk_token = unk_token\n",
    "        self.begin_seq_token = begin_seq_token\n",
    "        self.end_seq_token = end_seq_token\n",
    "\n",
    "        \n",
    "        self.mask_index = self.add_token(self.mask_token)\n",
    "        self.unk_index = self.add_token(self.unk_token)\n",
    "        self.begin_seq_index = self.add_token(self.begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self.end_seq_token)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \n",
    "#       만약 해당 토큰이 있으면 토큰 idx만 return\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "            \n",
    "#       만약 해당 토큰이 없으면 새로운 토큰 만들어줌\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f97c0211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word counter 생성 \n",
    "\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Counter()를 통해 어떤 단어가 얼만큼의 횟수로 들어있는지를 알 수 있다.\n",
    "word_counts = Counter()\n",
    "for t in df.title:\n",
    "    for word in t.split(\" \"):\n",
    "        # word가 .(구두점,punctuation)이 아닐 경우 word에 추가\n",
    "        if word not in string.punctuation:\n",
    "            word_counts[word] += 1\n",
    "\n",
    "# word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9daef709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary 객체 생성\n",
    "\n",
    "# cutoff 보다 수가 많은 단어만 vocab에 추가\n",
    "cutoff = 25\n",
    "vocab = Vocabulary()\n",
    "\n",
    "# word_counts.items() -> ex) ('all', 24160)\n",
    "for word, count in word_counts.items():\n",
    "    if count > cutoff:\n",
    "        vocab.add_token(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "714ba3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<MASK>': 0, '<UNK>': 1, '<BEGIN>': 2, '<END>': 3, 'Jobs,': 4, 'tax': 5, 'cuts': 6, 'key': 7, 'issues': 8, 'for': 9, 'Bush': 10, 'Buying': 11, '#39;s': 12, 'Maker': 13, 'Retail': 14, 'sales': 15, 'show': 16, 'Customers': 17, 'Come': 18, 'Calling': 19}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(vocab.token_to_idx.items())[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "59bd61b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<MASK>', 1: '<UNK>', 2: '<BEGIN>', 3: '<END>', 4: 'Jobs,', 5: 'tax', 6: 'cuts', 7: 'key', 8: 'issues', 9: 'for', 10: 'Bush', 11: 'Buying', 12: '#39;s', 13: 'Maker', 14: 'Retail', 15: 'sales', 16: 'show', 17: 'Customers', 18: 'Come', 19: 'Calling'}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(vocab.idx_to_token.items())[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a876de65",
   "metadata": {},
   "source": [
    "### Category Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5bc9c74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Vocabulary at 0x7fca40678d00>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_vocab = Vocabulary()   \n",
    "category_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "49a5d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in sorted(set(df.category)):\n",
    "    category_vocab.add_token(category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f09bdfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<MASK>': 0, '<UNK>': 1, '<BEGIN>': 2, '<END>': 3, 'Business': 4, 'Sci/Tech': 5, 'Sports': 6, 'World': 7}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(category_vocab.token_to_idx.items())[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d18d7499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<MASK>', 1: '<UNK>', 2: '<BEGIN>', 3: '<END>', 4: 'Business', 5: 'Sci/Tech', 6: 'Sports', 7: 'World'}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(category_vocab.idx_to_token.items())[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ee407f",
   "metadata": {},
   "source": [
    "## 3. Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "166fd3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 토큰에 대응하는 인덱스 반환\n",
    "\n",
    "def lookup_token(vocabulary_class,token):\n",
    "\n",
    "# UNK 토큰이 있을 경우\n",
    "    if vocabulary_class.unk_index >= 0:\n",
    "#           토큰을 찾아보고 없으면 unk_index 반환, 있으면 해당 토큰의 idx를 반환\n",
    "        return vocabulary_class.token_to_idx.get(token, vocabulary_class.unk_index)\n",
    "    else:\n",
    "        return vocabulary_class.token_to_idx[token]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "db2d52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 인덱스에 대응하는 토큰 반환\n",
    "\n",
    "def lookup_index(vocabulary_class, index):\n",
    "        if index not in vocabulary_class.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return vocabulary_class.idx_to_token[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6853d180",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1764d004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2 880   1   1   1 237   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def vectorize(title, vector_length=-1):\n",
    "    \n",
    "    indices = [vocab.begin_seq_index]\n",
    "    indices.extend(lookup_token(vocab,token) for token in title.split(' '))\n",
    "    if vector_length < 0:\n",
    "        vector_length = len(indices)\n",
    "    \n",
    "    out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "    out_vector[:len(indices)] = indices\n",
    "#     문장 길이가 작아서 padding 진행하면 해당 부분 mask 처리\n",
    "    out_vector[len(indices):] = vocab.mask_index\n",
    "    \n",
    "    \n",
    "    return out_vector\n",
    "\n",
    "# 무조건 첫번째 토큰은 2(BEGIN) 토큰이다. \n",
    "print(vectorize(\"all dafs dfkdl dd good\",vector_length=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff427e2",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9d828249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, cbow_df):\n",
    "        \n",
    "        self.cbow_df = cbow_df\n",
    "        \n",
    "        measure_len = lambda title: len(title.split(\" \"))\n",
    "        self.max_seq_length = max(map(measure_len, cbow_df.title))+2 #시작 및 끝 토큰 고려\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cbow_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        row = self.cbow_df.iloc[index]\n",
    "        \n",
    "        title_vector = vectorize(row.title, self.max_seq_length)\n",
    "        category_index = lookup_token(category_vocab,row.category)\n",
    "\n",
    "        return {'x_data': title_vector,\n",
    "                'y_target': category_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "70ee0b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.NewsDataset at 0x7fc9f0972880>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋을 인스턴스화 해주어야 로더에 넣어줄 수 있다. \n",
    "\n",
    "train_dataset = NewsDataset(train_df)\n",
    "train_dataset\n",
    "\n",
    "valid_dataset = NewsDataset(val_df)\n",
    "valid_dataset\n",
    "\n",
    "test_dataset = NewsDataset(test_df)\n",
    "test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d68c4ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더 설정\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# drop_last=True -> 배치 사이즈보다 over하면 drop\n",
    "\n",
    "Traindataloader = DataLoader(dataset=train_dataset, batch_size=512,\n",
    "                            shuffle=True, drop_last=True)\n",
    "\n",
    "Validdataloader = DataLoader(dataset=valid_dataset, batch_size=512,\n",
    "                            shuffle=False, drop_last=True)\n",
    "\n",
    "Testdataloader = DataLoader(dataset=test_dataset, batch_size=512,\n",
    "                            shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "75f43e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84000 164\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset),len(Traindataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "46fdcf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'x_data': tensor([[   2, 3830, 2002,  ...,    0,    0,    0],\n",
      "        [   2,   49,  606,  ...,    0,    0,    0],\n",
      "        [   2,    1, 2414,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2,    1, 3188,  ...,    0,    0,    0],\n",
      "        [   2,  167,    1,  ...,    0,    0,    0],\n",
      "        [   2,    1,  946,  ...,    0,    0,    0]]), 'y_target': tensor([7, 4, 5, 6, 7, 7, 5, 7, 4, 6, 6, 6, 4, 4, 6, 5, 5, 5, 5, 7, 6, 6, 6, 5,\n",
      "        7, 7, 5, 4, 6, 5, 6, 7, 4, 5, 4, 6, 7, 7, 7, 6, 6, 7, 6, 6, 5, 7, 6, 5,\n",
      "        6, 4, 7, 7, 6, 5, 7, 5, 6, 7, 7, 5, 5, 5, 7, 5, 4, 5, 4, 5, 5, 4, 4, 6,\n",
      "        5, 6, 5, 5, 4, 5, 5, 6, 7, 6, 4, 5, 7, 5, 6, 7, 7, 5, 7, 4, 5, 5, 6, 4,\n",
      "        7, 7, 7, 6, 6, 5, 6, 4, 7, 5, 6, 4, 6, 5, 4, 7, 6, 4, 7, 6, 4, 4, 6, 5,\n",
      "        7, 5, 6, 6, 7, 5, 4, 6, 4, 4, 6, 4, 7, 5, 4, 6, 4, 6, 7, 5, 6, 7, 7, 6,\n",
      "        6, 5, 7, 4, 5, 4, 5, 7, 4, 6, 4, 4, 5, 7, 5, 4, 5, 5, 6, 4, 7, 4, 7, 6,\n",
      "        4, 5, 6, 4, 6, 6, 5, 7, 7, 5, 4, 5, 5, 6, 4, 6, 7, 5, 4, 4, 6, 4, 5, 7,\n",
      "        6, 5, 7, 4, 5, 7, 4, 5, 7, 7, 5, 4, 6, 6, 6, 7, 6, 6, 5, 5, 6, 6, 7, 6,\n",
      "        5, 7, 4, 7, 5, 5, 4, 5, 5, 4, 5, 4, 6, 5, 5, 7, 5, 5, 5, 7, 5, 5, 4, 6,\n",
      "        5, 4, 6, 7, 4, 5, 6, 5, 5, 5, 4, 7, 6, 4, 7, 6, 7, 7, 7, 4, 4, 6, 4, 6,\n",
      "        7, 4, 6, 5, 4, 6, 5, 7, 5, 4, 6, 7, 4, 5, 5, 4, 5, 7, 6, 5, 4, 6, 6, 6,\n",
      "        6, 7, 7, 7, 4, 7, 6, 6, 4, 7, 6, 5, 5, 5, 5, 6, 5, 7, 6, 7, 7, 7, 5, 7,\n",
      "        4, 5, 6, 5, 4, 7, 7, 7, 7, 6, 6, 7, 5, 5, 5, 6, 5, 4, 7, 6, 4, 4, 4, 6,\n",
      "        5, 7, 7, 4, 5, 5, 6, 4, 7, 5, 6, 4, 6, 5, 6, 7, 6, 4, 5, 6, 6, 6, 7, 5,\n",
      "        4, 5, 7, 4, 4, 4, 5, 6, 6, 4, 6, 7, 7, 5, 4, 5, 5, 7, 5, 4, 5, 6, 7, 7,\n",
      "        7, 6, 7, 5, 6, 5, 4, 7, 7, 6, 6, 7, 4, 5, 7, 6, 6, 7, 6, 7, 7, 5, 6, 7,\n",
      "        5, 7, 5, 7, 6, 6, 6, 6, 6, 6, 4, 6, 4, 6, 6, 4, 6, 6, 7, 6, 6, 5, 7, 7,\n",
      "        4, 5, 4, 7, 5, 6, 5, 4, 7, 7, 6, 4, 6, 7, 6, 4, 6, 4, 5, 6, 4, 6, 4, 7,\n",
      "        5, 4, 5, 6, 5, 6, 5, 7, 5, 6, 5, 7, 6, 7, 7, 7, 7, 5, 5, 4, 4, 6, 4, 5,\n",
      "        6, 6, 5, 5, 4, 5, 6, 7, 4, 6, 5, 7, 5, 7, 4, 5, 6, 6, 7, 6, 7, 7, 7, 5,\n",
      "        4, 6, 5, 7, 5, 6, 5, 5])}\n"
     ]
    }
   ],
   "source": [
    "for batch_index, batch_dict in enumerate(Traindataloader):\n",
    "    print(batch_index)\n",
    "    print(batch_dict)\n",
    "    \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5144fee",
   "metadata": {},
   "source": [
    "### 모델정의 NewsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d3679e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn은 neural network로 torch의 신경망 모듈이다.\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self, embedding_size, num_embeddings, num_channels,\n",
    "                hidden_dim, num_classes, dropout_p,\n",
    "                pretrained_embeddings=None, padding_idx=0):\n",
    "        \"\"\"\n",
    "            embedding_size (int): 임베딩 벡터의 크기\n",
    "            num_embeddings (int): 임베딩 벡터의 개수\n",
    "            num_channels (int): 합성곱 커널 개수 ex) 32, 64, ..\n",
    "            hidden_dim (int): 은닉 차원 크기\n",
    "            num_classes (int): 클래스 개수\n",
    "            dropout_p (float): 드롭아웃 확률\n",
    "            pretrained_embeddings (numpy.array): 사전에 훈련된 단어 임베딩\n",
    "            padding_idx (int): 패딩 인덱스\n",
    "        \"\"\"\n",
    "        \n",
    "        super(NewsClassifier, self).__init__()\n",
    "        \n",
    "        # 사전 학습 모델 사용여부 \n",
    "        if pretrained_embeddings is None:\n",
    "            self.emb = nn.Embedding(embedding_dim = embedding_size,\n",
    "                                   num_embeddings = num_embeddings,\n",
    "                                   padding_idx = padding_idx)\n",
    "        else:\n",
    "#             pretrained_embeddings -> 사전 학습된 임베딩 matrix\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.emb = nn.Embedding(embedding_dim = embedding_size,\n",
    "                                   num_embeddings=num_embeddings,\n",
    "                                   padding_idx = padding_idx,\n",
    "                                   _weight = pretrained_embeddings)\n",
    "        \n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = embedding_size,\n",
    "                     out_channels = num_channels, kernel_size=3),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels = num_channels, out_channels=num_channels,\n",
    "                     kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels = num_channels, out_channels=num_channels,\n",
    "                     kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels = num_channels, out_channels=num_channels,\n",
    "                     kernel_size=3),\n",
    "            nn.ELU()\n",
    "        \n",
    "        )\n",
    "        self.dropout_p = dropout_p\n",
    "        self.fc1 = nn.Linear(num_channels, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self,x_in, apply_softmax=False):\n",
    "#       permute -> 차원의 순서 바꾸기 \n",
    "#       임베딩 레이어의 출력은 (배치 크기, 시퀀스 길이, 임베딩 크기) 형태\n",
    "#       Conv1d 레이어의 입력은 일반적으로 (배치 크기, 채널 수, 시퀀스 길이) 형태\n",
    "\n",
    "        x_embedded = self.emb(x_in).permute(0,2,1)\n",
    "        features = self.convnet(x_embedded)\n",
    "        \n",
    "        \n",
    "        # 평균값 계산하기 \n",
    "        remaining_size = features.size(dim=2)\n",
    "        features = F.avg_pool1d(features, remaining_size).squeeze(dim=2)\n",
    "        features = F.dropout(features, p=self.dropout_p)\n",
    "        \n",
    "        # MLP classifier\n",
    "        intermediate_vector = F.relu(F.dropout(self.fc1(features), p=self.dropout_p))\n",
    "        prediction_vector = self.fc2(intermediate_vector)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "cb0b8bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original input data shape: torch.Size([32, 50])\n",
      "Embedded data shape after permute: torch.Size([32, 100, 50])\n"
     ]
    }
   ],
   "source": [
    "# 모델 들고와보기 (Permute 왜 하는지 )\n",
    "import torch\n",
    "\n",
    "# 모델 초기화\n",
    "embedding_size = 100\n",
    "num_embeddings = 10000\n",
    "num_channels = 32\n",
    "hidden_dim = 64\n",
    "num_classes = 10\n",
    "dropout_p = 0.5\n",
    "padding_idx = 0\n",
    "\n",
    "model = NewsClassifier(embedding_size, num_embeddings, num_channels, hidden_dim, num_classes, dropout_p)\n",
    "\n",
    "# 임의의 입력 데이터 생성\n",
    "batch_size = 32\n",
    "sequence_length = 50\n",
    "\n",
    "# 임의의 입력 데이터 생성 (배치 크기, 시퀀스 길이)\n",
    "input_data = torch.randint(0, num_embeddings, (batch_size, sequence_length))\n",
    "\n",
    "# 임베딩 후 permute\n",
    "embedded_data = model.emb(input_data).permute(0, 2, 1)\n",
    "\n",
    "print(\"Original input data shape:\", input_data.shape)\n",
    "print(\"Embedded data shape after permute:\", embedded_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b144933b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded data shape: torch.Size([5, 100])\n",
      "Embedded data: tensor([[ 1.2704e-01,  1.7831e-02, -2.3641e-02, -6.5017e-02, -1.2157e-02,\n",
      "          1.5102e-01,  5.3289e-02, -1.4382e-02, -2.0612e-01, -1.4185e-01,\n",
      "         -1.6901e-01, -1.7447e-01, -3.4675e-03, -2.1077e-01, -1.0859e-01,\n",
      "         -1.2947e-01, -1.6271e-01, -8.8284e-02, -1.2512e-01,  1.6890e-01,\n",
      "         -6.2940e-02,  1.2273e-01, -1.4146e-01, -1.7785e-01,  2.3973e-03,\n",
      "         -6.0074e-02,  9.0401e-02, -3.3368e-03, -8.4645e-02, -1.4040e-01,\n",
      "          2.0587e-01,  1.8994e-01,  1.2982e-01, -1.9011e-01, -1.7127e-01,\n",
      "         -1.5373e-01, -8.6781e-02,  3.0449e-02, -2.2515e-02, -2.2082e-01,\n",
      "         -4.5941e-02, -1.0774e-01, -1.8684e-01,  1.4941e-01, -1.9609e-01,\n",
      "         -2.1336e-01, -1.4369e-01,  1.8261e-01, -1.2049e-01, -2.3666e-01,\n",
      "          1.6654e-01,  1.1389e-01,  1.4494e-01,  2.9041e-02,  1.5172e-02,\n",
      "          8.0321e-02, -5.0443e-02,  2.1568e-01,  1.3720e-01, -1.1069e-01,\n",
      "          1.2359e-01,  1.2072e-01, -2.3217e-02,  2.0497e-01, -1.8290e-01,\n",
      "          1.9316e-01,  1.0233e-01,  1.4358e-01,  1.5903e-01, -2.9355e-02,\n",
      "          2.1774e-02, -1.0100e-01, -1.9809e-01, -9.5828e-02,  8.1047e-02,\n",
      "          1.2742e-01, -7.8497e-02,  1.3998e-01,  1.9965e-01,  3.1400e-02,\n",
      "         -6.6568e-02, -1.3865e-01, -1.0171e-01,  1.8958e-01, -2.1216e-01,\n",
      "         -1.4417e-01, -8.0509e-02, -5.6493e-02, -3.4010e-02, -2.1155e-01,\n",
      "          2.6047e-02,  1.9053e-01, -1.1657e-01, -1.9818e-01, -1.2294e-01,\n",
      "          7.5485e-02,  9.9435e-02, -1.7236e-01,  1.7594e-01, -8.8237e-02],\n",
      "        [ 1.0937e-01, -1.2055e-02,  1.8371e-01,  1.5647e-01,  2.1314e-01,\n",
      "          2.2461e-01, -1.6312e-01, -3.9171e-02, -1.3636e-01, -7.7871e-02,\n",
      "         -1.8290e-01,  2.1659e-01,  7.4238e-02,  1.4408e-01,  2.3855e-01,\n",
      "         -2.2772e-01,  2.0104e-02, -5.9112e-02,  1.0473e-02, -8.0256e-03,\n",
      "          1.9176e-01, -1.2405e-01, -1.2258e-01,  6.9156e-02,  7.9506e-02,\n",
      "          5.2681e-02, -8.3929e-02, -2.4193e-01, -4.8023e-02,  1.9301e-01,\n",
      "         -2.0752e-01, -2.1291e-01,  1.1645e-01, -9.5752e-02, -1.4635e-02,\n",
      "          5.6428e-03, -6.5377e-02, -2.3263e-01, -3.6715e-02, -3.7472e-02,\n",
      "         -3.9768e-02,  5.7588e-02, -2.5112e-02,  2.3888e-01,  7.5000e-02,\n",
      "          1.3205e-02,  2.0805e-01, -5.2648e-02, -2.2494e-02,  1.9728e-01,\n",
      "          1.4463e-01, -4.0088e-02, -1.6982e-01,  8.2705e-02,  2.0481e-01,\n",
      "         -2.0712e-01, -2.2176e-01, -1.3729e-01, -1.2897e-01, -1.1807e-01,\n",
      "         -1.6318e-01,  1.3480e-01, -7.5736e-02,  1.3360e-01,  1.0406e-01,\n",
      "         -6.0181e-02,  1.0342e-01,  1.4051e-01,  1.1733e-01, -8.9565e-02,\n",
      "         -2.2198e-01, -9.5507e-02,  1.2137e-01, -2.0948e-01,  9.0281e-02,\n",
      "          2.2263e-01,  3.6869e-02, -2.0324e-01,  1.1636e-01,  2.3373e-01,\n",
      "         -1.0377e-01, -8.4102e-02,  1.0539e-01,  1.9918e-01, -5.0494e-02,\n",
      "          1.4447e-01,  1.7972e-01,  8.7776e-02,  1.3149e-01, -8.2083e-02,\n",
      "          9.9671e-03,  1.8932e-01, -6.7929e-02,  1.9148e-01, -6.6904e-03,\n",
      "          1.4536e-01, -1.6461e-01, -4.9644e-02,  1.2776e-01, -1.4490e-01],\n",
      "        [ 2.2384e-01,  1.0732e-01, -1.3665e-02,  1.0642e-01,  1.4331e-01,\n",
      "          1.0166e-01,  1.3124e-01, -2.3405e-02,  2.1654e-01, -5.4026e-02,\n",
      "          2.3001e-02, -9.1685e-02,  2.2520e-01, -1.7381e-02,  2.1895e-01,\n",
      "          2.3985e-01,  2.3394e-01,  1.3723e-01,  9.5213e-02,  2.5710e-02,\n",
      "         -1.0145e-01, -1.1873e-01, -1.5310e-01,  2.3335e-01,  5.7887e-02,\n",
      "          9.4033e-02,  1.2088e-01,  2.1913e-01, -1.0423e-02, -8.1173e-03,\n",
      "         -3.8099e-02, -1.2004e-01,  1.9489e-03, -1.0147e-01,  1.8887e-01,\n",
      "         -9.1426e-02, -7.5240e-02, -1.1855e-01, -1.9106e-01,  2.2641e-01,\n",
      "          2.1110e-01,  1.8904e-01, -8.8265e-02,  6.7731e-02,  2.1807e-01,\n",
      "         -2.1432e-01, -1.6351e-01, -2.3420e-01, -3.0843e-02, -9.7460e-02,\n",
      "          1.9463e-01,  3.8076e-02,  2.0265e-01,  2.1496e-01, -1.3240e-02,\n",
      "         -6.4044e-04, -9.0158e-02, -5.3128e-02, -1.8016e-01,  9.5450e-02,\n",
      "          1.2799e-01,  1.2155e-01,  1.0930e-01,  1.5519e-01,  7.8930e-02,\n",
      "          6.6769e-02,  3.5894e-02,  1.1138e-01,  2.1922e-01, -9.1672e-02,\n",
      "         -1.2945e-01, -1.7831e-01, -1.7857e-01,  8.6485e-02,  1.1990e-01,\n",
      "         -9.7305e-02, -2.4062e-01,  1.4106e-01, -1.9957e-01,  5.0969e-02,\n",
      "          1.0388e-01,  2.0323e-01,  3.9843e-02, -1.3636e-01,  1.2030e-01,\n",
      "         -1.9180e-01, -1.1437e-02, -5.7148e-02, -1.3258e-01, -7.2945e-02,\n",
      "          1.4083e-01,  8.4751e-03, -1.6398e-01, -1.0794e-01,  4.7209e-02,\n",
      "         -2.0800e-01, -1.5167e-01, -2.0936e-01,  1.0845e-01, -1.9958e-01],\n",
      "        [-1.3120e-01, -2.6776e-02,  1.6432e-01,  6.8258e-02, -7.9213e-02,\n",
      "         -1.6366e-01, -1.8825e-03, -2.1351e-01, -1.1158e-01, -2.1737e-01,\n",
      "         -1.6553e-01,  2.3245e-01,  2.8837e-02,  4.4525e-02,  2.4042e-01,\n",
      "          2.1629e-01, -3.4005e-02, -1.9934e-01,  1.5837e-01, -5.8608e-02,\n",
      "         -1.2815e-01, -9.1233e-02, -3.0622e-02, -4.0909e-02,  1.6058e-02,\n",
      "         -7.7365e-02, -1.9487e-01, -2.6287e-03,  2.5812e-02, -2.3178e-01,\n",
      "          2.2009e-04,  1.6887e-02, -9.0132e-02,  2.8018e-02, -1.6187e-02,\n",
      "         -1.0738e-01, -1.1909e-01, -1.9704e-02, -1.1597e-01,  1.7043e-01,\n",
      "         -2.3291e-01,  8.8829e-02, -2.1885e-01,  3.0337e-02,  4.2589e-02,\n",
      "         -1.4349e-01,  2.0936e-01,  7.3521e-02, -1.6214e-01,  5.3609e-02,\n",
      "          1.9965e-01, -3.4697e-02, -1.7215e-02, -3.1987e-02,  1.8415e-01,\n",
      "          1.6188e-01,  1.1694e-01,  2.2829e-01,  8.4948e-02,  2.2403e-01,\n",
      "         -1.4978e-02,  1.9402e-01, -2.0975e-01, -2.0381e-01,  1.1186e-01,\n",
      "         -1.4356e-01,  6.1679e-02,  1.7485e-01, -1.3386e-02, -1.4147e-02,\n",
      "         -1.9436e-01,  7.2584e-02,  5.6494e-02, -6.1986e-02,  1.3508e-01,\n",
      "         -4.8682e-03,  1.6148e-01, -5.9127e-02, -2.0055e-01,  8.1399e-02,\n",
      "         -1.5899e-01, -1.5956e-01,  1.2135e-01, -1.1299e-01, -3.5152e-03,\n",
      "          1.8064e-02, -1.5570e-02,  1.7754e-01,  1.1752e-01,  1.1100e-01,\n",
      "         -1.5254e-01,  2.1344e-01, -7.3684e-03,  1.7082e-01,  1.3611e-01,\n",
      "         -1.2746e-01,  1.1169e-01, -1.4867e-01,  8.3657e-02,  1.2942e-01],\n",
      "        [-3.2956e-01,  2.5699e-01,  2.6541e-01, -6.3736e-01,  7.4103e-01,\n",
      "         -1.5453e-01, -7.3308e-01,  1.3252e-01,  1.9054e-01,  1.9857e-01,\n",
      "         -5.1303e-02,  4.0070e-01, -1.1451e+00,  3.2768e-02,  4.3573e-02,\n",
      "         -4.0231e-01,  4.8110e-01,  1.4148e-01, -3.8208e-01,  6.0523e-01,\n",
      "          3.2672e-01,  8.4854e-02, -2.4518e-01,  6.8121e-01, -8.1055e-01,\n",
      "         -5.1370e-01, -2.7855e-01, -9.4418e-01, -1.2328e+00, -7.3506e-01,\n",
      "          5.7032e-01,  7.1864e-01, -1.4554e-01, -8.1636e-01, -4.3756e-01,\n",
      "          6.1806e-01, -2.7640e-01, -6.1009e-01,  1.9030e-01, -6.4401e-02,\n",
      "         -1.9012e-01, -1.2941e+00,  5.5740e-01,  2.7678e-01, -4.3030e-01,\n",
      "         -1.0283e+00, -1.6881e-01, -4.9758e-01, -5.4861e-01, -1.0429e+00,\n",
      "          7.0316e-02, -3.8690e-01,  1.2275e-02,  8.2921e-01, -5.6201e-01,\n",
      "         -1.5673e+00,  1.0839e+00, -4.4237e-01,  2.5143e+00,  4.0023e-01,\n",
      "          3.9777e-01, -7.6208e-01, -4.3056e-01,  1.4260e-01,  8.9602e-01,\n",
      "         -5.6392e-01,  3.3228e-02, -2.2274e-01,  7.4508e-01, -1.2104e+00,\n",
      "          1.9206e-01, -3.7258e-01, -2.8101e-01,  2.7334e-01, -6.1552e-01,\n",
      "         -1.5219e-01,  1.0700e-01, -3.1498e-01, -9.7012e-01,  3.1000e-01,\n",
      "          1.0024e+00,  2.7707e-01,  4.3465e-01, -1.0205e+00, -1.1356e+00,\n",
      "         -3.3700e-01, -2.3929e-01, -7.0861e-01, -5.8425e-01, -9.1297e-01,\n",
      "         -4.7036e-01, -6.4252e-01, -4.8331e-01, -5.7680e-01, -3.4598e-01,\n",
      "         -2.1542e-01,  9.1034e-01,  9.1924e-02,  1.0854e+00, -9.8642e-01]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 사전 학습된 임베딩 matrix 불러오기 \n",
    "words = vocab.token_to_idx.keys()\n",
    "pretrained_embeddings = make_embedding_matrix(glove_filepath='glove.6B.100d.txt', \n",
    "                                   words=words)\n",
    "pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()  # NumPy 배열을 텐서로 변환\n",
    "\n",
    "# 예시로 임의의 데이터 생성\n",
    "input_data = torch.tensor([1, 2, 3, 4, 5])  # 임의의 데이터 (배치 크기 1)\n",
    "\n",
    "# 임베딩 레이어 정의\n",
    "embedding_size = 100  # 임베딩 차원 크기\n",
    "num_embeddings = len(words)  # 임베딩 벡터의 개수\n",
    "padding_idx = 0  # 패딩에 사용될 인덱스\n",
    "\n",
    "\n",
    "emb = nn.Embedding(embedding_dim=embedding_size,\n",
    "                         num_embeddings=num_embeddings,\n",
    "                         padding_idx=padding_idx,\n",
    "                         _weight=pretrained_embeddings)\n",
    "\n",
    "# 임베딩 레이어에 입력 데이터 통과\n",
    "embedded_data = emb(input_data)\n",
    "\n",
    "print(\"Embedded data shape:\", embedded_data.shape)\n",
    "print(\"Embedded data:\", embedded_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ddd5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embedding_matrix(glove_filepath, words):\n",
    "    \"\"\"\n",
    "    특정 단어 집합에 대한 임베딩 행렬을 만듭니다.\n",
    "    \n",
    "    매개변수:\n",
    "        glove_filepath (str): 임베딩 파일 경로\n",
    "        words (list): 단어 리스트\n",
    "    \"\"\"\n",
    "    \n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "\n",
    "    with open(glove_filepath, \"r\") as f:\n",
    "        for index, line in enumerate(f):\n",
    "            line = line.split(\" \") # each line: word num1 num2 ...\n",
    "            word_to_index[line[0]] = index # word = line[0] \n",
    "            embedding_i = np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "\n",
    "    glove_embeddings = np.stack(embeddings)\n",
    "    embedding_size = glove_embeddings.shape[1]\n",
    "    \n",
    "    final_embeddings = np.zeros((len(words), embedding_size))\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_index:\n",
    "            final_embeddings[i, :] = glove_embeddings[word_to_index[word]]\n",
    "        else:\n",
    "            embedding_i = torch.ones(1, embedding_size)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)\n",
    "            final_embeddings[i, :] = embedding_i\n",
    "\n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abeab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시\n",
    "words = vocab.token_to_idx.keys()\n",
    "embeddings = make_embedding_matrix(glove_filepath='glove.6B.100d.txt', \n",
    "                                       words=words)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5c47fa96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(category_vocab.token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c2794ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사전 훈련된 임베딩을 사용합니다\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NewsClassifier(\n",
       "  (emb): Embedding(4794, 100, padding_idx=0)\n",
       "  (convnet): Sequential(\n",
       "    (0): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Conv1d(100, 100, kernel_size=(3,), stride=(2,))\n",
       "    (3): ELU(alpha=1.0)\n",
       "    (4): Conv1d(100, 100, kernel_size=(3,), stride=(2,))\n",
       "    (5): ELU(alpha=1.0)\n",
       "    (6): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
       "    (7): ELU(alpha=1.0)\n",
       "  )\n",
       "  (fc1): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_glove = True\n",
    "embedding_size = 100\n",
    "num_channels = 100\n",
    "hidden_dim = 100\n",
    "dropout_p = 0.1\n",
    "\n",
    "# GloVe를 사용하거나 랜덤하게 임베딩을 초기화합니다\n",
    "if use_glove:\n",
    "    words = vocab.token_to_idx.keys()\n",
    "    embeddings = make_embedding_matrix(glove_filepath='glove.6B.100d.txt', \n",
    "                                       words=words)\n",
    "    print(\"사전 훈련된 임베딩을 사용합니다\")\n",
    "else:\n",
    "    printv(\"사전 훈련된 임베딩을 사용하지 않습니다\")\n",
    "    embeddings = None\n",
    "\n",
    "classifier = NewsClassifier(embedding_size=embedding_size, \n",
    "                            num_embeddings=len(vocab.token_to_idx),\n",
    "                            num_channels=num_channels,\n",
    "                            hidden_dim=hidden_dim, \n",
    "                            num_classes=len(category_vocab.token_to_idx), \n",
    "                            dropout_p=dropout_p,\n",
    "                            pretrained_embeddings=embeddings,\n",
    "                            padding_idx=0)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47ca78f",
   "metadata": {},
   "source": [
    "### 옵티마이저, loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2c316f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "364fd858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 옵티마이저\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = lr)\n",
    "optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "21621796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eb84c5",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ad6a478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "#      예측값과 타겟값을 비교하여 일치하는 개수를 계산\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "75993599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train state 초기화 \n",
    "def make_train_state():\n",
    "    return {\n",
    "        'stop_early':False,\n",
    "        'early_stopping_step':0,\n",
    "        'early_stopping_best_val':1e8,\n",
    "        'early_stopping_criteria' : 10,\n",
    "        'epoch_index' : 0,\n",
    "        'train_loss': [], \n",
    "        'train_acc' :[], \n",
    "        'val_loss' : [],\n",
    "        'val_acc' : [], \n",
    "        'test_loss' : [],\n",
    "        'test_acc' : [],\n",
    "         \n",
    "#       모델 저장파일\n",
    "        'model_filename' : 'model.pth'\n",
    "    } \n",
    "\n",
    "\n",
    "# Train update \n",
    "def update_train_state(model, train_state):\n",
    "    \n",
    "#   학습시작하면 초기에 모델 저장하기 \n",
    "    \n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(),train_state['model_filename'])\n",
    "        \n",
    "#   모델 성능이 향상되면 모델 저장(valid loss가 더 낮아지면)\n",
    "    elif train_state['epoch_index'] >=1 :\n",
    "        loss_t = train_state['val_loss'][-1]\n",
    "#        loss가 나빠지면 early stop step 업데이트\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            train_state['early_stopping_step']+=1\n",
    "            \n",
    "#        loss가 좋아지면   \n",
    "        else:\n",
    "#            early stop step 0으로 다시 초기화        \n",
    "            train_state['early_stopping_step']=0\n",
    "    \n",
    "#           최저 loss이면 모델 저장 \n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "                torch.save(model.state_dict(),train_state['model_filename'])\n",
    "\n",
    "#       기준점 넘으면 early stop \n",
    "        if train_state['early_stopping_step'] >= train_state['early_stopping_criteria']:\n",
    "            train_state['stop_early'] = True\n",
    "        \n",
    "        return train_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4d04a2e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop_early': False,\n",
       " 'early_stopping_step': 0,\n",
       " 'early_stopping_best_val': 100000000.0,\n",
       " 'early_stopping_criteria': 10,\n",
       " 'epoch_index': 0,\n",
       " 'train_loss': [],\n",
       " 'train_acc': [],\n",
       " 'val_loss': [],\n",
       " 'val_acc': [],\n",
       " 'test_loss': [],\n",
       " 'test_acc': [],\n",
       " 'model_filename': 'model.pth'}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 진행 상황 함수 초기화\n",
    "train_state = make_train_state()\n",
    "train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c502840a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                          | 1/100 [00:15<26:12, 15.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 0.9191021455185752\n",
      "val_acc 62.31026785714285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▊                                          | 2/100 [00:32<26:20, 16.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 0.7589134518589293\n",
      "val_acc 71.22209821428574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|█▎                                         | 3/100 [00:47<25:30, 15.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 0.6675949322325841\n",
      "val_acc 74.96651785714283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|█▋                                         | 4/100 [01:03<25:06, 15.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 0.6571825223309652\n",
      "val_acc 75.234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|██▏                                        | 5/100 [01:18<24:42, 15.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 0.6320760582174573\n",
      "val_acc 75.9933035714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|██▌                                        | 6/100 [01:35<25:09, 16.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 0.67661657844271\n",
      "val_acc 74.55357142857142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|███                                        | 7/100 [01:50<24:33, 15.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 0.6571794024535588\n",
      "val_acc 75.20647321428572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|███▍                                       | 8/100 [02:06<24:07, 15.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 0.6180685698986055\n",
      "val_acc 76.46205357142858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███▊                                       | 9/100 [02:21<23:34, 15.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 0.616837908540453\n",
      "val_acc 76.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▏                                     | 10/100 [02:40<25:00, 16.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 0.6654306109462467\n",
      "val_acc 74.7098214285714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████▌                                     | 11/100 [03:00<26:08, 17.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 0.6410550892353059\n",
      "val_acc 75.93749999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████                                     | 12/100 [03:15<24:50, 16.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 0.6652209094592504\n",
      "val_acc 74.97767857142858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█████▍                                    | 13/100 [03:31<23:51, 16.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 0.6656059878213065\n",
      "val_acc 74.94977678571428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█████▉                                    | 14/100 [03:46<23:12, 16.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 0.6555714087826866\n",
      "val_acc 75.53571428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|██████▎                                   | 15/100 [04:02<22:39, 15.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 0.7289681877408708\n",
      "val_acc 73.90066964285715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|██████▋                                   | 16/100 [04:17<22:07, 15.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 0.7440870612859726\n",
      "val_acc 72.53348214285712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████▏                                  | 17/100 [04:33<21:45, 15.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 0.7328091310603279\n",
      "val_acc 73.13058035714285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|███████▌                                  | 18/100 [04:48<21:20, 15.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 0.7174163673605237\n",
      "val_acc 74.09598214285714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|███████▌                                  | 18/100 [05:04<23:09, 16.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 0.7390894676957813\n",
      "val_acc 72.9575892857143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "# 에포크만큼\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "\n",
    "#     print('epoch',epoch)\n",
    "#     print(train_state['epoch_index']) \n",
    "    train_state['epoch_index'] +=1 \n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "\n",
    "#     모델을 학습 모드로 설정 -> 드롭아웃 및 배치 정규화와 같은 학습 중에만 적용되는 기법들이 활성화\n",
    "#     모델을 평가 모드로 전환하려면 classifier.eval()을 사용\n",
    "    classifier.train()\n",
    "# 배치 만큼\n",
    "    for batch_idx, batch_data in enumerate(Traindataloader):\n",
    "\n",
    "        \n",
    "\n",
    "#       1. 옵티마이저 그레디언트 0으로 초기화\n",
    "        optimizer.zero_grad()\n",
    "#       2. 모델에 데이터 넣어서 출력받기\n",
    "        y_pred = classifier(x_in=batch_data['x_data'])\n",
    "#       3. loss 계산하기\n",
    "        loss =  loss_func(y_pred, batch_data['y_target'])\n",
    "    \n",
    "#       tensor(0.3190) -> 0.3190, item()으로 스칼라 값만 추출\n",
    "        loss_t = loss.item()\n",
    "\n",
    "#       배치에서의 평균 loss 구하기\n",
    "        running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "\n",
    "#       4. gradient 계산하기\n",
    "        loss.backward()\n",
    "\n",
    "#       5. 옵티마이저 가중치 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "#       Accuracy 계산\n",
    "        acc_t = compute_accuracy(y_pred, batch_data['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "\n",
    "\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "\n",
    "\n",
    "#   valid에 대한 계산\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "    classifier.eval() # 모델 파라미터를 수정하지 못 하게 비활성화\n",
    "\n",
    "    for batch_idx, batch_data in enumerate(Validdataloader):\n",
    "\n",
    "#       1. 모델의 출력값(y_pred)계산\n",
    "        y_pred = classifier(x_in=batch_data['x_data'])\n",
    "\n",
    "#       2. loss 계산\n",
    "        loss = loss_func(y_pred,batch_data['y_target'])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "\n",
    "#       3. Accuracy 계산\n",
    "        acc_t = compute_accuracy(y_pred,batch_data['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "    \n",
    "    print(\"val_loss\",running_loss)\n",
    "    print(\"val_acc\",running_acc)\n",
    "\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "    \n",
    "\n",
    "#   전체 loss, acc 저장\n",
    "    train_state = update_train_state(model=classifier,\n",
    "                                     train_state=train_state)\n",
    "#   early stop해라고 했으면 학습 멈추기    \n",
    "    if train_state['stop_early']:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bc6c50",
   "metadata": {},
   "source": [
    "### Test 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4a6a4fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 좋은 모델을 사용해 테스트 세트의 손실과 정확도를 계산합니다\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "\n",
    "# 가중치 업데이트 하지 못 하게\n",
    "classifier.eval()\n",
    "\n",
    "for batch_idx, batch_data in enumerate(Testdataloader):\n",
    "    \n",
    "    y_pred = classifier(x_in=batch_data['x_data'])\n",
    "    loss = loss_func(y_pred,batch_data['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "    \n",
    "    acc_t = compute_accuracy(y_pred, batch_data['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "46446a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 0.519\n",
      "테스트 정확도: 80.97\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 손실: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {:.2f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a6278deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop_early': True,\n",
       " 'early_stopping_step': 10,\n",
       " 'early_stopping_best_val': 0.616837908540453,\n",
       " 'early_stopping_criteria': 10,\n",
       " 'epoch_index': 19,\n",
       " 'train_loss': [1.07824518913176,\n",
       "  0.5811618224876679,\n",
       "  0.5040205122857562,\n",
       "  0.470424688080462,\n",
       "  0.45024730646755634,\n",
       "  0.4311915592449466,\n",
       "  0.4177757503419388,\n",
       "  0.40504673286909015,\n",
       "  0.38667869186256004,\n",
       "  0.3764421701794717,\n",
       "  0.3623963018379561,\n",
       "  0.34775477011756195,\n",
       "  0.3371951899877407,\n",
       "  0.32293680010408904,\n",
       "  0.3086075559258461,\n",
       "  0.29571367518567454,\n",
       "  0.2822118337016279,\n",
       "  0.2710341033412189,\n",
       "  0.25818121651323805],\n",
       " 'train_acc': [53.102372332317074,\n",
       "  78.55492568597563,\n",
       "  81.63586128048784,\n",
       "  82.81369092987806,\n",
       "  83.5675495426829,\n",
       "  84.10227705792684,\n",
       "  84.65844131097563,\n",
       "  85.0550209603659,\n",
       "  85.80530678353665,\n",
       "  86.08755716463415,\n",
       "  86.70803163109753,\n",
       "  87.17606707317071,\n",
       "  87.60956554878051,\n",
       "  88.17644817073166,\n",
       "  88.70879382621953,\n",
       "  89.1946932164634,\n",
       "  89.7246570121951,\n",
       "  90.06764481707313,\n",
       "  90.57140815548783],\n",
       " 'val_loss': [0.9191021455185752,\n",
       "  0.7589134518589293,\n",
       "  0.6675949322325841,\n",
       "  0.6571825223309652,\n",
       "  0.6320760582174573,\n",
       "  0.67661657844271,\n",
       "  0.6571794024535588,\n",
       "  0.6180685698986055,\n",
       "  0.616837908540453,\n",
       "  0.6654306109462467,\n",
       "  0.6410550892353059,\n",
       "  0.6652209094592504,\n",
       "  0.6656059878213065,\n",
       "  0.6555714087826866,\n",
       "  0.7289681877408708,\n",
       "  0.7440870612859726,\n",
       "  0.7328091310603279,\n",
       "  0.7174163673605237,\n",
       "  0.7390894676957813],\n",
       " 'val_acc': [62.31026785714285,\n",
       "  71.22209821428574,\n",
       "  74.96651785714283,\n",
       "  75.234375,\n",
       "  75.9933035714286,\n",
       "  74.55357142857142,\n",
       "  75.20647321428572,\n",
       "  76.46205357142858,\n",
       "  76.5625,\n",
       "  74.7098214285714,\n",
       "  75.93749999999999,\n",
       "  74.97767857142858,\n",
       "  74.94977678571428,\n",
       "  75.53571428571429,\n",
       "  73.90066964285715,\n",
       "  72.53348214285712,\n",
       "  73.13058035714285,\n",
       "  74.09598214285714,\n",
       "  72.9575892857143],\n",
       " 'test_loss': 0.5185964116028379,\n",
       " 'test_acc': 80.96540178571428,\n",
       " 'model_filename': 'model.pth'}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d01434",
   "metadata": {},
   "source": [
    "### 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "996190a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 리뷰 텍스트를 전처리합니다\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0e54fa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(title, classifier, max_length):\n",
    "    \"\"\"뉴스 제목을 기반으로 카테고리를 예측합니다\n",
    "    \n",
    "    매개변수:\n",
    "        title (str): 원시 제목 문자열\n",
    "        classifier (NewsClassifier): 훈련된 분류기 객체\n",
    "        vectorizer (NewsVectorizer): 해당 Vectorizer\n",
    "        max_length (int): 최대 시퀀스 길이\n",
    "            노트: CNN은 입력 텐서 크기에 민감합니다. \n",
    "                 훈련 데이터처럼 동일한 크기를 갖도록 만듭니다.\n",
    "    \"\"\"\n",
    "#   전처리 \n",
    "    title = preprocess_text(title)\n",
    "#   벡터화 \n",
    "    vectorized_title = torch.tensor(vectorize(title, vector_length=max_length))\n",
    "#   모델에 적용\n",
    "#   추론과정에서는 loss function이 쓰이지 않기 때문에 소프트맥스를 사용\n",
    "    result = classifier(vectorized_title.unsqueeze(0), apply_softmax=True)\n",
    "\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    predicted_category = lookup_index(category_vocab,indices.item())\n",
    "\n",
    "    return {'category': predicted_category, \n",
    "            'probability': probability_values.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b0c00bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Business': ['AZ suspends marketing of cancer drug',\n",
       "  'Business world has mixed reaction to Perez move',\n",
       "  'Betting Against Bombay',\n",
       "  'Malpractice Insurers Face a Tough Market',\n",
       "  'NVIDIA Is Vindicated'],\n",
       " 'Sci/Tech': ['Spies prize webcam #39;s eyes',\n",
       "  'Sober worm causes headaches',\n",
       "  'Local Search: Missing Pieces Falling into Place',\n",
       "  'Hackers baiting Internet users with Beckham pix',\n",
       "  'Nokia adds BlackBerry support to Series 80 handsets'],\n",
       " 'Sports': ['Is Meyer the man to get Irish up?',\n",
       "  'Who? Who? And Clemens',\n",
       "  'Baseball Today (AP)',\n",
       "  'Mark Kreidler: Yao Ming epitomizes the Chinese athlete who is &lt;b&gt;...&lt;/b&gt;',\n",
       "  'No. 5 Miami Rebounds to Beat FSU in Overtime'],\n",
       " 'World': ['Arafat in pain but expected to recover-Shaath',\n",
       "  'Maoist rebels bomb Kathmandu building, no injuries (Reuters)',\n",
       "  \"Son Running for Ill. Rep.'s House Seat (AP)\",\n",
       "  'Strong Quake Hits in Japan',\n",
       "  'Israel assassinates Hamas militant in Damascus']}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_samples():\n",
    "    samples = {}\n",
    "    for cat in val_df.category.unique():\n",
    "        samples[cat] = val_df.title[val_df.category==cat].tolist()[:5]\n",
    "    return samples\n",
    "\n",
    "val_samples = get_samples()\n",
    "val_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9306e6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Category: Business\n",
      "==============================\n",
      "예측: World (p=0.56)\n",
      "\t + 샘플: AZ suspends marketing of cancer drug\n",
      "예측: Business (p=0.37)\n",
      "\t + 샘플: Business world has mixed reaction to Perez move\n",
      "예측: Sports (p=0.77)\n",
      "\t + 샘플: Betting Against Bombay\n",
      "예측: Business (p=0.74)\n",
      "\t + 샘플: Malpractice Insurers Face a Tough Market\n",
      "예측: Sports (p=0.78)\n",
      "\t + 샘플: NVIDIA Is Vindicated\n",
      "------------------------------\n",
      "\n",
      "True Category: Sci/Tech\n",
      "==============================\n",
      "예측: Sci/Tech (p=0.30)\n",
      "\t + 샘플: Spies prize webcam #39;s eyes\n",
      "예측: Sci/Tech (p=0.77)\n",
      "\t + 샘플: Sober worm causes headaches\n",
      "예측: Sports (p=0.69)\n",
      "\t + 샘플: Local Search: Missing Pieces Falling into Place\n",
      "예측: Sci/Tech (p=0.98)\n",
      "\t + 샘플: Hackers baiting Internet users with Beckham pix\n",
      "예측: Sports (p=0.85)\n",
      "\t + 샘플: Nokia adds BlackBerry support to Series 80 handsets\n",
      "------------------------------\n",
      "\n",
      "True Category: Sports\n",
      "==============================\n",
      "예측: Sports (p=0.48)\n",
      "\t + 샘플: Is Meyer the man to get Irish up?\n",
      "예측: Sports (p=0.72)\n",
      "\t + 샘플: Who? Who? And Clemens\n",
      "예측: Sports (p=0.99)\n",
      "\t + 샘플: Baseball Today (AP)\n",
      "예측: World (p=0.44)\n",
      "\t + 샘플: Mark Kreidler: Yao Ming epitomizes the Chinese athlete who is &lt;b&gt;...&lt;/b&gt;\n",
      "예측: Sports (p=0.86)\n",
      "\t + 샘플: No. 5 Miami Rebounds to Beat FSU in Overtime\n",
      "------------------------------\n",
      "\n",
      "True Category: World\n",
      "==============================\n",
      "예측: Business (p=0.35)\n",
      "\t + 샘플: Arafat in pain but expected to recover-Shaath\n",
      "예측: World (p=0.78)\n",
      "\t + 샘플: Maoist rebels bomb Kathmandu building, no injuries (Reuters)\n",
      "예측: World (p=0.66)\n",
      "\t + 샘플: Son Running for Ill. Rep.'s House Seat (AP)\n",
      "예측: Sports (p=0.33)\n",
      "\t + 샘플: Strong Quake Hits in Japan\n",
      "예측: World (p=0.97)\n",
      "\t + 샘플: Israel assassinates Hamas militant in Damascus\n",
      "------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "#title = input(\"Enter a news title to classify: \")\n",
    "\n",
    "measure_len = lambda title: len(title.split(\" \"))\n",
    "max_seq_length = max(map(measure_len, df.title))+2 #시작 및 끝 토큰 고려\n",
    "\n",
    "\n",
    "for truth, sample_group in val_samples.items():\n",
    "    print(f\"True Category: {truth}\")\n",
    "    print(\"=\"*30)\n",
    "    for sample in sample_group:\n",
    "        prediction = predict_category(sample, classifier, \n",
    "                                      max_seq_length + 1)\n",
    "        print(\"예측: {} (p={:0.2f})\".format(prediction['category'],\n",
    "                                                  prediction['probability']))\n",
    "        print(\"\\t + 샘플: {}\".format(sample))\n",
    "    print(\"-\"*30 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a30db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9560836c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
