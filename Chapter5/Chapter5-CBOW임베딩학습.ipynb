{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9abe6618",
   "metadata": {},
   "source": [
    "# 예제: CBOW 임베딩 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad1892f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>, or the</td>\n",
       "      <td>frankenstein</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frankenstein or the modern</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frankenstein , the modern prometheus</td>\n",
       "      <td>or</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frankenstein , or modern prometheus by</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>, or the prometheus by mary</td>\n",
       "      <td>modern</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90693</th>\n",
       "      <td>newsletter to hear new ebooks .</td>\n",
       "      <td>about</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90694</th>\n",
       "      <td>to hear about ebooks .</td>\n",
       "      <td>new</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90695</th>\n",
       "      <td>hear about new .</td>\n",
       "      <td>ebooks</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90696</th>\n",
       "      <td>about new ebooks</td>\n",
       "      <td>.</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90697</th>\n",
       "      <td>new ebooks .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      context        target  split\n",
       "0                                    , or the  frankenstein  train\n",
       "1                  frankenstein or the modern             ,  train\n",
       "2        frankenstein , the modern prometheus            or  train\n",
       "3      frankenstein , or modern prometheus by           the  train\n",
       "4                 , or the prometheus by mary        modern  train\n",
       "...                                       ...           ...    ...\n",
       "90693         newsletter to hear new ebooks .         about   test\n",
       "90694                 to hear about ebooks .            new   test\n",
       "90695                       hear about new .         ebooks   test\n",
       "90696                       about new ebooks              .   test\n",
       "90697                            new ebooks .           NaN   test\n",
       "\n",
       "[90698 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../data/frankenstein_with_splits.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa05145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 다시 train/valid/test로 나눠줌\n",
    "\n",
    "# train 데이터 \n",
    "train_df = df[df.split=='train']\n",
    "train_size = len(train_df)\n",
    "\n",
    "# valid 데이터 \n",
    "val_df = df[df.split=='val']\n",
    "val_size = len(val_df)\n",
    "\n",
    "# test 데이터 \n",
    "test_df = df[df.split=='test']\n",
    "test_size = len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab6d7f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  63489\n",
      "valid :  13605\n",
      "test :  13604\n"
     ]
    }
   ],
   "source": [
    "print(\"train : \",train_size)\n",
    "print(\"valid : \",val_size)\n",
    "print(\"test : \",test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ec341cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dict = {'train': (train_df, train_size),\n",
    "                             'val': (val_df, val_size),\n",
    "                             'test': (test_df, test_size)}\n",
    "# lookup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5109dd35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63479</th>\n",
       "      <td>i be alone</td>\n",
       "      <td>?</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63480</th>\n",
       "      <td>be alone ?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63481</th>\n",
       "      <td>had feelings of</td>\n",
       "      <td>i</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63482</th>\n",
       "      <td>i feelings of affection</td>\n",
       "      <td>had</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63483</th>\n",
       "      <td>i had of affection ,</td>\n",
       "      <td>feelings</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63484</th>\n",
       "      <td>i had feelings affection , and</td>\n",
       "      <td>of</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63485</th>\n",
       "      <td>had feelings of , and they</td>\n",
       "      <td>affection</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63486</th>\n",
       "      <td>feelings of affection and they were</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63487</th>\n",
       "      <td>of affection , they were requited</td>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63488</th>\n",
       "      <td>affection , and were requited by</td>\n",
       "      <td>they</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   context     target  split\n",
       "63479                          i be alone           ?  train\n",
       "63480                           be alone ?        NaN  train\n",
       "63481                      had feelings of          i  train\n",
       "63482              i feelings of affection        had  train\n",
       "63483                 i had of affection ,   feelings  train\n",
       "63484       i had feelings affection , and         of  train\n",
       "63485           had feelings of , and they  affection  train\n",
       "63486  feelings of affection and they were          ,  train\n",
       "63487    of affection , they were requited        and  train\n",
       "63488     affection , and were requited by       they  train"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d30142",
   "metadata": {},
   "source": [
    "### 2. Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fae986f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, mask_token=\"<MASK>\",add_unk=True):\n",
    "        self.token_to_idx = {}\n",
    "        self.idx_to_token = {}\n",
    "        \n",
    "        self.mask_index = self.add_token(mask_token)\n",
    "#         \"UNK\" 토큰이 추가되지 않는 경우에는 -1로 설정,\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "#         \"UNK\" 토큰이 추가될 경우에는 UNK에 해당하는 인덱스로 설정,\n",
    "            self.unk_index = self.add_token('<UNK>') \n",
    "\n",
    "    def add_token(self, token):\n",
    "        \n",
    "#       만약 해당 토큰이 있으면 토큰 idx만 return\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "            \n",
    "#       만약 해당 토큰이 없으면 새로운 토큰 만들어줌\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9daef709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW Vocabulary 객체 생성\n",
    "\n",
    "cbow_vocab = Vocabulary()\n",
    "for index, row in df.iterrows():\n",
    "    for token in row.context.split(' '):\n",
    "        cbow_vocab.add_token(token)\n",
    "    cbow_vocab.add_token(row.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "714ba3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<MASK>': 0, '<UNK>': 1, ',': 2, 'or': 3, 'the': 4}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(cbow_vocab.token_to_idx.items())[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59bd61b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<MASK>', 1: '<UNK>', 2: ',', 3: 'or', 4: 'the'}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(cbow_vocab.idx_to_token.items())[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57cf931b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7270\n"
     ]
    }
   ],
   "source": [
    "print(len(cbow_vocab.token_to_idx.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ee407f",
   "metadata": {},
   "source": [
    "## 3. Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "166fd3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 토큰에 대응하는 인덱스 반환\n",
    "\n",
    "def lookup_token(vocabulary_class,token):\n",
    "\n",
    "# UNK 토큰이 있을 경우\n",
    "    if vocabulary_class.unk_index >= 0:\n",
    "#           토큰을 찾아보고 없으면 unk_index 반환, 있으면 해당 토큰의 idx를 반환\n",
    "        return vocabulary_class.token_to_idx.get(token, vocabulary_class.unk_index)\n",
    "    else:\n",
    "        return vocabulary_class.token_to_idx[token]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db2d52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 인덱스에 대응하는 토큰 반환\n",
    "\n",
    "def lookup_index(vocabulary_class, index):\n",
    "        if index not in vocabulary_class.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return vocabulary_class.idx_to_token[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6853d180",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1764d004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[215   1   1   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def vectorize(context, vector_length=-1):\n",
    "    \n",
    "    indices = [lookup_token(cbow_vocab,token) for token in context.split(' ')]\n",
    "    if vector_length < 0:\n",
    "        vector_length = len(indices)\n",
    "    \n",
    "    \n",
    "#   인덱스와 mask로 이루어진 out_vector를 만든다.\n",
    "    out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "    out_vector[:len(indices)] = indices\n",
    "#     문장 길이가 작아서 padding 진행하면 해당 부분 mask 처리\n",
    "    out_vector[len(indices):] = cbow_vocab.mask_index\n",
    "    \n",
    "    \n",
    "    return out_vector\n",
    "\n",
    "# vector_length 보다 문장의 토큰의 개수가 작으면 MASK 처리된다. \n",
    "print(vectorize(\"all dafs dfkdl\",vector_length=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff427e2",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d828249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_seq_length를 구해야 한다. \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, cbow_df):\n",
    "        \n",
    "        self.cbow_df = cbow_df\n",
    "        \n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self.max_seq_length = max(map(measure_len, cbow_df.context))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cbow_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        row = self.cbow_df.iloc[index]\n",
    "\n",
    "        context_vector = vectorize(row.context, self.max_seq_length)\n",
    "        target_index = lookup_token(cbow_vocab,row.target)\n",
    "\n",
    "        return {'x_data': context_vector,\n",
    "                'y_target': target_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70ee0b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CBOWDataset at 0x7fd4d3250bb0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋을 인스턴스화 해주어야 로더에 넣어줄 수 있다. \n",
    "\n",
    "train_dataset = CBOWDataset(train_df)\n",
    "train_dataset\n",
    "\n",
    "valid_dataset = CBOWDataset(val_df)\n",
    "valid_dataset\n",
    "\n",
    "test_dataset = CBOWDataset(test_df)\n",
    "test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f448e176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fea8395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(valid_dataset.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bfbff630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abd30122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더 설정\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# drop_last=True -> 배치 사이즈보다 over하면 drop\n",
    "\n",
    "Traindataloader = DataLoader(dataset=train_dataset, batch_size=512,\n",
    "                            shuffle=True, drop_last=True)\n",
    "\n",
    "Validdataloader = DataLoader(dataset=valid_dataset, batch_size=512,\n",
    "                            shuffle=False, drop_last=True)\n",
    "\n",
    "Testdataloader = DataLoader(dataset=test_dataset, batch_size=512,\n",
    "                            shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ba7f661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63489 124\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset),len(Traindataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7ed795cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "{'x_data': tensor([[1508, 1406,  226, 1509, 1360,   49],\n",
      "        [  48, 3567,   72,  223,   15,    0],\n",
      "        [  48,  578,   49,  145,  257, 3465],\n",
      "        ...,\n",
      "        [4296,  160,    4,   33, 4969,   49],\n",
      "        [ 819, 3466,  573,  319,  742,    8],\n",
      "        [ 394,  264,   48, 4033,   19, 1509]]), 'y_target': tensor([ 152,  632,  551, 1930,    2,   82, 3807,   41,   72,   48,   33,  757,\n",
      "          19,   44,   68,   72, 3336,  154, 2049,   72,    2,  326,  788,  230,\n",
      "        4798, 2137,  819,  783, 4436,   19, 1312,   48,   49,  365,   44, 5204,\n",
      "        1033,  160,  765,    8,   44, 4968, 3090,   15,    2,    4,   15,   72,\n",
      "        1210,    4,   49,   33,  230,   72,  257,  785,  329,   49,   44,   26,\n",
      "          92,   44, 1146, 3349,  230,  255,    2,  383, 3396,   82,  326,    2,\n",
      "        3097,  567,   19, 5383,   44,   49,    2,    4,  411,  134,   27,    2,\n",
      "        1033,   88,  212,    4, 2073,   15,  345,  506,   49, 3035,  338,   49,\n",
      "          44,   48, 2083,  226,   48,   49, 4940, 5170,    2,  420, 1992, 1950,\n",
      "          60,   56,  365,  822,    4, 1205, 4546,    4,   37, 3849, 1998, 2594,\n",
      "         236, 1482,    2,    2,   72, 1306,  765,    2,   72, 2261,   44,   27,\n",
      "           2, 2419,   48,  226,   65,  212,   68,  226,   15,  414, 1063,  788,\n",
      "         780, 2532,   49, 4893, 1640,  559, 3507, 3537,  386,  719,  349, 6013,\n",
      "        2940,   15,    4, 1008,  365,  226,   19,   44,  115,  908,   60,   23,\n",
      "           2,   85,   33, 1169, 4097,   44, 2625,  434,  788, 3394, 5256, 3112,\n",
      "          49,    8,    4, 2812,    2, 1984, 1400,   24,  160, 2538, 5081,   33,\n",
      "          90,  215,   15,   53,   82,    2,   90,  199, 1735,  806,  496,    8,\n",
      "         356, 3805,  455,  340,   27,  160,   48, 2007, 3801, 3836,  766,  760,\n",
      "          72,  579,   48, 2202,   48,  554, 3354,   24,   60, 3610,   15, 4869,\n",
      "         838,    2,   39,  192,  137,   49,   68,    4,    2, 1532,  728,    4,\n",
      "          60,  989,   49,  518,    2, 3345,   19, 2667,  648,   82,  226,   19,\n",
      "           4,  788,   85,  237,  454,  230,  877,    4,    2,  377,  255,  395,\n",
      "        6047,    2,    4,  649,  874,   44,  406,    4, 4149,  284,   48,   68,\n",
      "          33,  137,  239, 2471,    2, 4835, 2073,    2,  230,   72,   19, 5424,\n",
      "          49,   39, 3208,  127, 3885, 2233,   33,  732,  374,  685,  115, 2723,\n",
      "        1611,    2,   72, 3697,   68,   23, 4303,  160,   48,    2,  319, 2661,\n",
      "        1940,  455,  581, 4658, 2053, 3453,   68,   33, 4528,   27,   15,   68,\n",
      "          15,   49, 6014,   23,    4,  137,  788,  319,  867, 1175, 3329,   49,\n",
      "         694, 5736,   48, 5792, 1628,   15,   49,   39,   48,   49, 3103,  393,\n",
      "        1211,  406,   60, 1326,   50,    2,  632,   19, 1369, 1286, 5739,  374,\n",
      "         115,   72,   19,  242,   48,    4, 1120,   33,    2,    4,  257,  298,\n",
      "         326,   19,   44,   51, 4501,   49,   48,  377, 1375,  115,   50,  981,\n",
      "         212,   28, 3034,   44,  470,  392,   82, 2820,    2,   19,  211, 2625,\n",
      "         123,  482, 2001,   19,  326, 4182,   23,    2,  212,   50,  160,   48,\n",
      "        3654,  398,   36,  225,    4,   33,   19,    2,   72,  109, 1287,  334,\n",
      "        4752,   48,   27,  455,   15, 4496,  136,   49, 3133,  788,  319,   19,\n",
      "         688, 3405, 4044, 4754, 3841,   48,    4, 4314,  137, 3477, 4810,    2,\n",
      "         212, 1919,   19,   27,    2,   83,  334,  321, 1278,   44, 1139,    2,\n",
      "         334,  334,   44,   19,   72, 3811,  745,   49,   33,  685,   36,  212,\n",
      "          48, 5208, 1640,  220,  230, 1844,  398,  888,  115,   15,    4,  255,\n",
      "           4,   15,  264,   48,   96,  551,   46,  501,  226,   50,  125,  225,\n",
      "         173,   15,  502,  211, 1774,    2,   19, 2001,  212, 2790,   44,    2,\n",
      "        1610, 3091,  358,   68,    2,  273,    2,  326])}\n"
     ]
    }
   ],
   "source": [
    "for batch_index, batch_dict in reversed(list(enumerate(Traindataloader))):\n",
    "    print(batch_index)\n",
    "    print(batch_dict)\n",
    "    \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0790db50",
   "metadata": {},
   "source": [
    "### 모델정의 NameClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5558da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_size를 지정해줘야 한다.\n",
    "# 1. Embedding layer를 이용해서 문맥의 단어를 나타내는 인덱스를 각 단어에 대한 벡터로 만든다.\n",
    "# 2. 전반적인 문맥을 감지하도록 벡터를 결합한다.(sum)\n",
    " \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CBOWClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    매개변수:\n",
    "        vocabulary_size (int): 어휘 사전 크기, 임베딩 개수와 예측 벡터 크기를 결정합니다\n",
    "        embedding_size (int): 임베딩 크기\n",
    "        padding_idx (int): 기본값 0; 임베딩은 이 인덱스를 사용하지 않습니다\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocabulary_size, embedding_size, padding_idx=0):\n",
    "        \n",
    "#       torch.nn.Module의 초기화 메서드를 실행하여 해당 클래스의 기능을 상속받음\n",
    "        super(CBOWClassifier, self).__init__()\n",
    "        # Embedding layer를 이용해서 문맥의 단어를 나타내는 인덱스를 각 단어에 대한 벡터로 만듦\n",
    "        \n",
    "        self.embedding =  nn.Embedding(num_embeddings=vocabulary_size, \n",
    "                                       embedding_dim=embedding_size,\n",
    "                                       padding_idx=padding_idx)\n",
    "#         embedding에서 나오면 embedding_size * vocab_size 차원이 된다. \n",
    "        self.fc1 = nn.Linear(in_features=embedding_size,\n",
    "                             out_features=vocabulary_size)\n",
    "    \n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        # 전반적인 문맥을 감지하도록 벡터를 결합함(sum)\n",
    "        x_embedded_sum = F.dropout(self.embedding(x_in).sum(dim=1), 0.3)\n",
    "        y_out = self.fc1(x_embedded_sum)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "            \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "110c5bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOWClassifier(\n",
       "  (embedding): Embedding(7270, 50, padding_idx=0)\n",
       "  (fc1): Linear(in_features=50, out_features=7270, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_size를 지정해줘야 한다. \n",
    "embedding_size=50 \n",
    "\n",
    "classifier = CBOWClassifier(vocabulary_size=len(cbow_vocab.token_to_idx), \n",
    "                            embedding_size=embedding_size)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5971ede",
   "metadata": {},
   "source": [
    "### 모델 구조 뜯어보기(번외)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "36d1f040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미니배치 0의 임베딩 결과:\n",
      "torch.Size([512, 6, 50])\n",
      "tensor([[[-5.3818e-01,  5.9857e-01, -1.3624e+00,  ...,  6.0974e-01,\n",
      "           4.6279e-01,  3.9132e-01],\n",
      "         [ 1.6738e+00, -1.9033e+00, -4.2344e-02,  ...,  6.0865e-01,\n",
      "           8.5834e-01,  4.7428e-01],\n",
      "         [ 1.1299e+00,  3.4581e-01,  6.3495e-01,  ..., -5.7004e-01,\n",
      "          -6.6446e-01, -1.7385e+00],\n",
      "         [-3.0684e-01,  1.4789e+00,  2.3915e-01,  ..., -1.0308e-01,\n",
      "          -1.0496e+00,  1.4137e-02],\n",
      "         [ 9.3971e-01, -2.0468e+00,  1.0562e-01,  ..., -7.2533e-01,\n",
      "          -1.1429e+00, -7.6809e-01],\n",
      "         [ 1.8508e-01,  6.8729e-01,  2.3410e+00,  ...,  1.9740e+00,\n",
      "          -7.7349e-01, -1.8343e-01]],\n",
      "\n",
      "        [[-1.3526e+00, -6.1518e-01,  3.3959e-01,  ...,  8.9573e-01,\n",
      "          -1.8137e+00, -4.8535e-01],\n",
      "         [ 5.8430e-01,  3.6420e-01,  1.7707e-01,  ..., -1.3824e+00,\n",
      "          -2.4476e+00,  2.2488e+00],\n",
      "         [ 1.3178e-02,  1.0733e+00,  6.8014e-01,  ..., -7.2086e-01,\n",
      "           2.8438e+00,  2.0107e-02],\n",
      "         [-1.1226e+00,  3.3115e-01,  1.3008e+00,  ..., -1.1064e+00,\n",
      "           6.2898e-01,  1.1197e+00],\n",
      "         [-2.4611e-01, -2.0898e-01, -2.5016e-01,  ..., -8.7513e-01,\n",
      "           8.6667e-01,  5.6250e-01],\n",
      "         [-9.1260e-01, -2.6443e-01, -1.4449e-02,  ..., -1.8473e+00,\n",
      "          -1.0113e+00, -2.0568e+00]],\n",
      "\n",
      "        [[-1.5869e+00,  6.1401e-01,  9.9146e-01,  ...,  1.1616e+00,\n",
      "          -4.1178e-01,  9.4983e-02],\n",
      "         [-3.0684e-01,  1.4789e+00,  2.3915e-01,  ..., -1.0308e-01,\n",
      "          -1.0496e+00,  1.4137e-02],\n",
      "         [-2.2469e+00,  4.7667e-01,  6.3817e-01,  ...,  1.3885e+00,\n",
      "           8.0726e-01,  3.7175e-01],\n",
      "         [-2.4611e-01, -2.0898e-01, -2.5016e-01,  ..., -8.7513e-01,\n",
      "           8.6667e-01,  5.6250e-01],\n",
      "         [-3.0684e-01,  1.4789e+00,  2.3915e-01,  ..., -1.0308e-01,\n",
      "          -1.0496e+00,  1.4137e-02],\n",
      "         [-2.3078e+00,  2.9304e-01, -2.0577e-02,  ..., -6.5228e-02,\n",
      "          -4.5941e-01, -1.4735e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.2276e-01,  4.9297e-01, -4.1046e-01,  ..., -9.0776e-01,\n",
      "          -1.4789e+00,  1.7485e+00],\n",
      "         [-1.3424e+00, -7.8083e-01, -1.5988e-01,  ..., -6.8961e-01,\n",
      "           5.4517e-01,  2.5366e-01],\n",
      "         [ 8.8316e-01, -5.0425e-01,  3.6390e-01,  ..., -5.6088e-02,\n",
      "          -1.4818e+00, -8.5532e-01],\n",
      "         [-3.0684e-01,  1.4789e+00,  2.3915e-01,  ..., -1.0308e-01,\n",
      "          -1.0496e+00,  1.4137e-02],\n",
      "         [ 1.5869e+00, -1.3647e+00,  4.4618e-01,  ...,  1.2829e+00,\n",
      "          -2.4871e-01,  1.0597e+00],\n",
      "         [-1.5869e+00,  6.1401e-01,  9.9146e-01,  ...,  1.1616e+00,\n",
      "          -4.1178e-01,  9.4983e-02]],\n",
      "\n",
      "        [[-1.1226e+00,  3.3115e-01,  1.3008e+00,  ..., -1.1064e+00,\n",
      "           6.2898e-01,  1.1197e+00],\n",
      "         [ 1.4576e+00,  3.2690e-01,  1.8910e+00,  ...,  1.0572e+00,\n",
      "          -6.0650e-01,  1.5773e+00],\n",
      "         [ 1.0140e+00, -8.4056e-01, -1.5490e-02,  ...,  6.3191e-01,\n",
      "           9.3181e-01,  3.3705e-01],\n",
      "         [-1.1226e+00,  3.3115e-01,  1.3008e+00,  ..., -1.1064e+00,\n",
      "           6.2898e-01,  1.1197e+00],\n",
      "         [ 1.1048e+00,  7.9396e-01, -2.1252e+00,  ...,  5.3631e-01,\n",
      "           1.1901e-01,  7.4597e-01],\n",
      "         [-9.3516e-02,  1.3029e+00, -8.5262e-01,  ..., -4.3750e-01,\n",
      "           4.7120e-01, -1.6249e+00]],\n",
      "\n",
      "        [[ 7.2849e-01, -8.8921e-01,  1.1494e+00,  ..., -5.2935e-01,\n",
      "           2.8260e-01, -3.4238e-01],\n",
      "         [ 2.4220e-01, -1.1259e+00, -1.3663e+00,  ..., -2.1269e+00,\n",
      "          -1.0781e+00,  3.6207e-01],\n",
      "         [ 7.2849e-01, -8.8921e-01,  1.1494e+00,  ..., -5.2935e-01,\n",
      "           2.8260e-01, -3.4238e-01],\n",
      "         [-8.1809e-01,  3.5271e-02, -2.9713e-01,  ..., -9.5441e-01,\n",
      "          -1.2134e+00,  3.9644e-01],\n",
      "         [-5.0232e-01,  8.0113e-02,  1.3567e-01,  ..., -3.6041e-01,\n",
      "          -2.8860e+00,  1.2042e+00],\n",
      "         [ 6.9854e-01, -1.1579e-01, -5.9724e-01,  ...,  2.3802e-03,\n",
      "           8.1474e-01, -2.9754e-01]]], grad_fn=<EmbeddingBackward0>)\n",
      " \n",
      "미니배치 0의 임베딩 sum 결과:\n",
      "torch.Size([512, 50])\n",
      "tensor([[ 3.0835, -0.8396,  1.9159,  ...,  1.7939, -2.3093, -1.8103],\n",
      "        [-3.0364,  0.6800,  2.2329,  ..., -5.0364, -0.9330,  1.4089],\n",
      "        [-7.0015,  4.1326,  1.8372,  ...,  1.4036, -1.2965, -0.4160],\n",
      "        ...,\n",
      "        [-0.6434, -0.0639,  1.4704,  ...,  0.6879, -4.1257,  2.3157],\n",
      "        [ 1.2376,  2.2455,  1.4992,  ..., -0.4250,  2.1735,  3.2748],\n",
      "        [ 1.0773, -2.9047,  0.1738,  ..., -4.4980, -3.7975,  0.9804]],\n",
      "       grad_fn=<SumBackward1>)\n",
      " \n",
      "미니배치 0의 임베딩 dropout 결과:\n",
      "torch.Size([512, 50])\n",
      "tensor([[  4.4050,  -1.1994,   0.0000,  ...,   2.5627,  -3.2991,  -2.5862],\n",
      "        [ -4.3377,   0.9715,   3.1899,  ...,  -7.1948,  -0.0000,   2.0127],\n",
      "        [-10.0021,   5.9037,   2.6246,  ...,   2.0051,  -0.0000,  -0.5943],\n",
      "        ...,\n",
      "        [ -0.0000,  -0.0913,   2.1005,  ...,   0.9827,  -5.8938,   3.3081],\n",
      "        [  1.7680,   3.2079,   2.1418,  ...,  -0.6071,   3.1050,   0.0000],\n",
      "        [  1.5390,  -4.1496,   0.2482,  ...,  -6.4258,  -0.0000,   1.4006]],\n",
      "       grad_fn=<MulBackward0>)\n",
      " \n",
      "미니배치 0의 fc1 결과:\n",
      "torch.Size([512, 7270])\n",
      "tensor([[-1.7535e+00, -1.2800e+00, -1.0107e+00,  ...,  4.2918e+00,\n",
      "          5.1441e+00, -2.2904e+00],\n",
      "        [ 3.9171e+00, -8.1542e-01, -1.5343e+00,  ...,  7.7978e-01,\n",
      "         -1.8867e+00,  5.2275e-01],\n",
      "        [-3.7269e+00,  2.3446e+00, -6.8073e-01,  ...,  2.9345e+00,\n",
      "          5.8298e+00, -7.0432e-01],\n",
      "        ...,\n",
      "        [ 1.5557e+00,  1.4035e-01, -6.8400e-01,  ...,  5.3893e-02,\n",
      "          2.0856e+00, -3.1893e-01],\n",
      "        [ 2.2646e+00,  3.6482e+00, -3.2625e+00,  ...,  1.8035e+00,\n",
      "         -1.5401e+00, -8.8771e-01],\n",
      "        [-9.5070e-01,  2.5303e+00, -2.2772e-03,  ...,  4.6582e-01,\n",
      "          5.2130e-01, -1.3168e+00]], grad_fn=<AddmmBackward0>)\n",
      " \n",
      "미니배치 0의 softmax 결과:\n",
      "torch.Size([512, 7270])\n",
      "tensor([[4.2493e-06, 6.8228e-06, 8.9312e-06,  ..., 1.7938e-03, 4.2062e-03,\n",
      "         2.4839e-06],\n",
      "        [2.3400e-03, 2.0603e-05, 1.0040e-05,  ..., 1.0156e-04, 7.0582e-06,\n",
      "         7.8539e-05],\n",
      "        [3.4806e-07, 1.5083e-04, 7.3212e-06,  ..., 2.7206e-04, 4.9214e-03,\n",
      "         7.1505e-06],\n",
      "        ...,\n",
      "        [1.0070e-04, 2.4455e-05, 1.0724e-05,  ..., 2.2430e-05, 1.7108e-04,\n",
      "         1.5449e-05],\n",
      "        [1.3798e-04, 5.5046e-04, 5.4880e-07,  ..., 8.7006e-05, 3.0722e-06,\n",
      "         5.8991e-06],\n",
      "        [9.2346e-06, 3.0004e-04, 2.3840e-05,  ..., 3.8072e-05, 4.0244e-05,\n",
      "         6.4034e-06]], grad_fn=<SoftmaxBackward0>)\n",
      " \n",
      "미니배치 0의 softmax dim=1 결과:\n",
      "torch.Size([512, 7270])\n",
      "tensor([[0.0001, 0.0001, 0.0001,  ..., 0.0001, 0.0001, 0.0001],\n",
      "        [0.0001, 0.0001, 0.0001,  ..., 0.0001, 0.0001, 0.0001],\n",
      "        [0.0001, 0.0001, 0.0001,  ..., 0.0001, 0.0001, 0.0001],\n",
      "        ...,\n",
      "        [0.0001, 0.0001, 0.0001,  ..., 0.0001, 0.0001, 0.0001],\n",
      "        [0.0001, 0.0001, 0.0001,  ..., 0.0001, 0.0001, 0.0001],\n",
      "        [0.0001, 0.0001, 0.0001,  ..., 0.0001, 0.0001, 0.0001]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bz/1fkmkxnn70gbzvvkys2cw_k40000gn/T/ipykernel_70924/3186516556.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_out = F.softmax(y_out)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocabulary_size = len(cbow_vocab.token_to_idx) \n",
    "embedding_size=50\n",
    "padding_idx=0\n",
    "\n",
    "embedding =  nn.Embedding(num_embeddings=vocabulary_size, \n",
    "                           embedding_dim=embedding_size,\n",
    "                           padding_idx=padding_idx)\n",
    "\n",
    "fc1 = nn.Linear(in_features=embedding_size,\n",
    "                     out_features=vocabulary_size)\n",
    "    \n",
    "\n",
    "# 데이터 로더에서 미니배치를 반복합니다.\n",
    "for batch_index, batch in enumerate(Traindataloader):\n",
    "    # 각 미니배치에서 샘플을 가져옵니다.\n",
    "    x_in = batch['x_data']\n",
    "    y_in = batch['y_target']\n",
    "    \n",
    "    \n",
    "    embedded_sample = embedding(x_in)\n",
    "    \n",
    "    # 임베딩 결과를 출력합니다.\n",
    "    print(f\"미니배치 {batch_index}의 임베딩 결과:\")\n",
    "    print(embedded_sample.size())\n",
    "    print(embedded_sample)\n",
    "    \n",
    "    \n",
    "    sum_result = embedding(x_in).sum(dim=1)\n",
    "    \n",
    "    print(\" \")\n",
    "    print(f\"미니배치 {batch_index}의 임베딩 sum 결과:\")\n",
    "    print(sum_result.size())\n",
    "    print(sum_result)\n",
    "    \n",
    "    x_embedded_sum = F.dropout(sum_result, 0.3)\n",
    "    \n",
    "    print(\" \")\n",
    "    print(f\"미니배치 {batch_index}의 임베딩 dropout 결과:\")\n",
    "    print(x_embedded_sum.size())\n",
    "    print(x_embedded_sum)\n",
    "    \n",
    "    y_out = fc1(x_embedded_sum)\n",
    "    \n",
    "    print(\" \")\n",
    "    print(f\"미니배치 {batch_index}의 fc1 결과:\")\n",
    "    print(y_out.size())\n",
    "    print(y_out)\n",
    "    \n",
    "    y_out = F.softmax(y_out)\n",
    "    \n",
    "    print(\" \")\n",
    "    print(f\"미니배치 {batch_index}의 softmax 결과:\")\n",
    "    print(y_out.size())\n",
    "    print(y_out)\n",
    "    \n",
    "    y_out = F.softmax(y_out, dim=1) # 미니배치 각 행에 softmax 적용\n",
    "    \n",
    "    print(\" \")\n",
    "    print(f\"미니배치 {batch_index}의 softmax dim=1 결과:\")\n",
    "    print(y_out.size())\n",
    "    print(y_out)\n",
    "    \n",
    "#     y_pred = y_out\n",
    "#     loss =  loss_func(y_pred, y_in)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ad82b",
   "metadata": {},
   "source": [
    "### 옵티마이저, loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13216f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5eaa3504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 옵티마이저\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = lr)\n",
    "optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b32b926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31608c79",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75e5f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "#      예측값과 타겟값을 비교하여 일치하는 개수를 계산\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ef89fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train state 초기화 \n",
    "def make_train_state():\n",
    "    return {\n",
    "        'stop_early':False,\n",
    "        'early_stopping_step':0,\n",
    "        'early_stopping_best_val':1e8,\n",
    "        'early_stopping_criteria' : 10,\n",
    "        'epoch_index' : 0,\n",
    "        'train_loss': [], \n",
    "        'train_acc' :[], \n",
    "        'val_loss' : [],\n",
    "        'val_acc' : [], \n",
    "        'test_loss' : [],\n",
    "        'test_acc' : [],\n",
    "         \n",
    "#       모델 저장파일\n",
    "        'model_filename' : 'model.pth'\n",
    "    } \n",
    "\n",
    "\n",
    "# Train update \n",
    "def update_train_state(model, train_state):\n",
    "    \n",
    "#   학습시작하면 초기에 모델 저장하기 \n",
    "    \n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(),train_state['model_filename'])\n",
    "        \n",
    "#   모델 성능이 향상되면 모델 저장(valid loss가 더 낮아지면)\n",
    "    elif train_state['epoch_index'] >=1 :\n",
    "        loss_t = train_state['val_loss'][-1]\n",
    "#        loss가 나빠지면 early stop step 업데이트\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            train_state['early_stopping_step']+=1\n",
    "            \n",
    "#        loss가 좋아지면   \n",
    "        else:\n",
    "#            early stop step 0으로 다시 초기화        \n",
    "            train_state['early_stopping_step']=0\n",
    "    \n",
    "#           최저 loss이면 모델 저장 \n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "                torch.save(model.state_dict(),train_state['model_filename'])\n",
    "\n",
    "#       기준점 넘으면 early stop \n",
    "        if train_state['early_stopping_step'] >= train_state['early_stopping_criteria']:\n",
    "            train_state['stop_early'] = True\n",
    "        \n",
    "        return train_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66b6b449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop_early': False,\n",
       " 'early_stopping_step': 0,\n",
       " 'early_stopping_best_val': 100000000.0,\n",
       " 'early_stopping_criteria': 10,\n",
       " 'epoch_index': 0,\n",
       " 'train_loss': [],\n",
       " 'train_acc': [],\n",
       " 'val_loss': [],\n",
       " 'val_acc': [],\n",
       " 'test_loss': [],\n",
       " 'test_acc': [],\n",
       " 'model_filename': 'model.pth'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 진행 상황 함수 초기화\n",
    "train_state = make_train_state()\n",
    "train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ef8576c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                          | 1/100 [00:07<11:36,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 7.613911078526424\n",
      "val_acc 7.158954326923076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▊                                          | 2/100 [00:13<11:03,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 7.004817229050856\n",
      "val_acc 10.223858173076922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|█▎                                         | 3/100 [00:20<11:07,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.769168120164138\n",
      "val_acc 11.70372596153846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|█▋                                         | 4/100 [00:27<11:09,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.636771440505982\n",
      "val_acc 12.620192307692308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|██▏                                        | 5/100 [00:34<10:52,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.557459134321947\n",
      "val_acc 13.45402644230769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|██▌                                        | 6/100 [00:41<10:41,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.511043603603657\n",
      "val_acc 13.536658653846153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|███                                        | 7/100 [00:47<10:34,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.484455603819627\n",
      "val_acc 13.581730769230768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|███▍                                       | 8/100 [00:54<10:19,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.448257996485784\n",
      "val_acc 14.212740384615387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███▊                                       | 9/100 [01:01<10:15,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.435615429511437\n",
      "val_acc 14.002403846153845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▏                                     | 10/100 [01:09<10:38,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.432023066740769\n",
      "val_acc 14.024939903846152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████▌                                     | 11/100 [01:16<10:29,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.439559789804312\n",
      "val_acc 14.04747596153846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████                                     | 12/100 [01:23<10:21,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.439962717202995\n",
      "val_acc 14.325420673076922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█████▍                                    | 13/100 [01:29<10:05,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.446129450431237\n",
      "val_acc 14.197716346153848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█████▉                                    | 14/100 [01:36<09:51,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.4573691991659325\n",
      "val_acc 14.460637019230768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|██████▎                                   | 15/100 [01:43<09:40,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.452565504954411\n",
      "val_acc 14.423076923076922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|██████▋                                   | 16/100 [01:50<09:47,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.448926980678851\n",
      "val_acc 14.693509615384615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████▏                                  | 17/100 [01:57<09:30,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.4620731610518245\n",
      "val_acc 14.678485576923078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|███████▌                                  | 18/100 [02:04<09:25,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.488405814537636\n",
      "val_acc 14.438100961538463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████▉                                  | 19/100 [02:11<09:22,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.479242746646589\n",
      "val_acc 14.595853365384613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████▉                                  | 19/100 [02:18<09:52,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.48716009580172\n",
      "val_acc 14.75360576923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "# 에포크만큼\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "\n",
    "#     print('epoch',epoch)\n",
    "#     print(train_state['epoch_index']) \n",
    "    train_state['epoch_index'] +=1 \n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "\n",
    "#     모델을 학습 모드로 설정 -> 드롭아웃 및 배치 정규화와 같은 학습 중에만 적용되는 기법들이 활성화\n",
    "#     모델을 평가 모드로 전환하려면 classifier.eval()을 사용\n",
    "    classifier.train()\n",
    "# 배치 만큼\n",
    "    for batch_idx, batch_data in enumerate(Traindataloader):\n",
    "\n",
    "        \n",
    "\n",
    "#       1. 옵티마이저 그레디언트 0으로 초기화\n",
    "        optimizer.zero_grad()\n",
    "#       2. 모델에 데이터 넣어서 출력받기\n",
    "        y_pred = classifier(x_in=batch_data['x_data'])\n",
    "#       3. loss 계산하기\n",
    "        loss =  loss_func(y_pred, batch_data['y_target'])\n",
    "    \n",
    "#       tensor(0.3190) -> 0.3190, item()으로 스칼라 값만 추출\n",
    "        loss_t = loss.item()\n",
    "\n",
    "#       배치에서의 평균 loss 구하기\n",
    "        running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "\n",
    "#       4. gradient 계산하기\n",
    "        loss.backward()\n",
    "\n",
    "#       5. 옵티마이저 가중치 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "#       Accuracy 계산\n",
    "        acc_t = compute_accuracy(y_pred, batch_data['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "\n",
    "\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "\n",
    "\n",
    "#   valid에 대한 계산\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "    classifier.eval() # 모델 파라미터를 수정하지 못 하게 비활성화\n",
    "\n",
    "    for batch_idx, batch_data in enumerate(Validdataloader):\n",
    "\n",
    "#       1. 모델의 출력값(y_pred)계산\n",
    "        y_pred = classifier(x_in=batch_data['x_data'])\n",
    "\n",
    "#       2. loss 계산\n",
    "        loss = loss_func(y_pred,batch_data['y_target'])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "\n",
    "#       3. Accuracy 계산\n",
    "        acc_t = compute_accuracy(y_pred,batch_data['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "    \n",
    "    print(\"val_loss\",running_loss)\n",
    "    print(\"val_acc\",running_acc)\n",
    "\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "    \n",
    "\n",
    "#   전체 loss, acc 저장\n",
    "    train_state = update_train_state(model=classifier,\n",
    "                                     train_state=train_state)\n",
    "#   early stop해라고 했으면 학습 멈추기    \n",
    "    if train_state['stop_early']:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037c61f7",
   "metadata": {},
   "source": [
    "### Test 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "331de996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 좋은 모델을 사용해 테스트 세트의 손실과 정확도를 계산합니다\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "\n",
    "# 가중치 업데이트 하지 못 하게\n",
    "classifier.eval()\n",
    "\n",
    "for batch_idx, batch_data in enumerate(Testdataloader):\n",
    "    \n",
    "    y_pred = classifier(x_in=batch_data['x_data'])\n",
    "    loss = loss_func(y_pred,batch_data['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "    \n",
    "    acc_t = compute_accuracy(y_pred, batch_data['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74df48a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 7.265\n",
      "테스트 정확도: 13.17\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 손실: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {:.2f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "342a2c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop_early': True,\n",
       " 'early_stopping_step': 10,\n",
       " 'early_stopping_best_val': 6.432023066740769,\n",
       " 'early_stopping_criteria': 10,\n",
       " 'epoch_index': 20,\n",
       " 'train_loss': [8.603957253117713,\n",
       "  7.111192495592179,\n",
       "  6.5485151544693965,\n",
       "  6.213718360470187,\n",
       "  5.98085355758667,\n",
       "  5.796104023533482,\n",
       "  5.652096659906449,\n",
       "  5.525909473819118,\n",
       "  5.419759035110474,\n",
       "  5.318656921386717,\n",
       "  5.229448260799533,\n",
       "  5.148317594682014,\n",
       "  5.074486763246597,\n",
       "  5.004274406740742,\n",
       "  4.942227005958556,\n",
       "  4.885120161118047,\n",
       "  4.825584430848396,\n",
       "  4.779782606709388,\n",
       "  4.728214671534876,\n",
       "  4.682388713282925],\n",
       " 'train_acc': [3.0257686491935494,\n",
       "  9.452179939516125,\n",
       "  11.364352318548386,\n",
       "  12.614982358870972,\n",
       "  13.289125504032262,\n",
       "  13.623046875000002,\n",
       "  14.134954637096778,\n",
       "  14.501953125000007,\n",
       "  14.703566028225806,\n",
       "  14.990234375000002,\n",
       "  15.368258568548384,\n",
       "  15.661227318548383,\n",
       "  15.854964717741938,\n",
       "  16.396799395161285,\n",
       "  16.692918346774196,\n",
       "  17.16072328629032,\n",
       "  17.634828629032267,\n",
       "  17.89944556451614,\n",
       "  18.329448084677423,\n",
       "  18.669669858870968],\n",
       " 'val_loss': [7.613911078526424,\n",
       "  7.004817229050856,\n",
       "  6.769168120164138,\n",
       "  6.636771440505982,\n",
       "  6.557459134321947,\n",
       "  6.511043603603657,\n",
       "  6.484455603819627,\n",
       "  6.448257996485784,\n",
       "  6.435615429511437,\n",
       "  6.432023066740769,\n",
       "  6.439559789804312,\n",
       "  6.439962717202995,\n",
       "  6.446129450431237,\n",
       "  6.4573691991659325,\n",
       "  6.452565504954411,\n",
       "  6.448926980678851,\n",
       "  6.4620731610518245,\n",
       "  6.488405814537636,\n",
       "  6.479242746646589,\n",
       "  6.48716009580172],\n",
       " 'val_acc': [7.158954326923076,\n",
       "  10.223858173076922,\n",
       "  11.70372596153846,\n",
       "  12.620192307692308,\n",
       "  13.45402644230769,\n",
       "  13.536658653846153,\n",
       "  13.581730769230768,\n",
       "  14.212740384615387,\n",
       "  14.002403846153845,\n",
       "  14.024939903846152,\n",
       "  14.04747596153846,\n",
       "  14.325420673076922,\n",
       "  14.197716346153848,\n",
       "  14.460637019230768,\n",
       "  14.423076923076922,\n",
       "  14.693509615384615,\n",
       "  14.678485576923078,\n",
       "  14.438100961538463,\n",
       "  14.595853365384613,\n",
       "  14.75360576923077],\n",
       " 'test_loss': 7.265490091764011,\n",
       " 'test_acc': 13.168569711538463,\n",
       " 'model_filename': 'model.pth'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9403cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(results):\n",
    "    \"\"\"\n",
    "    임베딩 결과를 출력합니다\n",
    "    \"\"\"\n",
    "    for item in results:\n",
    "        print (\"...[%.2f] - %s\"%(item[1], item[0]))\n",
    "\n",
    "def get_closest(target_word, word_to_idx, embeddings, n=5):\n",
    "    \"\"\"\n",
    "    n개의 최근접 단어를 찾습니다.\n",
    "    \"\"\"\n",
    "\n",
    "    # 다른 모든 단어까지 거리를 계산합니다\n",
    "    word_embedding = embeddings[word_to_idx[target_word.lower()]]\n",
    "    distances = []\n",
    "    for word, index in word_to_idx.items():\n",
    "        if word == \"<MASK>\" or word == target_word:\n",
    "            continue\n",
    "        distances.append((word, torch.dist(word_embedding, embeddings[index])))\n",
    "    \n",
    "    results = sorted(distances, key=lambda x: x[1])[1:n+2]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00dcde86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어를 입력해 주세요: of\n",
      "...[6.83] - pained\n",
      "...[6.89] - extremity\n",
      "...[6.92] - dreaming\n",
      "...[7.05] - contradictory\n",
      "...[7.10] - elect\n",
      "...[7.27] - expressed\n"
     ]
    }
   ],
   "source": [
    "word = input('단어를 입력해 주세요: ')\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = cbow_vocab.token_to_idx\n",
    "pretty_print(get_closest(word, word_to_idx, embeddings, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d908444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======frankenstein=======\n",
      "...[6.67] - shuddered\n",
      "...[6.81] - ingratitude\n",
      "...[6.85] - mannheim\n",
      "...[6.92] - thoughts\n",
      "...[6.96] - sincerity\n",
      "...[7.01] - dragging\n",
      "=======monster=======\n",
      "...[6.46] - deserted\n",
      "...[6.55] - distinguished\n",
      "...[6.57] - recovery\n",
      "...[6.58] - almighty\n",
      "...[6.62] - inclemency\n",
      "...[6.63] - contrary\n",
      "=======science=======\n",
      "...[7.12] - playfellow\n",
      "...[7.13] - meeting\n",
      "...[7.19] - breezes\n",
      "...[7.25] - teachers\n",
      "...[7.27] - bestowing\n",
      "...[7.32] - struck\n",
      "=======sickness=======\n",
      "...[7.01] - destroy\n",
      "...[7.14] - somewhat\n",
      "...[7.19] - copy\n",
      "...[7.31] - doth\n",
      "...[7.32] - bent\n",
      "...[7.38] - classes\n",
      "=======lonely=======\n",
      "...[6.41] - luxury\n",
      "...[6.61] - account\n",
      "...[6.66] - moulded\n",
      "...[6.66] - clasped\n",
      "...[6.68] - married\n",
      "...[6.70] - juras\n",
      "=======happy=======\n",
      "...[6.46] - keep\n",
      "...[6.53] - stump\n",
      "...[6.54] - belonging\n",
      "...[6.56] - brutality\n",
      "...[6.74] - intoxicating\n",
      "...[6.74] - avidity\n"
     ]
    }
   ],
   "source": [
    "target_words = ['frankenstein', 'monster', 'science', 'sickness', 'lonely', 'happy']\n",
    "\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = cbow_vocab.token_to_idx\n",
    "\n",
    "for target_word in target_words: \n",
    "    print(f\"======={target_word}=======\")\n",
    "    if target_word not in word_to_idx:\n",
    "        print(\"Not in vocabulary\")\n",
    "        continue\n",
    "    pretty_print(get_closest(target_word, word_to_idx, embeddings, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d2c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8e8b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
