{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9abe6618",
   "metadata": {},
   "source": [
    "# 예제: CBOW 임베딩 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad1892f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>, or the</td>\n",
       "      <td>frankenstein</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frankenstein or the modern</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frankenstein , the modern prometheus</td>\n",
       "      <td>or</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frankenstein , or modern prometheus by</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>, or the prometheus by mary</td>\n",
       "      <td>modern</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90693</th>\n",
       "      <td>newsletter to hear new ebooks .</td>\n",
       "      <td>about</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90694</th>\n",
       "      <td>to hear about ebooks .</td>\n",
       "      <td>new</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90695</th>\n",
       "      <td>hear about new .</td>\n",
       "      <td>ebooks</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90696</th>\n",
       "      <td>about new ebooks</td>\n",
       "      <td>.</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90697</th>\n",
       "      <td>new ebooks .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      context        target  split\n",
       "0                                    , or the  frankenstein  train\n",
       "1                  frankenstein or the modern             ,  train\n",
       "2        frankenstein , the modern prometheus            or  train\n",
       "3      frankenstein , or modern prometheus by           the  train\n",
       "4                 , or the prometheus by mary        modern  train\n",
       "...                                       ...           ...    ...\n",
       "90693         newsletter to hear new ebooks .         about   test\n",
       "90694                 to hear about ebooks .            new   test\n",
       "90695                       hear about new .         ebooks   test\n",
       "90696                       about new ebooks              .   test\n",
       "90697                            new ebooks .           NaN   test\n",
       "\n",
       "[90698 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../data/frankenstein_with_splits.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa05145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 다시 train/valid/test로 나눠줌\n",
    "\n",
    "# train 데이터 \n",
    "train_df = df[df.split=='train']\n",
    "train_size = len(train_df)\n",
    "\n",
    "# valid 데이터 \n",
    "val_df = df[df.split=='val']\n",
    "val_size = len(val_df)\n",
    "\n",
    "# test 데이터 \n",
    "test_df = df[df.split=='test']\n",
    "test_size = len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be1d3f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  63489\n",
      "valid :  13605\n",
      "test :  13604\n"
     ]
    }
   ],
   "source": [
    "print(\"train : \",train_size)\n",
    "print(\"valid : \",val_size)\n",
    "print(\"test : \",test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ec341cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dict = {'train': (train_df, train_size),\n",
    "                             'val': (val_df, val_size),\n",
    "                             'test': (test_df, test_size)}\n",
    "# lookup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5109dd35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63479</th>\n",
       "      <td>i be alone</td>\n",
       "      <td>?</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63480</th>\n",
       "      <td>be alone ?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63481</th>\n",
       "      <td>had feelings of</td>\n",
       "      <td>i</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63482</th>\n",
       "      <td>i feelings of affection</td>\n",
       "      <td>had</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63483</th>\n",
       "      <td>i had of affection ,</td>\n",
       "      <td>feelings</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63484</th>\n",
       "      <td>i had feelings affection , and</td>\n",
       "      <td>of</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63485</th>\n",
       "      <td>had feelings of , and they</td>\n",
       "      <td>affection</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63486</th>\n",
       "      <td>feelings of affection and they were</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63487</th>\n",
       "      <td>of affection , they were requited</td>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63488</th>\n",
       "      <td>affection , and were requited by</td>\n",
       "      <td>they</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   context     target  split\n",
       "63479                          i be alone           ?  train\n",
       "63480                           be alone ?        NaN  train\n",
       "63481                      had feelings of          i  train\n",
       "63482              i feelings of affection        had  train\n",
       "63483                 i had of affection ,   feelings  train\n",
       "63484       i had feelings affection , and         of  train\n",
       "63485           had feelings of , and they  affection  train\n",
       "63486  feelings of affection and they were          ,  train\n",
       "63487    of affection , they were requited        and  train\n",
       "63488     affection , and were requited by       they  train"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d30142",
   "metadata": {},
   "source": [
    "### 2. Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fae986f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, mask_token=\"<MASK>\",add_unk=True):\n",
    "        self.token_to_idx = {}\n",
    "        self.idx_to_token = {}\n",
    "        \n",
    "        self.mask_index = self.add_token(mask_token)\n",
    "#         \"UNK\" 토큰이 추가되지 않는 경우에는 -1로 설정,\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "#         \"UNK\" 토큰이 추가될 경우에는 UNK에 해당하는 인덱스로 설정,\n",
    "            self.unk_index = self.add_token('<UNK>') \n",
    "\n",
    "    def add_token(self, token):\n",
    "        \n",
    "#       만약 해당 토큰이 있으면 토큰 idx만 return\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "            \n",
    "#       만약 해당 토큰이 없으면 새로운 토큰 만들어줌\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9daef709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW Vocabulary 객체 생성\n",
    "\n",
    "cbow_vocab = Vocabulary()\n",
    "for index, row in df.iterrows():\n",
    "    for token in row.context.split(' '):\n",
    "        cbow_vocab.add_token(token)\n",
    "    cbow_vocab.add_token(row.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "714ba3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<MASK>': 0, '<UNK>': 1, ',': 2, 'or': 3, 'the': 4}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(cbow_vocab.token_to_idx.items())[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59bd61b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<MASK>', 1: '<UNK>', 2: ',', 3: 'or', 4: 'the'}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(cbow_vocab.idx_to_token.items())[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8233872a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7270\n"
     ]
    }
   ],
   "source": [
    "print(len(cbow_vocab.token_to_idx.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ee407f",
   "metadata": {},
   "source": [
    "## 3. Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "166fd3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 토큰에 대응하는 인덱스 반환\n",
    "\n",
    "def lookup_token(vocabulary_class,token):\n",
    "\n",
    "# UNK 토큰이 있을 경우\n",
    "    if vocabulary_class.unk_index >= 0:\n",
    "#           토큰을 찾아보고 없으면 unk_index 반환, 있으면 해당 토큰의 idx를 반환\n",
    "        return vocabulary_class.token_to_idx.get(token, vocabulary_class.unk_index)\n",
    "    else:\n",
    "        return vocabulary_class.token_to_idx[token]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db2d52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 인덱스에 대응하는 토큰 반환\n",
    "\n",
    "def lookup_index(vocabulary_class, index):\n",
    "        if index not in vocabulary_class.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return vocabulary_class.idx_to_token[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6853d180",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1764d004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[215   1   1   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def vectorize(context, vector_length=-1):\n",
    "    \n",
    "    indices = [lookup_token(cbow_vocab,token) for token in context.split(' ')]\n",
    "    if vector_length < 0:\n",
    "        vector_length = len(indices)\n",
    "    \n",
    "    \n",
    "#   인덱스와 mask로 이루어진 out_vector를 만든다.\n",
    "    out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "    out_vector[:len(indices)] = indices\n",
    "#     문장 길이가 작아서 padding 진행하면 해당 부분 mask 처리\n",
    "    out_vector[len(indices):] = cbow_vocab.mask_index\n",
    "    \n",
    "    \n",
    "    return out_vector\n",
    "\n",
    "# vector_length 보다 문장의 토큰의 개수가 작으면 MASK 처리된다. \n",
    "print(vectorize(\"all dafs dfkdl\",vector_length=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff427e2",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d828249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_seq_length를 구해야 한다. \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, cbow_df):\n",
    "        \n",
    "        self.cbow_df = cbow_df\n",
    "        \n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self.max_seq_length = max(map(measure_len, cbow_df.context))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cbow_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        row = self.cbow_df.iloc[index]\n",
    "\n",
    "        context_vector = vectorize(row.context, self.max_seq_length)\n",
    "        target_index = lookup_token(cbow_vocab,row.target)\n",
    "\n",
    "        return {'x_data': context_vector,\n",
    "                'y_target': target_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70ee0b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CBOWDataset at 0x7fd4d3250bb0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋을 인스턴스화 해주어야 로더에 넣어줄 수 있다. \n",
    "\n",
    "train_dataset = CBOWDataset(train_df)\n",
    "train_dataset\n",
    "\n",
    "valid_dataset = CBOWDataset(val_df)\n",
    "valid_dataset\n",
    "\n",
    "test_dataset = CBOWDataset(test_df)\n",
    "test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "534c52d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61d41de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(valid_dataset.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9c60545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abd30122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더 설정\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# drop_last=True -> 배치 사이즈보다 over하면 drop\n",
    "\n",
    "Traindataloader = DataLoader(dataset=train_dataset, batch_size=512,\n",
    "                            shuffle=True, drop_last=True)\n",
    "\n",
    "Validdataloader = DataLoader(dataset=valid_dataset, batch_size=512,\n",
    "                            shuffle=False, drop_last=True)\n",
    "\n",
    "Testdataloader = DataLoader(dataset=test_dataset, batch_size=512,\n",
    "                            shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ba7f661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63489 124\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset),len(Traindataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ed795cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "{'x_data': tensor([[ 115, 2917,   82,   15,   43,    0],\n",
      "        [  72, 5566,   33,  176,  239,   48],\n",
      "        [  33,  230, 1651,  225,  212,  908],\n",
      "        ...,\n",
      "        [ 642, 1164,   19,   15,   43,    0],\n",
      "        [5232,   33, 1326,   19,    4, 5233],\n",
      "        [ 496, 1359,   49,   33,  688, 1482]]), 'y_target': tensor([1383,    4,    2, 4968, 1166,   33,    4,  426,    8, 1844,  295,  698,\n",
      "        3138,  676,   49,    4,   39,  273,  115,   19,    4,   33,   44,  209,\n",
      "         239, 4081, 3854,  766,   39,    4,   19,  282,  377,   72, 1380, 1533,\n",
      "         377,    2, 3451,  319,  192,  474,   39,  749,  334, 1160,  255,   85,\n",
      "         337,   33,   60,  682,   86, 5513,   44,   19, 3228,  806, 1752,  204,\n",
      "          49,  298,   19,   44,    2,   19, 1716,   33, 1651, 1229,   19, 2222,\n",
      "         760,  515,   53,  230, 4213,  226,   72,   60,   44,  180,    2,    4,\n",
      "          23,  406,  349, 3228,   15,  319,    4,  226, 5929,   72,  345,  590,\n",
      "         732,  656,   48, 3412,  870,   39,   44,   53,  300,   50,  726, 1801,\n",
      "        1752,   33,  157, 1885,   44, 2276,    4,  319,  334,   19,  963,   23,\n",
      "         319,   53,  766,  467,   15, 4368, 2774, 1935, 2532,   15,    2, 1082,\n",
      "          90,  384,   60, 3498,   51, 1377,   48,    2,   48,    2,   15, 3370,\n",
      "           4,   44, 2914,   23,  173,  421,  796,  689,    2,  804, 5152,  160,\n",
      "          88, 1195,   19,   19,  474,   72,   50,   49,    4,   72,   50,    2,\n",
      "         349,   48,   50, 1204,  785,   33,   48,  104,   19,    2,    4,   72,\n",
      "           8, 1169,   44,   96,   68, 1705,  552,   33, 4969, 5795,  145,  290,\n",
      "         319,  225,   48,   49,    4, 1234, 1523,   49,   15,   82,   33,    4,\n",
      "          82,   82,   33,  496,   33,  203, 1169, 1611,  228,  152, 1280,  104,\n",
      "         173, 2547,   50,   27,  417,  173,   60,   60,  557,  369,  607,   50,\n",
      "         178,  209,   19,   44,   44,   44,  313, 1234,  414,  264,   15,  273,\n",
      "          23,  595,   44,   49,   49,   33, 1082,  205,  550, 1082,   23,  290,\n",
      "        4132,   49,   50,   15,  455, 1308,  255,   49,  729, 4568,   82, 1254,\n",
      "           2,  300,    4,    2, 4104,  104,  136,   15, 4388,    2,  223,    4,\n",
      "          19,   82,   44, 5929,    4,  192,   44,   15, 2226, 1437, 3309,  145,\n",
      "           4,   44,   33, 1082, 2846,  689,  213,  981,   72, 2432,   19,   86,\n",
      "        2164,   48,  273,   15, 1332,  202,   44, 4814, 1174,  319,   50,  109,\n",
      "        3507,  334, 3138,   49,   36, 1484, 3338,  255, 2201,   15,    4,  939,\n",
      "         648, 4603,  474,  319,   50, 2302,   44,  656,  300,    4,  157,   36,\n",
      "        1668, 2595,    2, 1209,    4, 1798,   48,   50,   15,    8, 4518,   44,\n",
      "          44, 1267, 3295,   60,   49,   90, 2969,   33, 1234, 3847,   19,  781,\n",
      "         452,    8,  554,    2,   44, 2339,   44,    2,   82,   49, 3025,   39,\n",
      "         123,   51,   60,  471, 3795, 1008,    2, 2553,    4,  137,   53,   68,\n",
      "         209,    2,    4, 2928,  474,   68,   27, 1407,   33,  215,  471,  907,\n",
      "          15,   23,  750, 2129,  729,    2,    2,  230,   48,   49, 3644, 5742,\n",
      "         104,   49,  998,   33,  319,  632, 1586,   19,  766,  334, 4233,  875,\n",
      "          33,   15, 5729, 1867,    4,   49,  971,   15,  239,   48,   72,   72,\n",
      "          60,   50,  345,   19,   33, 2582,  319, 4901,   44, 1619, 2287,    2,\n",
      "          35, 1061,   44,  226, 1402,   44,   68,   15,   72, 1074, 1084,   88,\n",
      "        4348,  157,   49, 4035,  226,   55, 2151,  719,    4,   72,   19,  724,\n",
      "         398,   49,  828, 1552, 4595,  666, 2753,    4,  127, 5833,  983,  363,\n",
      "           2, 1757,   19, 1629,   49,  226, 4175,   82, 3035,    4,   44,    4,\n",
      "          49,    2, 1169,   82, 1857, 1930, 5642,   49,  774,  715, 4258, 1164,\n",
      "         603,  226, 4669,   49, 4454,   82,  341, 1818])}\n"
     ]
    }
   ],
   "source": [
    "for batch_index, batch_dict in reversed(list(enumerate(Traindataloader))):\n",
    "    print(batch_index)\n",
    "    print(batch_dict)\n",
    "    \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0790db50",
   "metadata": {},
   "source": [
    "### 모델정의 NameClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5558da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_size를 지정해줘야 한다.\n",
    "# 1. Embedding layer를 이용해서 문맥의 단어를 나타내는 인덱스를 각 단어에 대한 벡터로 만든다.\n",
    "# 2. 전반적인 문맥을 감지하도록 벡터를 결합한다.(sum)\n",
    " \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CBOWClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    매개변수:\n",
    "        vocabulary_size (int): 어휘 사전 크기, 임베딩 개수와 예측 벡터 크기를 결정합니다\n",
    "        embedding_size (int): 임베딩 크기\n",
    "        padding_idx (int): 기본값 0; 임베딩은 이 인덱스를 사용하지 않습니다\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocabulary_size, embedding_size, padding_idx=0):\n",
    "        \n",
    "#       torch.nn.Module의 초기화 메서드를 실행하여 해당 클래스의 기능을 상속받음\n",
    "        super(CBOWClassifier, self).__init__()\n",
    "        # Embedding layer를 이용해서 문맥의 단어를 나타내는 인덱스를 각 단어에 대한 벡터로 만듦\n",
    "        \n",
    "        self.embedding =  nn.Embedding(num_embeddings=vocabulary_size, \n",
    "                                       embedding_dim=embedding_size,\n",
    "                                       padding_idx=padding_idx)\n",
    "#         embedding에서 나오면 embedding_size * vocab_size 차원이 된다. \n",
    "        self.fc1 = nn.Linear(in_features=embedding_size,\n",
    "                             out_features=vocabulary_size)\n",
    "    \n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        # 전반적인 문맥을 감지하도록 벡터를 결합함(sum)\n",
    "        x_embedded_sum = F.dropout(self.embedding(x_in).sum(dim=1), 0.3)\n",
    "        y_out = self.fc1(x_embedded_sum)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "            \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "110c5bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOWClassifier(\n",
       "  (embedding): Embedding(7270, 50, padding_idx=0)\n",
       "  (fc1): Linear(in_features=50, out_features=7270, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_size를 지정해줘야 한다. \n",
    "embedding_size=50 \n",
    "\n",
    "classifier = CBOWClassifier(vocabulary_size=len(cbow_vocab.token_to_idx), \n",
    "                            embedding_size=embedding_size)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b523901b",
   "metadata": {},
   "source": [
    "### 모델 구조 뜯어보기(번외)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "94e56229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "미니배치 0의 임베딩 결과:\n",
      "torch.Size([512, 6, 50])\n",
      "tensor([[[-0.7889, -0.7210, -0.7649,  ...,  0.7189,  0.8779,  0.1937],\n",
      "         [ 0.6209,  1.1932,  0.3220,  ..., -0.4130,  0.3254, -1.3003],\n",
      "         [-0.9514,  2.3714, -0.8090,  ...,  0.2681, -1.5132,  1.7616],\n",
      "         [-0.8146,  0.2504,  1.0204,  ..., -0.0242,  0.5439, -0.0686],\n",
      "         [-1.3744,  0.5519,  0.4760,  ...,  1.2907,  0.5133, -0.5947],\n",
      "         [ 2.2647, -1.1518,  0.4901,  ..., -0.8892, -0.9164,  0.1901]],\n",
      "\n",
      "        [[-1.2474, -0.1531, -0.4585,  ..., -0.1053, -1.6733, -1.3185],\n",
      "         [-0.1541,  1.0257, -1.8475,  ..., -0.4574, -0.8569,  0.6387],\n",
      "         [ 0.0754,  0.3734, -1.3440,  ...,  0.6772, -1.1051,  1.0829],\n",
      "         [-0.5872, -1.6810,  0.9565,  ..., -0.4432, -1.7029,  1.0940],\n",
      "         [ 1.4295,  1.0614,  0.6999,  ...,  1.1473, -0.4687,  1.0768],\n",
      "         [-0.1541,  1.0257, -1.8475,  ..., -0.4574, -0.8569,  0.6387]],\n",
      "\n",
      "        [[-0.8634,  0.6928, -0.7762,  ..., -0.2246,  0.7483,  0.4033],\n",
      "         [-0.6907, -0.8391,  0.6456,  ..., -0.9123,  0.5243, -0.1573],\n",
      "         [-1.6232, -1.3897, -1.0328,  ...,  1.1573,  0.9717,  2.4791],\n",
      "         [ 0.3409, -0.2702,  1.1305,  ...,  0.0978,  0.0357, -0.0813],\n",
      "         [-1.2343,  0.5136,  0.9402,  ..., -0.0957, -0.5740,  0.8504],\n",
      "         [-0.0583, -1.2754,  1.0114,  ...,  2.1730, -0.6088,  1.2727]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.7425, -0.0943,  1.1839,  ...,  0.7776, -0.6535,  0.4218],\n",
      "         [-0.4846,  0.0316,  1.0157,  ...,  0.1718, -0.0671, -1.1902],\n",
      "         [-0.5889,  1.8170,  0.3181,  ..., -0.0061,  0.1302, -0.2569],\n",
      "         [ 0.5111,  0.0402, -0.9335,  ..., -0.4540,  0.4699,  0.5937],\n",
      "         [-0.4980,  0.5566,  0.4389,  ...,  0.8931,  0.7855,  0.5661],\n",
      "         [ 0.8890, -0.1600, -0.0091,  ..., -1.4424,  2.0882, -0.0111]],\n",
      "\n",
      "        [[-1.7425, -0.0943,  1.1839,  ...,  0.7776, -0.6535,  0.4218],\n",
      "         [ 0.3409, -0.2702,  1.1305,  ...,  0.0978,  0.0357, -0.0813],\n",
      "         [ 0.5880, -0.9409,  1.3191,  ..., -0.2136,  0.1742, -0.7123],\n",
      "         [-1.2428,  1.5540,  2.0999,  ..., -1.6607,  2.2484,  0.6807],\n",
      "         [ 0.0730,  0.1296,  2.2095,  ..., -1.9645, -0.4689, -1.4479],\n",
      "         [ 0.5394,  0.1900,  3.2225,  ..., -1.2168,  1.3054,  1.4979]],\n",
      "\n",
      "        [[ 0.4994,  2.4460, -0.6155,  ...,  0.4389, -0.4842, -0.2727],\n",
      "         [-0.6907, -0.8391,  0.6456,  ..., -0.9123,  0.5243, -0.1573],\n",
      "         [ 1.1062, -0.1268, -0.3623,  ...,  1.2378, -0.8946, -2.2847],\n",
      "         [-0.7204, -0.1484,  0.2421,  ...,  0.5970,  0.4435, -0.2038],\n",
      "         [-0.6907, -0.8391,  0.6456,  ..., -0.9123,  0.5243, -0.1573],\n",
      "         [-0.8547,  0.6681, -1.9133,  ...,  0.3467,  0.7366,  0.5141]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      " \n",
      "미니배치 0의 임베딩 sum 결과:\n",
      "torch.Size([512, 50])\n",
      "tensor([[-1.0437,  2.4941,  0.7347,  ...,  0.9513, -0.1691,  0.1818],\n",
      "        [-0.6379,  1.6523, -3.8411,  ...,  0.3612, -6.6638,  3.2125],\n",
      "        [-4.1289, -2.5679,  1.9186,  ...,  2.1954,  1.0973,  4.7670],\n",
      "        ...,\n",
      "        [-1.9140,  2.1911,  2.0139,  ..., -0.0600,  2.7532,  0.1235],\n",
      "        [-1.4440,  0.5681, 11.1653,  ..., -4.1802,  2.6413,  0.3589],\n",
      "        [-1.3510,  1.1607, -1.3577,  ...,  0.7958,  0.8499, -2.5616]],\n",
      "       grad_fn=<SumBackward1>)\n",
      " \n",
      "미니배치 0의 임베딩 dropout 결과:\n",
      "torch.Size([512, 50])\n",
      "tensor([[-1.4909,  3.5630,  0.0000,  ...,  0.0000, -0.2415,  0.2597],\n",
      "        [-0.9112,  2.3604, -0.0000,  ...,  0.5160, -0.0000,  0.0000],\n",
      "        [-5.8985, -3.6685,  0.0000,  ...,  3.1363,  1.5675,  6.8100],\n",
      "        ...,\n",
      "        [-2.7342,  0.0000,  2.8770,  ..., -0.0000,  3.9331,  0.1764],\n",
      "        [-0.0000,  0.8116, 15.9504,  ..., -5.9717,  3.7733,  0.0000],\n",
      "        [-1.9300,  0.0000, -1.9396,  ...,  1.1368,  1.2142, -3.6594]],\n",
      "       grad_fn=<MulBackward0>)\n",
      " \n",
      "미니배치 0의 fc1 결과:\n",
      "torch.Size([512, 7270])\n",
      "tensor([[-0.9729,  0.1527,  1.6448,  ..., -0.5564, -0.3527, -3.8191],\n",
      "        [-3.0437, -0.4823, -2.2815,  ..., -2.3132, -1.1192,  2.1428],\n",
      "        [-1.2133, -0.4542,  0.8635,  ..., -1.0864,  1.7446,  1.0944],\n",
      "        ...,\n",
      "        [-0.5324, -0.3920,  2.0183,  ..., -0.6921, -1.5134, -2.0867],\n",
      "        [ 4.2615,  3.8399, -4.0185,  ...,  1.7990, -1.7112, -0.5972],\n",
      "        [-0.3855, -1.6581,  0.2672,  ...,  1.4251,  2.7618,  0.7693]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocabulary_size = len(cbow_vocab.token_to_idx) \n",
    "embedding_size = embedding_size  \n",
    "embedding_size=50\n",
    "padding_idx=0\n",
    "\n",
    "embedding =  nn.Embedding(num_embeddings=vocabulary_size, \n",
    "                           embedding_dim=embedding_size,\n",
    "                           padding_idx=padding_idx)\n",
    "\n",
    "fc1 = nn.Linear(in_features=embedding_size,\n",
    "                     out_features=vocabulary_size)\n",
    "    \n",
    "\n",
    "# 데이터 로더에서 미니배치를 반복합니다.\n",
    "for batch_index, batch in enumerate(Traindataloader):\n",
    "    # 각 미니배치에서 샘플을 가져옵니다.\n",
    "    x_in = batch['x_data']\n",
    "    y_in = batch['y_target']\n",
    "    \n",
    "    \n",
    "    embedded_sample = embedding(x_in)\n",
    "    \n",
    "    # 임베딩 결과를 출력합니다.\n",
    "    print(f\"미니배치 {batch_index}의 임베딩 결과:\")\n",
    "    print(embedded_sample.size())\n",
    "    print(embedded_sample)\n",
    "    \n",
    "    \n",
    "    sum_result = embedding(x_in).sum(dim=1)\n",
    "    \n",
    "    print(\" \")\n",
    "    print(f\"미니배치 {batch_index}의 임베딩 sum 결과:\")\n",
    "    print(sum_result.size())\n",
    "    print(sum_result)\n",
    "    \n",
    "    x_embedded_sum = F.dropout(sum_result, 0.3)\n",
    "    \n",
    "    print(\" \")\n",
    "    print(f\"미니배치 {batch_index}의 임베딩 dropout 결과:\")\n",
    "    print(x_embedded_sum.size())\n",
    "    print(x_embedded_sum)\n",
    "    \n",
    "    y_out = fc1(x_embedded_sum)\n",
    "    \n",
    "    print(\" \")\n",
    "    print(f\"미니배치 {batch_index}의 fc1 결과:\")\n",
    "    print(y_out.size())\n",
    "    print(y_out)\n",
    "    \n",
    "    y_out = F.softmax(y_out, dim=1)\n",
    "    \n",
    "    print(\" \")\n",
    "    print(f\"미니배치 {batch_index}의 fc1 결과:\")\n",
    "    print(y_out.size())\n",
    "    print(y_out)\n",
    "    \n",
    "    y_out = F.softmax(y_out, dim=1)\n",
    "    \n",
    "    print(\" \")\n",
    "    print(f\"미니배치 {batch_index}의 fc1 결과:\")\n",
    "    print(y_out.size())\n",
    "    print(y_out)\n",
    "    \n",
    "#     y_pred = y_out\n",
    "#     loss =  loss_func(y_pred, y_in)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ad82b",
   "metadata": {},
   "source": [
    "### 옵티마이저, loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13216f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5eaa3504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 옵티마이저\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = lr)\n",
    "optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b32b926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31608c79",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75e5f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "#      예측값과 타겟값을 비교하여 일치하는 개수를 계산\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ef89fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train state 초기화 \n",
    "def make_train_state():\n",
    "    return {\n",
    "        'stop_early':False,\n",
    "        'early_stopping_step':0,\n",
    "        'early_stopping_best_val':1e8,\n",
    "        'early_stopping_criteria' : 10,\n",
    "        'epoch_index' : 0,\n",
    "        'train_loss': [], \n",
    "        'train_acc' :[], \n",
    "        'val_loss' : [],\n",
    "        'val_acc' : [], \n",
    "        'test_loss' : [],\n",
    "        'test_acc' : [],\n",
    "         \n",
    "#       모델 저장파일\n",
    "        'model_filename' : 'model.pth'\n",
    "    } \n",
    "\n",
    "\n",
    "# Train update \n",
    "def update_train_state(model, train_state):\n",
    "    \n",
    "#   학습시작하면 초기에 모델 저장하기 \n",
    "    \n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(),train_state['model_filename'])\n",
    "        \n",
    "#   모델 성능이 향상되면 모델 저장(valid loss가 더 낮아지면)\n",
    "    elif train_state['epoch_index'] >=1 :\n",
    "        loss_t = train_state['val_loss'][-1]\n",
    "#        loss가 나빠지면 early stop step 업데이트\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            train_state['early_stopping_step']+=1\n",
    "            \n",
    "#        loss가 좋아지면   \n",
    "        else:\n",
    "#            early stop step 0으로 다시 초기화        \n",
    "            train_state['early_stopping_step']=0\n",
    "    \n",
    "#           최저 loss이면 모델 저장 \n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "                torch.save(model.state_dict(),train_state['model_filename'])\n",
    "\n",
    "#       기준점 넘으면 early stop \n",
    "        if train_state['early_stopping_step'] >= train_state['early_stopping_criteria']:\n",
    "            train_state['stop_early'] = True\n",
    "        \n",
    "        return train_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66b6b449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop_early': False,\n",
       " 'early_stopping_step': 0,\n",
       " 'early_stopping_best_val': 100000000.0,\n",
       " 'early_stopping_criteria': 10,\n",
       " 'epoch_index': 0,\n",
       " 'train_loss': [],\n",
       " 'train_acc': [],\n",
       " 'val_loss': [],\n",
       " 'val_acc': [],\n",
       " 'test_loss': [],\n",
       " 'test_acc': [],\n",
       " 'model_filename': 'model.pth'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 진행 상황 함수 초기화\n",
    "train_state = make_train_state()\n",
    "train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ef8576c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                          | 1/100 [00:07<11:36,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 7.613911078526424\n",
      "val_acc 7.158954326923076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▊                                          | 2/100 [00:13<11:03,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 7.004817229050856\n",
      "val_acc 10.223858173076922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|█▎                                         | 3/100 [00:20<11:07,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.769168120164138\n",
      "val_acc 11.70372596153846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|█▋                                         | 4/100 [00:27<11:09,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.636771440505982\n",
      "val_acc 12.620192307692308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|██▏                                        | 5/100 [00:34<10:52,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.557459134321947\n",
      "val_acc 13.45402644230769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|██▌                                        | 6/100 [00:41<10:41,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.511043603603657\n",
      "val_acc 13.536658653846153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|███                                        | 7/100 [00:47<10:34,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.484455603819627\n",
      "val_acc 13.581730769230768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|███▍                                       | 8/100 [00:54<10:19,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.448257996485784\n",
      "val_acc 14.212740384615387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███▊                                       | 9/100 [01:01<10:15,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.435615429511437\n",
      "val_acc 14.002403846153845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▏                                     | 10/100 [01:09<10:38,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.432023066740769\n",
      "val_acc 14.024939903846152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████▌                                     | 11/100 [01:16<10:29,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.439559789804312\n",
      "val_acc 14.04747596153846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████                                     | 12/100 [01:23<10:21,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.439962717202995\n",
      "val_acc 14.325420673076922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█████▍                                    | 13/100 [01:29<10:05,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.446129450431237\n",
      "val_acc 14.197716346153848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█████▉                                    | 14/100 [01:36<09:51,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.4573691991659325\n",
      "val_acc 14.460637019230768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|██████▎                                   | 15/100 [01:43<09:40,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.452565504954411\n",
      "val_acc 14.423076923076922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|██████▋                                   | 16/100 [01:50<09:47,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.448926980678851\n",
      "val_acc 14.693509615384615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████▏                                  | 17/100 [01:57<09:30,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.4620731610518245\n",
      "val_acc 14.678485576923078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|███████▌                                  | 18/100 [02:04<09:25,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.488405814537636\n",
      "val_acc 14.438100961538463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████▉                                  | 19/100 [02:11<09:22,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.479242746646589\n",
      "val_acc 14.595853365384613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████▉                                  | 19/100 [02:18<09:52,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.48716009580172\n",
      "val_acc 14.75360576923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "# 에포크만큼\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "\n",
    "#     print('epoch',epoch)\n",
    "#     print(train_state['epoch_index']) \n",
    "    train_state['epoch_index'] +=1 \n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "\n",
    "#     모델을 학습 모드로 설정 -> 드롭아웃 및 배치 정규화와 같은 학습 중에만 적용되는 기법들이 활성화\n",
    "#     모델을 평가 모드로 전환하려면 classifier.eval()을 사용\n",
    "    classifier.train()\n",
    "# 배치 만큼\n",
    "    for batch_idx, batch_data in enumerate(Traindataloader):\n",
    "\n",
    "        \n",
    "\n",
    "#       1. 옵티마이저 그레디언트 0으로 초기화\n",
    "        optimizer.zero_grad()\n",
    "#       2. 모델에 데이터 넣어서 출력받기\n",
    "        y_pred = classifier(x_in=batch_data['x_data'])\n",
    "#       3. loss 계산하기\n",
    "        loss =  loss_func(y_pred, batch_data['y_target'])\n",
    "    \n",
    "#       tensor(0.3190) -> 0.3190, item()으로 스칼라 값만 추출\n",
    "        loss_t = loss.item()\n",
    "\n",
    "#       배치에서의 평균 loss 구하기\n",
    "        running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "\n",
    "#       4. gradient 계산하기\n",
    "        loss.backward()\n",
    "\n",
    "#       5. 옵티마이저 가중치 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "#       Accuracy 계산\n",
    "        acc_t = compute_accuracy(y_pred, batch_data['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "\n",
    "\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "\n",
    "\n",
    "#   valid에 대한 계산\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "    classifier.eval() # 모델 파라미터를 수정하지 못 하게 비활성화\n",
    "\n",
    "    for batch_idx, batch_data in enumerate(Validdataloader):\n",
    "\n",
    "#       1. 모델의 출력값(y_pred)계산\n",
    "        y_pred = classifier(x_in=batch_data['x_data'])\n",
    "\n",
    "#       2. loss 계산\n",
    "        loss = loss_func(y_pred,batch_data['y_target'])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "\n",
    "#       3. Accuracy 계산\n",
    "        acc_t = compute_accuracy(y_pred,batch_data['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "    \n",
    "    print(\"val_loss\",running_loss)\n",
    "    print(\"val_acc\",running_acc)\n",
    "\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "    \n",
    "\n",
    "#   전체 loss, acc 저장\n",
    "    train_state = update_train_state(model=classifier,\n",
    "                                     train_state=train_state)\n",
    "#   early stop해라고 했으면 학습 멈추기    \n",
    "    if train_state['stop_early']:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037c61f7",
   "metadata": {},
   "source": [
    "### Test 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "331de996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 좋은 모델을 사용해 테스트 세트의 손실과 정확도를 계산합니다\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "\n",
    "# 가중치 업데이트 하지 못 하게\n",
    "classifier.eval()\n",
    "\n",
    "for batch_idx, batch_data in enumerate(Testdataloader):\n",
    "    \n",
    "    y_pred = classifier(x_in=batch_data['x_data'])\n",
    "    loss = loss_func(y_pred,batch_data['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "    \n",
    "    acc_t = compute_accuracy(y_pred, batch_data['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74df48a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 7.265\n",
      "테스트 정확도: 13.17\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 손실: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {:.2f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "342a2c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop_early': True,\n",
       " 'early_stopping_step': 10,\n",
       " 'early_stopping_best_val': 6.432023066740769,\n",
       " 'early_stopping_criteria': 10,\n",
       " 'epoch_index': 20,\n",
       " 'train_loss': [8.603957253117713,\n",
       "  7.111192495592179,\n",
       "  6.5485151544693965,\n",
       "  6.213718360470187,\n",
       "  5.98085355758667,\n",
       "  5.796104023533482,\n",
       "  5.652096659906449,\n",
       "  5.525909473819118,\n",
       "  5.419759035110474,\n",
       "  5.318656921386717,\n",
       "  5.229448260799533,\n",
       "  5.148317594682014,\n",
       "  5.074486763246597,\n",
       "  5.004274406740742,\n",
       "  4.942227005958556,\n",
       "  4.885120161118047,\n",
       "  4.825584430848396,\n",
       "  4.779782606709388,\n",
       "  4.728214671534876,\n",
       "  4.682388713282925],\n",
       " 'train_acc': [3.0257686491935494,\n",
       "  9.452179939516125,\n",
       "  11.364352318548386,\n",
       "  12.614982358870972,\n",
       "  13.289125504032262,\n",
       "  13.623046875000002,\n",
       "  14.134954637096778,\n",
       "  14.501953125000007,\n",
       "  14.703566028225806,\n",
       "  14.990234375000002,\n",
       "  15.368258568548384,\n",
       "  15.661227318548383,\n",
       "  15.854964717741938,\n",
       "  16.396799395161285,\n",
       "  16.692918346774196,\n",
       "  17.16072328629032,\n",
       "  17.634828629032267,\n",
       "  17.89944556451614,\n",
       "  18.329448084677423,\n",
       "  18.669669858870968],\n",
       " 'val_loss': [7.613911078526424,\n",
       "  7.004817229050856,\n",
       "  6.769168120164138,\n",
       "  6.636771440505982,\n",
       "  6.557459134321947,\n",
       "  6.511043603603657,\n",
       "  6.484455603819627,\n",
       "  6.448257996485784,\n",
       "  6.435615429511437,\n",
       "  6.432023066740769,\n",
       "  6.439559789804312,\n",
       "  6.439962717202995,\n",
       "  6.446129450431237,\n",
       "  6.4573691991659325,\n",
       "  6.452565504954411,\n",
       "  6.448926980678851,\n",
       "  6.4620731610518245,\n",
       "  6.488405814537636,\n",
       "  6.479242746646589,\n",
       "  6.48716009580172],\n",
       " 'val_acc': [7.158954326923076,\n",
       "  10.223858173076922,\n",
       "  11.70372596153846,\n",
       "  12.620192307692308,\n",
       "  13.45402644230769,\n",
       "  13.536658653846153,\n",
       "  13.581730769230768,\n",
       "  14.212740384615387,\n",
       "  14.002403846153845,\n",
       "  14.024939903846152,\n",
       "  14.04747596153846,\n",
       "  14.325420673076922,\n",
       "  14.197716346153848,\n",
       "  14.460637019230768,\n",
       "  14.423076923076922,\n",
       "  14.693509615384615,\n",
       "  14.678485576923078,\n",
       "  14.438100961538463,\n",
       "  14.595853365384613,\n",
       "  14.75360576923077],\n",
       " 'test_loss': 7.265490091764011,\n",
       " 'test_acc': 13.168569711538463,\n",
       " 'model_filename': 'model.pth'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9403cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(results):\n",
    "    \"\"\"\n",
    "    임베딩 결과를 출력합니다\n",
    "    \"\"\"\n",
    "    for item in results:\n",
    "        print (\"...[%.2f] - %s\"%(item[1], item[0]))\n",
    "\n",
    "def get_closest(target_word, word_to_idx, embeddings, n=5):\n",
    "    \"\"\"\n",
    "    n개의 최근접 단어를 찾습니다.\n",
    "    \"\"\"\n",
    "\n",
    "    # 다른 모든 단어까지 거리를 계산합니다\n",
    "    word_embedding = embeddings[word_to_idx[target_word.lower()]]\n",
    "    distances = []\n",
    "    for word, index in word_to_idx.items():\n",
    "        if word == \"<MASK>\" or word == target_word:\n",
    "            continue\n",
    "        distances.append((word, torch.dist(word_embedding, embeddings[index])))\n",
    "    \n",
    "    results = sorted(distances, key=lambda x: x[1])[1:n+2]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00dcde86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어를 입력해 주세요: of\n",
      "...[6.83] - pained\n",
      "...[6.89] - extremity\n",
      "...[6.92] - dreaming\n",
      "...[7.05] - contradictory\n",
      "...[7.10] - elect\n",
      "...[7.27] - expressed\n"
     ]
    }
   ],
   "source": [
    "word = input('단어를 입력해 주세요: ')\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = cbow_vocab.token_to_idx\n",
    "pretty_print(get_closest(word, word_to_idx, embeddings, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d908444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======frankenstein=======\n",
      "...[6.67] - shuddered\n",
      "...[6.81] - ingratitude\n",
      "...[6.85] - mannheim\n",
      "...[6.92] - thoughts\n",
      "...[6.96] - sincerity\n",
      "...[7.01] - dragging\n",
      "=======monster=======\n",
      "...[6.46] - deserted\n",
      "...[6.55] - distinguished\n",
      "...[6.57] - recovery\n",
      "...[6.58] - almighty\n",
      "...[6.62] - inclemency\n",
      "...[6.63] - contrary\n",
      "=======science=======\n",
      "...[7.12] - playfellow\n",
      "...[7.13] - meeting\n",
      "...[7.19] - breezes\n",
      "...[7.25] - teachers\n",
      "...[7.27] - bestowing\n",
      "...[7.32] - struck\n",
      "=======sickness=======\n",
      "...[7.01] - destroy\n",
      "...[7.14] - somewhat\n",
      "...[7.19] - copy\n",
      "...[7.31] - doth\n",
      "...[7.32] - bent\n",
      "...[7.38] - classes\n",
      "=======lonely=======\n",
      "...[6.41] - luxury\n",
      "...[6.61] - account\n",
      "...[6.66] - moulded\n",
      "...[6.66] - clasped\n",
      "...[6.68] - married\n",
      "...[6.70] - juras\n",
      "=======happy=======\n",
      "...[6.46] - keep\n",
      "...[6.53] - stump\n",
      "...[6.54] - belonging\n",
      "...[6.56] - brutality\n",
      "...[6.74] - intoxicating\n",
      "...[6.74] - avidity\n"
     ]
    }
   ],
   "source": [
    "target_words = ['frankenstein', 'monster', 'science', 'sickness', 'lonely', 'happy']\n",
    "\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = cbow_vocab.token_to_idx\n",
    "\n",
    "for target_word in target_words: \n",
    "    print(f\"======={target_word}=======\")\n",
    "    if target_word not in word_to_idx:\n",
    "        print(\"Not in vocabulary\")\n",
    "        continue\n",
    "    pretty_print(get_closest(target_word, word_to_idx, embeddings, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d2c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d47ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
