{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a306a3da",
   "metadata": {},
   "source": [
    "### 사전학습된 임베딩 들고오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "684546a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "316b7ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/glove.6B.100d.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f267e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "with open('../../data/glove.6B.100d.txt') as f:\n",
    "    print(len(f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f5fcffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\n",
      "\n",
      "{'the': 0}\n",
      "[array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
      "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
      "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
      "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
      "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
      "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
      "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
      "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
      "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
      "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
      "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
      "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
      "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
      "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
      "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
      "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
      "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ])]\n"
     ]
    }
   ],
   "source": [
    "# 일부만 출력해서 확인하기 \n",
    "word_to_index = {}\n",
    "word_vectors = []\n",
    "\n",
    "with open('../../data/glove.6B.100d.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        print(line)\n",
    "        line = line.split(\" \") # ex) the -0.038194 -0.24487 0.72812 -0.39961 .....\n",
    "        word = line[0] # ex) the\n",
    "        vec = np.array([float(x) for x in line[1:]]) # ex) -0.038194 -0.24487 0.72812 .....\n",
    "\n",
    "        word_to_index[word] = len(word_to_index) # 딕셔너리에 할당, ex) {'the': 0}\n",
    "        word_vectors.append(vec) # ex) [-0.038194 -0.24487 0.72812 ..... ]\n",
    "        \n",
    "        print(word_to_index)\n",
    "        print(word_vectors)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "625b680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {}\n",
    "word_vectors = []\n",
    "\n",
    "with open('../../data/glove.6B.100d.txt') as f:\n",
    "    for line in f.readlines():\n",
    "#         print(line)\n",
    "        line = line.split(\" \") # ex) the -0.038194 -0.24487 0.72812 -0.39961 .....\n",
    "        word = line[0] # ex) the\n",
    "        vec = np.array([float(x) for x in line[1:]]) # ex) -0.038194 -0.24487 0.72812 .....\n",
    "\n",
    "        word_to_index[word] = len(word_to_index) # 딕셔너리에 할당, ex) {'the': 0}\n",
    "        word_vectors.append(vec) # ex) [-0.038194 -0.24487 0.72812 ..... ]\n",
    "                    \n",
    "#         print(word_to_index)\n",
    "#         print(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f69bf6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.90715   0.064843  1.1566   -0.3465    0.14296  -0.27646  -0.017816\n",
      " -0.12121  -1.3285   -0.09812   0.87256  -0.16852   0.047454 -0.33343\n",
      " -0.27088   0.28921   0.33383  -0.05773   0.18968   0.53989   0.68687\n",
      "  0.56052   0.52556   0.53909   0.34341  -0.61371  -0.26548  -0.42634\n",
      "  0.49248  -0.54542  -0.29126  -0.092725  0.046827  0.84663  -0.48321\n",
      "  0.17015  -0.6567    0.23944  -0.55302   0.091274  0.25736  -0.27976\n",
      " -0.013608  0.065933  0.3542   -0.75957   0.51267  -1.1732    0.15933\n",
      " -1.6229    0.56459  -0.26746   0.45518   1.0818    0.47754  -2.7297\n",
      " -0.072848  0.25136   1.4372    1.0144   -0.36866  -0.44527  -0.35746\n",
      "  0.081572 -0.043148 -0.17133   0.57359   0.48109   0.64189  -0.35552\n",
      " -0.03176   0.8223   -0.44467   0.47329   0.028232  0.057876 -0.42354\n",
      " -0.71167  -0.60101  -0.12724   0.70369   0.5142   -0.058689  0.08078\n",
      " -0.7596   -0.31802   0.21919  -0.41005  -0.092171  0.80333  -0.37917\n",
      "  0.77842  -0.3825   -0.36406  -0.38988  -0.29918  -0.75252  -0.11745\n",
      "  0.61162  -0.30405 ]\n"
     ]
    }
   ],
   "source": [
    "# 어떤 단어에 대한 vector 출력하는 함수\n",
    "def get_embedding(word):\n",
    "    return word_vectors[word_to_index[word]]\n",
    "\n",
    "print(get_embedding('record'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9ec4321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 0, ',': 1, '.': 2, 'of': 3, 'to': 4}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(word_to_index.items())[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af1bd48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 임베딩을 Annoy 인덱스에 추가하고 해당 인덱스를 구축하여 검색을 위한 준비\n",
    "\n",
    "index = AnnoyIndex(len(word_vectors[0]),metric='euclidean')\n",
    "\n",
    "for _, i in word_to_index.items():\n",
    "    index.add_item(i, word_vectors[i])\n",
    "index.build(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ffa84f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'the', 1: ',', 2: '.', 3: 'of', 4: 'to'}\n"
     ]
    }
   ],
   "source": [
    "index_to_word = {v:k for k,v in word_to_index.items()}\n",
    "\n",
    "print(dict(list(index_to_word.items())[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9e1b798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['js03', 'bulletinyyy', '65stk', 'mo95', 'bdb94']\n"
     ]
    }
   ],
   "source": [
    "example_vector = [0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1]\n",
    "\n",
    "def get_closet_to_vector(vector, n=1):\n",
    "    \"\"\"벡터가 주어지면 최근접 이웃을 n개 반환한다. \"\"\"\n",
    "    nn_indices = index.get_nns_by_vector(vector, n)\n",
    "    return [index_to_word[neighbor] for neighbor in nn_indices]\n",
    "\n",
    "print(get_closet_to_vector(example_vector,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e4a0c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_print_analogy(word1,word2,word3):\n",
    "    \n",
    "    vec1 = get_embedding(word1)\n",
    "    vec2 = get_embedding(word2)\n",
    "    vec3 = get_embedding(word3)\n",
    "    \n",
    "#     단순 가정 : 이 유추는 공간적 관계입니다. \n",
    "    spatial_relationship = vec2 - vec1\n",
    "    vec4 = vec3 + spatial_relationship\n",
    "    \n",
    "    closest_words = get_closet_to_vector(vec4,n=4)\n",
    "    existing_words = ([word1,word2,word3])\n",
    "    closest_words = [ word for word in closest_words \n",
    "                             if word not in existing_words]\n",
    "    \n",
    "    if len(closest_words)==0:\n",
    "        print(\"계산된 벡터와 가장 가까운 이웃을 찾을 수 없습니다.\")\n",
    "        return \n",
    "    \n",
    "    for word4 in closest_words:\n",
    "        print(\"{} : {} :: {} : {}\".format(word1,word2,word3,word4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b343cd",
   "metadata": {},
   "source": [
    "### word4를 추론하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c864f131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : he :: woman : she\n",
      "man : he :: woman : never\n"
     ]
    }
   ],
   "source": [
    "# 관계 1 : 성별 명사와 대명사의 관계\n",
    "compute_and_print_analogy('man', 'he', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0849621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fly : plane :: sail : ship\n",
      "fly : plane :: sail : vessel\n"
     ]
    }
   ],
   "source": [
    "# 관계 2 : 동사-명사 관계 \n",
    "compute_and_print_analogy('fly', 'plane', 'sail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae583fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat : kitten :: dog : puppy\n",
      "cat : kitten :: dog : puppies\n",
      "cat : kitten :: dog : panting\n"
     ]
    }
   ],
   "source": [
    "# 관계 3 : 명사-명사 관계 \n",
    "compute_and_print_analogy('cat', 'kitten', 'dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a0269f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue : color :: dog : cat\n",
      "blue : color :: dog : animal\n",
      "blue : color :: dog : breed\n"
     ]
    }
   ],
   "source": [
    "# 관계 4 : 상위어 관계 \n",
    "compute_and_print_analogy('blue', 'color', 'dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ef7463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toe : foot :: finger : hand\n",
      "toe : foot :: finger : kept\n"
     ]
    }
   ],
   "source": [
    "# 관계 5 : 부분-전체 관계 \n",
    "compute_and_print_analogy('toe', 'foot', 'finger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba100d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk : communicate :: read : typed\n",
      "talk : communicate :: read : correctly\n"
     ]
    }
   ],
   "source": [
    "# 관계 6 : 방식 차이\n",
    "compute_and_print_analogy('talk', 'communicate', 'read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea061ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue : democrat :: red : republican\n",
      "blue : democrat :: red : congressman\n",
      "blue : democrat :: red : senator\n"
     ]
    }
   ],
   "source": [
    "# 관계 7 : 전체 의미 표현(관습 /인물)\n",
    "compute_and_print_analogy('blue', 'democrat', 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed0caaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast : fastest :: young : youngest\n",
      "fast : fastest :: young : sixth\n",
      "fast : fastest :: young : fifth\n",
      "fast : fastest :: young : fourth\n"
     ]
    }
   ],
   "source": [
    "# 관계 8 : 비교급\n",
    "compute_and_print_analogy('fast', 'fastest', 'young')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91717cc",
   "metadata": {},
   "source": [
    "### 잘못된 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c694c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : king :: woman : queen\n",
      "man : king :: woman : monarch\n",
      "man : king :: woman : throne\n"
     ]
    }
   ],
   "source": [
    "compute_and_print_analogy('man', 'king', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44915938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : doctor :: woman : nurse\n",
      "man : doctor :: woman : physician\n"
     ]
    }
   ],
   "source": [
    "compute_and_print_analogy('man', 'doctor', 'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe6618",
   "metadata": {},
   "source": [
    "# 예제: CBOW 임베딩 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ad1892f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>, or the</td>\n",
       "      <td>frankenstein</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frankenstein or the modern</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frankenstein , the modern prometheus</td>\n",
       "      <td>or</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frankenstein , or modern prometheus by</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>, or the prometheus by mary</td>\n",
       "      <td>modern</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90693</th>\n",
       "      <td>newsletter to hear new ebooks .</td>\n",
       "      <td>about</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90694</th>\n",
       "      <td>to hear about ebooks .</td>\n",
       "      <td>new</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90695</th>\n",
       "      <td>hear about new .</td>\n",
       "      <td>ebooks</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90696</th>\n",
       "      <td>about new ebooks</td>\n",
       "      <td>.</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90697</th>\n",
       "      <td>new ebooks .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      context        target  split\n",
       "0                                    , or the  frankenstein  train\n",
       "1                  frankenstein or the modern             ,  train\n",
       "2        frankenstein , the modern prometheus            or  train\n",
       "3      frankenstein , or modern prometheus by           the  train\n",
       "4                 , or the prometheus by mary        modern  train\n",
       "...                                       ...           ...    ...\n",
       "90693         newsletter to hear new ebooks .         about   test\n",
       "90694                 to hear about ebooks .            new   test\n",
       "90695                       hear about new .         ebooks   test\n",
       "90696                       about new ebooks              .   test\n",
       "90697                            new ebooks .           NaN   test\n",
       "\n",
       "[90698 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"frankenstein_with_splits.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa05145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 다시 train/valid/test로 나눠줌\n",
    "\n",
    "# train 데이터 \n",
    "train_df = df[df.split=='train']\n",
    "train_size = len(train_df)\n",
    "\n",
    "# valid 데이터 \n",
    "val_df = df[df.split=='val']\n",
    "val_size = len(val_df)\n",
    "\n",
    "# test 데이터 \n",
    "test_df = df[df.split=='test']\n",
    "test_size = len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ec341cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dict = {'train': (train_df, train_size),\n",
    "                             'val': (val_df, val_size),\n",
    "                             'test': (test_df, test_size)}\n",
    "# lookup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5109dd35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>, or the</td>\n",
       "      <td>frankenstein</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frankenstein or the modern</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frankenstein , the modern prometheus</td>\n",
       "      <td>or</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frankenstein , or modern prometheus by</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>, or the prometheus by mary</td>\n",
       "      <td>modern</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63484</th>\n",
       "      <td>i had feelings affection , and</td>\n",
       "      <td>of</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63485</th>\n",
       "      <td>had feelings of , and they</td>\n",
       "      <td>affection</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63486</th>\n",
       "      <td>feelings of affection and they were</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63487</th>\n",
       "      <td>of affection , they were requited</td>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63488</th>\n",
       "      <td>affection , and were requited by</td>\n",
       "      <td>they</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63489 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      context        target  split\n",
       "0                                    , or the  frankenstein  train\n",
       "1                  frankenstein or the modern             ,  train\n",
       "2        frankenstein , the modern prometheus            or  train\n",
       "3      frankenstein , or modern prometheus by           the  train\n",
       "4                 , or the prometheus by mary        modern  train\n",
       "...                                       ...           ...    ...\n",
       "63484          i had feelings affection , and            of  train\n",
       "63485              had feelings of , and they     affection  train\n",
       "63486     feelings of affection and they were             ,  train\n",
       "63487       of affection , they were requited           and  train\n",
       "63488        affection , and were requited by          they  train\n",
       "\n",
       "[63489 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d30142",
   "metadata": {},
   "source": [
    "### 2. Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b833a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Counter()를 통해 어떤 단어가 얼만큼의 횟수로 들어있는지를 알 수 있다.\n",
    "word_counts = Counter()\n",
    "for name_text in df.context:\n",
    "    for word in name_text.split(\" \"):\n",
    "        # word가 .(구두점,punctuation)이 아닐 경우 word에 추가\n",
    "        if word not in string.punctuation:\n",
    "            word_counts[word] += 1\n",
    "\n",
    "# word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fae986f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <MASK> 토큰을 사용할 것이기 때문에 추가해준다. \n",
    "# add_unk=True를 하면 '<UNK>': 0 토큰을 추가해줌 !\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, mask_token=\"<MASK>\",add_unk=True):\n",
    "        self.token_to_idx = {}\n",
    "        self.idx_to_token = {}\n",
    "        \n",
    "        self.mask_index = self.add_token(mask_token)\n",
    "#         \"UNK\" 토큰이 추가되지 않는 경우에는 -1로 설정,\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "#         \"UNK\" 토큰이 추가될 경우에는 UNK에 해당하는 인덱스로 설정,\n",
    "            self.unk_index = self.add_token('<UNK>') \n",
    "\n",
    "    def add_token(self, token):\n",
    "        \n",
    "#       만약 해당 토큰이 있으면 토큰 idx만 return\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "            \n",
    "#       만약 해당 토큰이 없으면 새로운 토큰 만들어줌\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9daef709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW Vocabulary 객체 생성\n",
    "\n",
    "cbow_vocab = Vocabulary()\n",
    "for index, row in df.iterrows():\n",
    "    for token in row.context.split(' '):\n",
    "        cbow_vocab.add_token(token)\n",
    "    cbow_vocab.add_token(row.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "714ba3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<MASK>': 0, '<UNK>': 1, ',': 2, 'or': 3, 'the': 4}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(cbow_vocab.token_to_idx.items())[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59bd61b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<MASK>', 1: '<UNK>', 2: ',', 3: 'or', 4: 'the'}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(cbow_vocab.idx_to_token.items())[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ee407f",
   "metadata": {},
   "source": [
    "## 3. Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "166fd3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 토큰에 대응하는 인덱스 반환\n",
    "\n",
    "def lookup_token(vocabulary_class,token):\n",
    "\n",
    "# UNK 토큰이 있을 경우\n",
    "    if vocabulary_class.unk_index >= 0:\n",
    "#           토큰을 찾아보고 없으면 unk_index 반환, 있으면 해당 토큰의 idx를 반환\n",
    "        return vocabulary_class.token_to_idx.get(token, vocabulary_class.unk_index)\n",
    "    else:\n",
    "        return vocabulary_class.token_to_idx[token]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db2d52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 인덱스에 대응하는 토큰 반환\n",
    "\n",
    "def lookup_index(vocabulary_class, index):\n",
    "        if index not in vocabulary_class.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return vocabulary_class.idx_to_token[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6853d180",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1764d004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[215   1   1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def vectorize(context, vector_length=-1):\n",
    "    \n",
    "    indices = [lookup_token(cbow_vocab,token) for token in context.split(' ')]\n",
    "    if vector_length < 0:\n",
    "        vector_length = len(indices)\n",
    "    \n",
    "    \n",
    "#   인덱스와 mask로 이루어진 out_vector를 만든다.\n",
    "    out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "    out_vector[:len(indices)] = indices\n",
    "#     문장 길이가 작아서 padding 진행하면 해당 부분 mask 처리\n",
    "    out_vector[len(indices):] = cbow_vocab.mask_index\n",
    "    \n",
    "    \n",
    "    return out_vector\n",
    "\n",
    "# vector_length 보다 문장의 토큰의 개수가 작으면 MASK 처리된다. \n",
    "print(vectorize(\"all dafs dfkdl\",vector_length=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff427e2",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d828249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_seq_length를 구해야 한다. \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, cbow_df):\n",
    "        \n",
    "        self.cbow_df = cbow_df\n",
    "        \n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self.max_seq_length = max(map(measure_len, cbow_df.context))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cbow_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        row = self.cbow_df.iloc[index]\n",
    "\n",
    "        context_vector = vectorize(row.context, self.max_seq_length)\n",
    "        target_index = lookup_token(cbow_vocab,row.target)\n",
    "\n",
    "        return {'x_data': context_vector,\n",
    "                'y_target': target_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70ee0b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CBOWDataset at 0x7fb8f18f5940>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋을 인스턴스화 해주어야 로더에 넣어줄 수 있다. \n",
    "\n",
    "train_dataset = CBOWDataset(train_df)\n",
    "train_dataset\n",
    "\n",
    "valid_dataset = CBOWDataset(val_df)\n",
    "valid_dataset\n",
    "\n",
    "test_dataset = CBOWDataset(test_df)\n",
    "test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "abd30122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더 설정\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# drop_last=True -> 배치 사이즈보다 over하면 drop\n",
    "\n",
    "Traindataloader = DataLoader(dataset=train_dataset, batch_size=512,\n",
    "                            shuffle=True, drop_last=True)\n",
    "\n",
    "Validdataloader = DataLoader(dataset=valid_dataset, batch_size=512,\n",
    "                            shuffle=False, drop_last=True)\n",
    "\n",
    "Testdataloader = DataLoader(dataset=test_dataset, batch_size=512,\n",
    "                            shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ba7f661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63489 124\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset),len(Traindataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ed795cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'x_data': tensor([[ 356, 5006,    2,  422, 1921,   49],\n",
      "        [3584,   53, 2545,    0,    0,    0],\n",
      "        [  37,  888,  859,   63,  125, 5637],\n",
      "        ...,\n",
      "        [ 962,   27,  226,   90,   85, 1934],\n",
      "        [  92,  211, 2049,   49,   37, 5644],\n",
      "        [3872,    8,   34,   33, 4306, 1354]]), 'y_target': tensor([   4,   44,   48,    2,   33,  255, 2130,  496, 1860,   44,    2, 1248,\n",
      "         213,  249,  830,  294,  592,   60, 1404,    2,  471,   44,    2,   60,\n",
      "         721,   41,   33, 1191,   72,   26,   49,   48,   44,   15,   85, 2038,\n",
      "         136,  212,   33,   36,   69,   19, 1584, 3710,   19,   50,   19,   33,\n",
      "         237,   60, 4868,   15, 1179, 2097, 3227,  137,  568, 1648, 3662, 1443,\n",
      "         463,   96,   44,    2,  682,   33, 1383,   33,    2,    4,  494, 2632,\n",
      "         136,    4,   50, 3998, 2532,   15,  542,    4,  766,   60,   48,   33,\n",
      "         420,  273,   44,   18,   60,   27,    8, 1897, 3507,   33,   33,    2,\n",
      "          33,  174,   48,  345,   15, 3041, 1132,  766,  104,   51,  365,   27,\n",
      "          48, 1854,   60,  358,   49,  788,  598,  160,   47, 2114,  334, 3908,\n",
      "        5371, 4360, 1802,   49,   82,  529,  173, 3055,   15,   48,    2,    2,\n",
      "          33,  780,  620,  983,  932,  151,   15,   36,  552, 3535,    8,   44,\n",
      "          90, 1466,    2,   60,  334,   15,   33,  358,  399,  326,   68,  239,\n",
      "           2,    2,    3,  412,  365,   82, 1710,  551,   44,   82,   19, 2057,\n",
      "           2,    4,  120,  376,  181,   50, 2278, 1443, 1192, 2733,  792,   90,\n",
      "           2, 1078,   15,   48, 2853, 3329, 1894,   49,   50,    4,    4,   90,\n",
      "          48,   36,   34,   36,    2,  577,  226,   23,    4,  497, 3485,   48,\n",
      "          49,   33,  908, 3239,    2,   48,   49, 1243,   60,  225,  797, 3174,\n",
      "        3754,  620, 3309,  319, 1533,   15, 4997,   65,    2,  768,  578,  605,\n",
      "          14,   48,   44, 3779,   19,    2,   15,   48,  756,    8,  239, 1762,\n",
      "        2426,   27,    4,  173,   49, 1072,   39,  598,   49,  474,   77, 4782,\n",
      "           4,  209,   48,   15,   33,   48,   33, 1033,  204,   49,  334,    2,\n",
      "          48,   60,   33, 5543, 2632,    4, 2709,  226,  230,    4,  992, 1369,\n",
      "         496,  909,  715,   50,   28,   49,   60,   33,    4, 5858,   50,    2,\n",
      "           2, 2107,  817,   33,    2,    2,   53,  319,   15, 2321,  104, 1192,\n",
      "         915, 1141,  239, 1564,  356,   15,   15, 3406,   15, 3230, 5831,   48,\n",
      "         396,   49,   49,  175,    4,  136,    4, 1725,    2,  583,  264, 3229,\n",
      "         278, 1154,   72,    4,   15,  792,  563, 1810, 4001,  151,   15,  334,\n",
      "          72,   50,  788,   15,   72,    2, 4958,    2,   33, 5478, 1307,   72,\n",
      "          33, 5081,    4, 1376, 4475,   33,   60,  830,   33,   48,   44,   27,\n",
      "          19,  178, 2738,   48, 3856,    2, 1775,   44,  395,  275, 2554,  173,\n",
      "          60,    2,   85, 3918,  319, 4466,    2,   33,    4,   48,    2, 1734,\n",
      "         766,   19,   36, 2391,    4,  255,   49,   33,  255, 4006,  173, 1884,\n",
      "          27, 1668,   44,  319, 3361,  774,  925, 1080,   49,    2,   15,   60,\n",
      "         389,  849,  603,   15, 5792, 1463, 2222, 4189,   33,   15, 1348,    4,\n",
      "         289,    2,  549,  605,   24, 4298,   96,   44,   48,  573,    2,   49,\n",
      "           2,  592,  179,   27,   48, 3781,   49,  226,    4,    4, 1973,   33,\n",
      "        2501,   37,   19, 1384,  394,   19,   49,  334,   56,  417,   60,   36,\n",
      "         127,   72,   19,    2,  529,   72,   44, 5123,   40,   44,  360,   49,\n",
      "          49,   48, 5939,   44, 1045,   19,   60, 5557,  226, 2627,   19,   49,\n",
      "         173,    2,  732,   48, 2555, 5169,  115, 2514,   44,  301,  337, 1743,\n",
      "         484,  321, 2024,   82,    2,   23, 1445, 2844,  492,   44,   36, 1082,\n",
      "        3616,   19,   27,   48, 2008, 1859,    2, 1126])}\n"
     ]
    }
   ],
   "source": [
    "for batch_index, batch_dict in enumerate(Traindataloader):\n",
    "    print(batch_index)\n",
    "    print(batch_dict)\n",
    "    \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0790db50",
   "metadata": {},
   "source": [
    "### 모델정의 NameClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5558da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_size를 지정해줘야 한다.\n",
    "# 1. Embedding layer를 이용해서 문맥의 단어를 나타내는 인덱스를 각 단어에 대한 벡터로 만든다.\n",
    "# 2. 전반적인 문맥을 감지하도록 벡터를 결합한다.(sum)\n",
    " \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CBOWClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    매개변수:\n",
    "        vocabulary_size (int): 어휘 사전 크기, 임베딩 개수와 예측 벡터 크기를 결정합니다\n",
    "        embedding_size (int): 임베딩 크기\n",
    "        padding_idx (int): 기본값 0; 임베딩은 이 인덱스를 사용하지 않습니다\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocabulary_size, embedding_size, padding_idx=0):\n",
    "        \n",
    "#       torch.nn.Module의 초기화 메서드를 실행하여 해당 클래스의 기능을 상속받음\n",
    "        super(CBOWClassifier, self).__init__()\n",
    "        # Embedding layer를 이용해서 문맥의 단어를 나타내는 인덱스를 각 단어에 대한 벡터로 만듦\n",
    "        \n",
    "        self.embedding =  nn.Embedding(num_embeddings=vocabulary_size, \n",
    "                                       embedding_dim=embedding_size,\n",
    "                                       padding_idx=padding_idx)\n",
    "#         embedding에서 나오면 embedding_size * vocab_size 차원이 된다. \n",
    "        self.fc1 = nn.Linear(in_features=embedding_size,\n",
    "                             out_features=vocabulary_size)\n",
    "    \n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        # 전반적인 문맥을 감지하도록 벡터를 결합함(sum)\n",
    "        x_embedded_sum = F.dropout(self.embedding(x_in).sum(dim=1), 0.3)\n",
    "        y_out = self.fc1(x_embedded_sum)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "            \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "110c5bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOWClassifier(\n",
       "  (embedding): Embedding(7270, 50, padding_idx=0)\n",
       "  (fc1): Linear(in_features=50, out_features=7270, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_size를 지정해줘야 한다. \n",
    "embedding_size=50 \n",
    "\n",
    "classifier = CBOWClassifier(vocabulary_size=len(cbow_vocab.token_to_idx), \n",
    "                            embedding_size=embedding_size)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ad82b",
   "metadata": {},
   "source": [
    "### 옵티마이저, loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "13216f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5eaa3504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 옵티마이저\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = lr)\n",
    "optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b32b926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31608c79",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75e5f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "#      예측값과 타겟값을 비교하여 일치하는 개수를 계산\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ef89fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train state 초기화 \n",
    "def make_train_state():\n",
    "    return {\n",
    "        'stop_early':False,\n",
    "        'early_stopping_step':0,\n",
    "        'early_stopping_best_val':1e8,\n",
    "        'early_stopping_criteria' : 10,\n",
    "        'epoch_index' : 0,\n",
    "        'train_loss': [], \n",
    "        'train_acc' :[], \n",
    "        'val_loss' : [],\n",
    "        'val_acc' : [], \n",
    "        'test_loss' : [],\n",
    "        'test_acc' : [],\n",
    "         \n",
    "#       모델 저장파일\n",
    "        'model_filename' : 'model.pth'\n",
    "    } \n",
    "\n",
    "\n",
    "# Train update \n",
    "def update_train_state(model, train_state):\n",
    "    \n",
    "#   학습시작하면 초기에 모델 저장하기 \n",
    "    \n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(),train_state['model_filename'])\n",
    "        \n",
    "#   모델 성능이 향상되면 모델 저장(valid loss가 더 낮아지면)\n",
    "    elif train_state['epoch_index'] >=1 :\n",
    "        loss_t = train_state['val_loss'][-1]\n",
    "#        loss가 나빠지면 early stop step 업데이트\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            train_state['early_stopping_step']+=1\n",
    "            \n",
    "#        loss가 좋아지면   \n",
    "        else:\n",
    "#            early stop step 0으로 다시 초기화        \n",
    "            train_state['early_stopping_step']=0\n",
    "    \n",
    "#           최저 loss이면 모델 저장 \n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "                torch.save(model.state_dict(),train_state['model_filename'])\n",
    "\n",
    "#       기준점 넘으면 early stop \n",
    "        if train_state['early_stopping_step'] >= train_state['early_stopping_criteria']:\n",
    "            train_state['stop_early'] = True\n",
    "        \n",
    "        return train_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66b6b449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop_early': False,\n",
       " 'early_stopping_step': 0,\n",
       " 'early_stopping_best_val': 100000000.0,\n",
       " 'early_stopping_criteria': 10,\n",
       " 'epoch_index': 0,\n",
       " 'train_loss': [],\n",
       " 'train_acc': [],\n",
       " 'val_loss': [],\n",
       " 'val_acc': [],\n",
       " 'test_loss': [],\n",
       " 'test_acc': [],\n",
       " 'model_filename': 'model.pth'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 진행 상황 함수 초기화\n",
    "train_state = make_train_state()\n",
    "train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0ef8576c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                          | 1/100 [00:12<21:21, 12.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 7.004987111458411\n",
      "val_acc 10.073617788461538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▊                                          | 2/100 [00:25<20:54, 12.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.7591050404768716\n",
      "val_acc 11.523437499999996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|█▎                                         | 3/100 [00:38<20:45, 12.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.653256489680364\n",
      "val_acc 12.372295673076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|█▋                                         | 4/100 [00:51<20:56, 13.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.572822075623732\n",
      "val_acc 12.49248798076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|██▏                                        | 5/100 [01:04<20:28, 12.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.517270711752085\n",
      "val_acc 12.725360576923075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|██▌                                        | 6/100 [01:17<20:13, 12.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.476829748887282\n",
      "val_acc 13.326322115384615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|███                                        | 7/100 [01:30<19:57, 12.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.468164664048415\n",
      "val_acc 13.529146634615385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|███▍                                       | 8/100 [01:43<19:49, 12.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.439299528415387\n",
      "val_acc 13.574218750000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███▊                                       | 9/100 [01:57<20:09, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.437481366671049\n",
      "val_acc 13.837139423076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▏                                     | 10/100 [02:10<19:41, 13.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.421062671221219\n",
      "val_acc 13.979867788461538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████▌                                     | 11/100 [02:23<19:19, 13.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.44003750727727\n",
      "val_acc 14.107572115384615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████                                     | 12/100 [02:35<19:03, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.438934858028706\n",
      "val_acc 14.197716346153848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█████▍                                    | 13/100 [02:49<18:58, 13.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.446268540162307\n",
      "val_acc 14.085036057692308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█████▉                                    | 14/100 [03:02<18:38, 13.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.4550894407125625\n",
      "val_acc 14.27283653846154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|██████▎                                   | 15/100 [03:14<18:18, 12.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.459408980149491\n",
      "val_acc 14.25030048076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|██████▋                                   | 16/100 [03:27<18:01, 12.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.47457896746122\n",
      "val_acc 14.212740384615383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████▏                                  | 17/100 [03:40<17:58, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.474959520193247\n",
      "val_acc 14.400540865384617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|███████▌                                  | 18/100 [03:54<17:55, 13.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.507918027731089\n",
      "val_acc 14.445612980769232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████▉                                  | 19/100 [04:07<17:38, 13.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.481477975845337\n",
      "val_acc 14.53575721153846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████▉                                  | 19/100 [04:19<18:28, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.511375830723689\n",
      "val_acc 14.242788461538462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "# 에포크만큼\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "\n",
    "#     print('epoch',epoch)\n",
    "#     print(train_state['epoch_index']) \n",
    "    train_state['epoch_index'] +=1 \n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "\n",
    "#     모델을 학습 모드로 설정 -> 드롭아웃 및 배치 정규화와 같은 학습 중에만 적용되는 기법들이 활성화\n",
    "#     모델을 평가 모드로 전환하려면 classifier.eval()을 사용\n",
    "    classifier.train()\n",
    "# 배치 만큼\n",
    "    for batch_idx, batch_data in enumerate(Traindataloader):\n",
    "\n",
    "        \n",
    "\n",
    "#       1. 옵티마이저 그레디언트 0으로 초기화\n",
    "        optimizer.zero_grad()\n",
    "#       2. 모델에 데이터 넣어서 출력받기\n",
    "        y_pred = classifier(x_in=batch_data['x_data'])\n",
    "#       3. loss 계산하기\n",
    "        loss =  loss_func(y_pred, batch_data['y_target'])\n",
    "    \n",
    "#       tensor(0.3190) -> 0.3190, item()으로 스칼라 값만 추출\n",
    "        loss_t = loss.item()\n",
    "\n",
    "#       배치에서의 평균 loss 구하기\n",
    "        running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "\n",
    "#       4. gradient 계산하기\n",
    "        loss.backward()\n",
    "\n",
    "#       5. 옵티마이저 가중치 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "#       Accuracy 계산\n",
    "        acc_t = compute_accuracy(y_pred, batch_data['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "\n",
    "\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "\n",
    "\n",
    "#   valid에 대한 계산\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "    classifier.eval() # 모델 파라미터를 수정하지 못 하게 비활성화\n",
    "\n",
    "    for batch_idx, batch_data in enumerate(Validdataloader):\n",
    "\n",
    "#       1. 모델의 출력값(y_pred)계산\n",
    "        y_pred = classifier(x_in=batch_data['x_data'])\n",
    "\n",
    "#       2. loss 계산\n",
    "        loss = loss_func(y_pred,batch_data['y_target'])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "\n",
    "#       3. Accuracy 계산\n",
    "        acc_t = compute_accuracy(y_pred,batch_data['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "    \n",
    "    print(\"val_loss\",running_loss)\n",
    "    print(\"val_acc\",running_acc)\n",
    "\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "    \n",
    "\n",
    "#   전체 loss, acc 저장\n",
    "    train_state = update_train_state(model=classifier,\n",
    "                                     train_state=train_state)\n",
    "#   early stop해라고 했으면 학습 멈추기    \n",
    "    if train_state['stop_early']:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037c61f7",
   "metadata": {},
   "source": [
    "### Test 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "331de996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 좋은 모델을 사용해 테스트 세트의 손실과 정확도를 계산합니다\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "\n",
    "# 가중치 업데이트 하지 못 하게\n",
    "classifier.eval()\n",
    "\n",
    "for batch_idx, batch_data in enumerate(Testdataloader):\n",
    "    \n",
    "    y_pred = classifier(x_in=batch_data['x_data'])\n",
    "    loss = loss_func(y_pred,batch_data['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "    \n",
    "    acc_t = compute_accuracy(y_pred, batch_data['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "74df48a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 7.323\n",
      "테스트 정확도: 13.48\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 손실: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {:.2f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "342a2c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop_early': True,\n",
       " 'early_stopping_step': 10,\n",
       " 'early_stopping_best_val': 6.421062671221219,\n",
       " 'early_stopping_criteria': 10,\n",
       " 'epoch_index': 22,\n",
       " 'train_loss': [8.769133717783035,\n",
       "  7.03628062817358,\n",
       "  6.522220976891057,\n",
       "  6.20068036740826,\n",
       "  5.971037410920665,\n",
       "  5.795840478712515,\n",
       "  5.647560573393301,\n",
       "  5.522859184972702,\n",
       "  5.4152467443097025,\n",
       "  5.315567604957089,\n",
       "  5.231619077344095,\n",
       "  5.151370709942233,\n",
       "  5.076750001599714,\n",
       "  5.007491900074864,\n",
       "  4.949088873401769,\n",
       "  4.883807874494986,\n",
       "  4.835539360200204,\n",
       "  4.781407860017592,\n",
       "  4.724345587914989,\n",
       "  4.685608140883904,\n",
       "  4.6482952602448],\n",
       " 'train_acc': [2.3988785282258074,\n",
       "  9.308845766129037,\n",
       "  11.151713709677422,\n",
       "  12.34721522177419,\n",
       "  12.9914314516129,\n",
       "  13.545866935483872,\n",
       "  13.788432459677418,\n",
       "  14.175907258064516,\n",
       "  14.650012600806448,\n",
       "  14.903603830645155,\n",
       "  15.251701108870966,\n",
       "  15.500567036290326,\n",
       "  15.905367943548383,\n",
       "  16.204637096774185,\n",
       "  16.53855846774193,\n",
       "  17.097719254032256,\n",
       "  17.357610887096783,\n",
       "  17.80021421370969,\n",
       "  18.261718749999993,\n",
       "  18.502709173387093,\n",
       "  18.902784778225804],\n",
       " 'val_loss': [7.753033362902128,\n",
       "  7.004987111458411,\n",
       "  6.7591050404768716,\n",
       "  6.653256489680364,\n",
       "  6.572822075623732,\n",
       "  6.517270711752085,\n",
       "  6.476829748887282,\n",
       "  6.468164664048415,\n",
       "  6.439299528415387,\n",
       "  6.437481366671049,\n",
       "  6.421062671221219,\n",
       "  6.44003750727727,\n",
       "  6.438934858028706,\n",
       "  6.446268540162307,\n",
       "  6.4550894407125625,\n",
       "  6.459408980149491,\n",
       "  6.47457896746122,\n",
       "  6.474959520193247,\n",
       "  6.507918027731089,\n",
       "  6.481477975845337,\n",
       "  6.511375830723689],\n",
       " 'val_acc': [6.820913461538463,\n",
       "  10.073617788461538,\n",
       "  11.523437499999996,\n",
       "  12.372295673076923,\n",
       "  12.49248798076923,\n",
       "  12.725360576923075,\n",
       "  13.326322115384615,\n",
       "  13.529146634615385,\n",
       "  13.574218750000002,\n",
       "  13.837139423076923,\n",
       "  13.979867788461538,\n",
       "  14.107572115384615,\n",
       "  14.197716346153848,\n",
       "  14.085036057692308,\n",
       "  14.27283653846154,\n",
       "  14.25030048076923,\n",
       "  14.212740384615383,\n",
       "  14.400540865384617,\n",
       "  14.445612980769232,\n",
       "  14.53575721153846,\n",
       "  14.242788461538462],\n",
       " 'test_loss': 7.32296763933622,\n",
       " 'test_acc': 13.476562499999998,\n",
       " 'model_filename': 'model.pth'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9403cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(results):\n",
    "    \"\"\"\n",
    "    임베딩 결과를 출력합니다\n",
    "    \"\"\"\n",
    "    for item in results:\n",
    "        print (\"...[%.2f] - %s\"%(item[1], item[0]))\n",
    "\n",
    "def get_closest(target_word, word_to_idx, embeddings, n=5):\n",
    "    \"\"\"\n",
    "    n개의 최근접 단어를 찾습니다.\n",
    "    \"\"\"\n",
    "\n",
    "    # 다른 모든 단어까지 거리를 계산합니다\n",
    "    word_embedding = embeddings[word_to_idx[target_word.lower()]]\n",
    "    distances = []\n",
    "    for word, index in word_to_idx.items():\n",
    "        if word == \"<MASK>\" or word == target_word:\n",
    "            continue\n",
    "        distances.append((word, torch.dist(word_embedding, embeddings[index])))\n",
    "    \n",
    "    results = sorted(distances, key=lambda x: x[1])[1:n+2]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "00dcde86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어를 입력해 주세요: monster\n",
      "...[6.43] - recourse\n",
      "...[6.44] - disclose\n",
      "...[6.50] - perceptibly\n",
      "...[6.54] - affectionate\n",
      "...[6.60] - began\n",
      "...[6.60] - survive\n"
     ]
    }
   ],
   "source": [
    "word = input('단어를 입력해 주세요: ')\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = cbow_vocab.token_to_idx\n",
    "pretty_print(get_closest(word, word_to_idx, embeddings, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d908444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======frankenstein=======\n",
      "...[6.68] - recurrence\n",
      "...[6.84] - robert\n",
      "...[6.88] - consoled\n",
      "...[7.02] - tolerably\n",
      "...[7.02] - catching\n",
      "...[7.05] - ideas\n",
      "=======monster=======\n",
      "...[6.43] - recourse\n",
      "...[6.44] - disclose\n",
      "...[6.50] - perceptibly\n",
      "...[6.54] - affectionate\n",
      "...[6.60] - began\n",
      "...[6.60] - survive\n",
      "=======science=======\n",
      "...[6.34] - orkneys\n",
      "...[6.38] - somewhat\n",
      "...[6.57] - sicken\n",
      "...[6.64] - spent\n",
      "...[6.70] - theory\n",
      "...[6.80] - entertained\n",
      "=======sickness=======\n",
      "...[6.56] - else\n",
      "...[6.61] - tolerably\n",
      "...[6.65] - sadness\n",
      "...[6.67] - stolen\n",
      "...[6.72] - collections\n",
      "...[6.77] - asleep\n",
      "=======lonely=======\n",
      "...[6.49] - early\n",
      "...[6.55] - muttered\n",
      "...[6.58] - large\n",
      "...[6.60] - d\n",
      "...[6.63] - cheerfulness\n",
      "...[6.64] - brink\n",
      "=======happy=======\n",
      "...[6.28] - surrounded\n",
      "...[6.49] - sledge\n",
      "...[6.50] - imagine\n",
      "...[6.59] - strangled\n",
      "...[6.62] - sharing\n",
      "...[6.63] - appointment\n"
     ]
    }
   ],
   "source": [
    "target_words = ['frankenstein', 'monster', 'science', 'sickness', 'lonely', 'happy']\n",
    "\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = cbow_vocab.token_to_idx\n",
    "\n",
    "for target_word in target_words: \n",
    "    print(f\"======={target_word}=======\")\n",
    "    if target_word not in word_to_idx:\n",
    "        print(\"Not in vocabulary\")\n",
    "        continue\n",
    "    pretty_print(get_closest(target_word, word_to_idx, embeddings, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d2c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
