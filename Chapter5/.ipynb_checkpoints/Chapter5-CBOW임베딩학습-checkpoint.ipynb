{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9abe6618",
   "metadata": {},
   "source": [
    "# 예제: CBOW 임베딩 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad1892f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>, or the</td>\n",
       "      <td>frankenstein</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frankenstein or the modern</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frankenstein , the modern prometheus</td>\n",
       "      <td>or</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frankenstein , or modern prometheus by</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>, or the prometheus by mary</td>\n",
       "      <td>modern</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90693</th>\n",
       "      <td>newsletter to hear new ebooks .</td>\n",
       "      <td>about</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90694</th>\n",
       "      <td>to hear about ebooks .</td>\n",
       "      <td>new</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90695</th>\n",
       "      <td>hear about new .</td>\n",
       "      <td>ebooks</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90696</th>\n",
       "      <td>about new ebooks</td>\n",
       "      <td>.</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90697</th>\n",
       "      <td>new ebooks .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      context        target  split\n",
       "0                                    , or the  frankenstein  train\n",
       "1                  frankenstein or the modern             ,  train\n",
       "2        frankenstein , the modern prometheus            or  train\n",
       "3      frankenstein , or modern prometheus by           the  train\n",
       "4                 , or the prometheus by mary        modern  train\n",
       "...                                       ...           ...    ...\n",
       "90693         newsletter to hear new ebooks .         about   test\n",
       "90694                 to hear about ebooks .            new   test\n",
       "90695                       hear about new .         ebooks   test\n",
       "90696                       about new ebooks              .   test\n",
       "90697                            new ebooks .           NaN   test\n",
       "\n",
       "[90698 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../../data/frankenstein_with_splits.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa05145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 다시 train/valid/test로 나눠줌\n",
    "\n",
    "# train 데이터 \n",
    "train_df = df[df.split=='train']\n",
    "train_size = len(train_df)\n",
    "\n",
    "# valid 데이터 \n",
    "val_df = df[df.split=='val']\n",
    "val_size = len(val_df)\n",
    "\n",
    "# test 데이터 \n",
    "test_df = df[df.split=='test']\n",
    "test_size = len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "605f8f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  63489\n",
      "valid :  13605\n",
      "test :  13604\n"
     ]
    }
   ],
   "source": [
    "print(\"train : \",train_size)\n",
    "print(\"valid : \",val_size)\n",
    "print(\"test : \",test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ec341cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dict = {'train': (train_df, train_size),\n",
    "                             'val': (val_df, val_size),\n",
    "                             'test': (test_df, test_size)}\n",
    "# lookup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5109dd35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63479</th>\n",
       "      <td>i be alone</td>\n",
       "      <td>?</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63480</th>\n",
       "      <td>be alone ?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63481</th>\n",
       "      <td>had feelings of</td>\n",
       "      <td>i</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63482</th>\n",
       "      <td>i feelings of affection</td>\n",
       "      <td>had</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63483</th>\n",
       "      <td>i had of affection ,</td>\n",
       "      <td>feelings</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63484</th>\n",
       "      <td>i had feelings affection , and</td>\n",
       "      <td>of</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63485</th>\n",
       "      <td>had feelings of , and they</td>\n",
       "      <td>affection</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63486</th>\n",
       "      <td>feelings of affection and they were</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63487</th>\n",
       "      <td>of affection , they were requited</td>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63488</th>\n",
       "      <td>affection , and were requited by</td>\n",
       "      <td>they</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   context     target  split\n",
       "63479                          i be alone           ?  train\n",
       "63480                           be alone ?        NaN  train\n",
       "63481                      had feelings of          i  train\n",
       "63482              i feelings of affection        had  train\n",
       "63483                 i had of affection ,   feelings  train\n",
       "63484       i had feelings affection , and         of  train\n",
       "63485           had feelings of , and they  affection  train\n",
       "63486  feelings of affection and they were          ,  train\n",
       "63487    of affection , they were requited        and  train\n",
       "63488     affection , and were requited by       they  train"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d30142",
   "metadata": {},
   "source": [
    "### 2. Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b833a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# import string\n",
    "\n",
    "# # Counter()를 통해 어떤 단어가 얼만큼의 횟수로 들어있는지를 알 수 있다.\n",
    "# word_counts = Counter()\n",
    "# for name_text in df.context:\n",
    "#     for word in name_text.split(\" \"):\n",
    "#         # word가 .(구두점,punctuation)이 아닐 경우 word에 추가\n",
    "#         if word not in string.punctuation:\n",
    "#             word_counts[word] += 1\n",
    "\n",
    "# # word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fae986f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, mask_token=\"<MASK>\",add_unk=True):\n",
    "        self.token_to_idx = {}\n",
    "        self.idx_to_token = {}\n",
    "        \n",
    "        self.mask_index = self.add_token(mask_token)\n",
    "#         \"UNK\" 토큰이 추가되지 않는 경우에는 -1로 설정,\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "#         \"UNK\" 토큰이 추가될 경우에는 UNK에 해당하는 인덱스로 설정,\n",
    "            self.unk_index = self.add_token('<UNK>') \n",
    "\n",
    "    def add_token(self, token):\n",
    "        \n",
    "#       만약 해당 토큰이 있으면 토큰 idx만 return\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "            \n",
    "#       만약 해당 토큰이 없으면 새로운 토큰 만들어줌\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9daef709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW Vocabulary 객체 생성\n",
    "\n",
    "cbow_vocab = Vocabulary()\n",
    "for index, row in df.iterrows():\n",
    "    for token in row.context.split(' '):\n",
    "        cbow_vocab.add_token(token)\n",
    "    cbow_vocab.add_token(row.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "714ba3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<MASK>': 0, '<UNK>': 1, ',': 2, 'or': 3, 'the': 4}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(cbow_vocab.token_to_idx.items())[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59bd61b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<MASK>', 1: '<UNK>', 2: ',', 3: 'or', 4: 'the'}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(cbow_vocab.idx_to_token.items())[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ee407f",
   "metadata": {},
   "source": [
    "## 3. Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "166fd3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 토큰에 대응하는 인덱스 반환\n",
    "\n",
    "def lookup_token(vocabulary_class,token):\n",
    "\n",
    "# UNK 토큰이 있을 경우\n",
    "    if vocabulary_class.unk_index >= 0:\n",
    "#           토큰을 찾아보고 없으면 unk_index 반환, 있으면 해당 토큰의 idx를 반환\n",
    "        return vocabulary_class.token_to_idx.get(token, vocabulary_class.unk_index)\n",
    "    else:\n",
    "        return vocabulary_class.token_to_idx[token]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db2d52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 인덱스에 대응하는 토큰 반환\n",
    "\n",
    "def lookup_index(vocabulary_class, index):\n",
    "        if index not in vocabulary_class.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return vocabulary_class.idx_to_token[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6853d180",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1764d004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[215   1   1   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def vectorize(context, vector_length=-1):\n",
    "    \n",
    "    indices = [lookup_token(cbow_vocab,token) for token in context.split(' ')]\n",
    "    if vector_length < 0:\n",
    "        vector_length = len(indices)\n",
    "    \n",
    "    \n",
    "#   인덱스와 mask로 이루어진 out_vector를 만든다.\n",
    "    out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "    out_vector[:len(indices)] = indices\n",
    "#     문장 길이가 작아서 padding 진행하면 해당 부분 mask 처리\n",
    "    out_vector[len(indices):] = cbow_vocab.mask_index\n",
    "    \n",
    "    \n",
    "    return out_vector\n",
    "\n",
    "# vector_length 보다 문장의 토큰의 개수가 작으면 MASK 처리된다. \n",
    "print(vectorize(\"all dafs dfkdl\",vector_length=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff427e2",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d828249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_seq_length를 구해야 한다. \n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, cbow_df):\n",
    "        \n",
    "        self.cbow_df = cbow_df\n",
    "        \n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self.max_seq_length = max(map(measure_len, cbow_df.context))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cbow_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        row = self.cbow_df.iloc[index]\n",
    "\n",
    "        context_vector = vectorize(row.context, self.max_seq_length)\n",
    "        target_index = lookup_token(cbow_vocab,row.target)\n",
    "\n",
    "        return {'x_data': context_vector,\n",
    "                'y_target': target_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f100a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70ee0b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CBOWDataset at 0x7fd83901fd30>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋을 인스턴스화 해주어야 로더에 넣어줄 수 있다. \n",
    "\n",
    "train_dataset = CBOWDataset(train_df)\n",
    "train_dataset\n",
    "\n",
    "valid_dataset = CBOWDataset(val_df)\n",
    "valid_dataset\n",
    "\n",
    "test_dataset = CBOWDataset(test_df)\n",
    "test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abd30122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더 설정\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# drop_last=True -> 배치 사이즈보다 over하면 drop\n",
    "\n",
    "Traindataloader = DataLoader(dataset=train_dataset, batch_size=512,\n",
    "                            shuffle=True, drop_last=True)\n",
    "\n",
    "Validdataloader = DataLoader(dataset=valid_dataset, batch_size=512,\n",
    "                            shuffle=False, drop_last=True)\n",
    "\n",
    "Testdataloader = DataLoader(dataset=test_dataset, batch_size=512,\n",
    "                            shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ba7f661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63489 124\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset),len(Traindataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ed795cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'x_data': tensor([[ 455,  173,   26, 1637,  319, 1089],\n",
      "        [  48,  319,  632,  648, 5482,   27],\n",
      "        [  48, 1275,   23,   19,  794,   82],\n",
      "        ...,\n",
      "        [ 760,  196,  116,  983,  568,   48],\n",
      "        [2449, 4453,    2,   48,  783,   28],\n",
      "        [  72,  154,  203,  207,    8,    4]]), 'y_target': tensor([ 358,   72,  173, 1139,   19,   19, 5194, 3724, 4969,  226, 1383,   96,\n",
      "           8,    2,   44, 6068, 6065,  760,  305,  225,    2,    2,   63, 2658,\n",
      "         819,   27,  766,    4,  658,   44,  255, 3026,  534,  569, 5413,    4,\n",
      "           2,   44, 2725,  255,    2, 2339,   15,   33, 2174, 2954,   50,   19,\n",
      "          39,  642,   44, 1774, 5275,    2,  925,   50,   50,    3,   49,  715,\n",
      "         535, 1206,  689,   85,   48,   33,   44,   15, 2528,   68,   10,   15,\n",
      "           2, 1874,  280,  474,   50,   49,   48,   48, 1177,  724, 2252,   34,\n",
      "          48,  194, 5264,  715,  104,    4, 4001,   33,  218, 4572,   48,   19,\n",
      "        2222,   72,  535,  939, 3170, 1706,  139,  569,   44,  417,  932,  516,\n",
      "          49,  151,   90, 5714,    2, 2322,   15,   19,  225, 2278,   60,    2,\n",
      "          50,    4, 2400,  116,   48,   15,    8, 1082,   48, 1736, 1859,  239,\n",
      "         567,  144,  819, 4607,   33,   60,   15, 1443,    4,   48,  813,  992,\n",
      "          49,  609,  319, 2312,   49,    2,   47,    2,   87,  474,  313,   19,\n",
      "          39,   15,  688,  792,   44, 5941, 1355,    2,   68,  910,  334,  409,\n",
      "        1015,   50,   44, 2259,    4,  226,   33,  474,    4,    2,  870,  908,\n",
      "          33,    4,   72,   48,    2,  426, 2890, 3503,  554, 1898,  223,  118,\n",
      "           4, 1606, 1253,    2,   44, 3844, 2333,   36,    4, 4040, 4123, 5130,\n",
      "        1245,  968,   44,  622,  909, 1891,    2,  337,   72,   27, 4669, 2153,\n",
      "          19,  264,  337,    4,  636, 4104, 1693,  145,  475,  212, 1733,   60,\n",
      "          15,    4,  103,   49,   48,  474,  151,   88,   49, 2114,    4, 4372,\n",
      "         650,   60, 4608,  369, 3824,    2,   40,  766,   44,   33,   88,  455,\n",
      "         783,  230, 1701,   40,  749,  295,   44,  235,  766, 1164, 2881,   34,\n",
      "        2536, 1031, 3727,   27,  160, 2637,    2, 1388, 2896,   15,  622, 1715,\n",
      "        1141,  356, 1074,   28,   33,  255,  417, 3918,  230,    3,  783,  176,\n",
      "           2,   15, 3297,   27,    2,   72,   48,  602, 1698,   33, 1518,   33,\n",
      "          48,  426,    2,  559, 3917,   33,    4,   33,   82,   19,   82,   19,\n",
      "          19,    4,   48,  766,  715, 3809,    2,    2,  672, 3575,  145,    2,\n",
      "          82,   49, 5883,    2, 1840,   15,    4, 3033,   48,    2,  158,   15,\n",
      "         137,   48, 2402,  230, 2587,   60, 1093,   48,   39, 1733,   49,  337,\n",
      "          48, 1438,   50,   34,    2,    4, 1164,   49, 1828,   33, 1463, 1103,\n",
      "          85, 1369, 3912,   27,   44,  319,   15,  230, 1213,    4, 1815,   77,\n",
      "        2350,  268, 2764,    4,   33,   19,  841,  532,    4,   72,  225,  914,\n",
      "        1991,  548,   48,   19, 1774, 2257,   27,   15,    8,  173,   15,  390,\n",
      "        1989,   15, 5438,    4,  209, 2503,  144,   19, 4932,   39,   48,   53,\n",
      "        3795,  788,   48,    4,   27,  255,  148, 1877, 4651,   72,   33,   33,\n",
      "        2831,   49,  932, 1080, 2539,   39,   33,   15,  467,   44,  204, 1369,\n",
      "          36,  326,  257, 3328,   19,  835,  355,   33,  319,   82,  151,   50,\n",
      "          15, 1997,   15, 4683, 1279,  314,    2,  356,  212,   82,   44,  326,\n",
      "         160, 1108,   23, 2047,   60,   33, 4720,   49,    2,    2,    2,   48,\n",
      "          49, 3641, 3782, 3070, 3554,   37,    4,  103, 3347,  178,  257, 1279,\n",
      "        1040,   33, 1995,  395,  136,    2,  173,  760, 1144,   44,   60,   33,\n",
      "         319,    6, 1195,  395,   19,   49, 2305,   44,   15,    2,  458,   44,\n",
      "        3185,  298,   33,   68, 1770,  567,  239,  204])}\n"
     ]
    }
   ],
   "source": [
    "for batch_index, batch_dict in enumerate(Traindataloader):\n",
    "    print(batch_index)\n",
    "    print(batch_dict)\n",
    "    \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0790db50",
   "metadata": {},
   "source": [
    "### 모델정의 NameClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5558da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_size를 지정해줘야 한다.\n",
    "# 1. Embedding layer를 이용해서 문맥의 단어를 나타내는 인덱스를 각 단어에 대한 벡터로 만든다.\n",
    "# 2. 전반적인 문맥을 감지하도록 벡터를 결합한다.(sum)\n",
    " \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CBOWClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    매개변수:\n",
    "        vocabulary_size (int): 어휘 사전 크기, 임베딩 개수와 예측 벡터 크기를 결정합니다\n",
    "        embedding_size (int): 임베딩 크기\n",
    "        padding_idx (int): 기본값 0; 임베딩은 이 인덱스를 사용하지 않습니다\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocabulary_size, embedding_size, padding_idx=0):\n",
    "        \n",
    "#       torch.nn.Module의 초기화 메서드를 실행하여 해당 클래스의 기능을 상속받음\n",
    "        super(CBOWClassifier, self).__init__()\n",
    "        # Embedding layer를 이용해서 문맥의 단어를 나타내는 인덱스를 각 단어에 대한 벡터로 만듦\n",
    "        \n",
    "        self.embedding =  nn.Embedding(num_embeddings=vocabulary_size, \n",
    "                                       embedding_dim=embedding_size,\n",
    "                                       padding_idx=padding_idx)\n",
    "#         embedding에서 나오면 embedding_size * vocab_size 차원이 된다. \n",
    "        self.fc1 = nn.Linear(in_features=embedding_size,\n",
    "                             out_features=vocabulary_size)\n",
    "    \n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        # 전반적인 문맥을 감지하도록 벡터를 결합함(sum)\n",
    "        x_embedded_sum = F.dropout(self.embedding(x_in).sum(dim=1), 0.3)\n",
    "        y_out = self.fc1(x_embedded_sum)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "            \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "110c5bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOWClassifier(\n",
       "  (embedding): Embedding(7270, 50, padding_idx=0)\n",
       "  (fc1): Linear(in_features=50, out_features=7270, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_size를 지정해줘야 한다. \n",
    "embedding_size=50 \n",
    "\n",
    "classifier = CBOWClassifier(vocabulary_size=len(cbow_vocab.token_to_idx), \n",
    "                            embedding_size=embedding_size)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ad82b",
   "metadata": {},
   "source": [
    "### 옵티마이저, loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13216f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5eaa3504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 옵티마이저\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = lr)\n",
    "optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b32b926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31608c79",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75e5f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "#      예측값과 타겟값을 비교하여 일치하는 개수를 계산\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ef89fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train state 초기화 \n",
    "def make_train_state():\n",
    "    return {\n",
    "        'stop_early':False,\n",
    "        'early_stopping_step':0,\n",
    "        'early_stopping_best_val':1e8,\n",
    "        'early_stopping_criteria' : 10,\n",
    "        'epoch_index' : 0,\n",
    "        'train_loss': [], \n",
    "        'train_acc' :[], \n",
    "        'val_loss' : [],\n",
    "        'val_acc' : [], \n",
    "        'test_loss' : [],\n",
    "        'test_acc' : [],\n",
    "         \n",
    "#       모델 저장파일\n",
    "        'model_filename' : 'model.pth'\n",
    "    } \n",
    "\n",
    "\n",
    "# Train update \n",
    "def update_train_state(model, train_state):\n",
    "    \n",
    "#   학습시작하면 초기에 모델 저장하기 \n",
    "    \n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(),train_state['model_filename'])\n",
    "        \n",
    "#   모델 성능이 향상되면 모델 저장(valid loss가 더 낮아지면)\n",
    "    elif train_state['epoch_index'] >=1 :\n",
    "        loss_t = train_state['val_loss'][-1]\n",
    "#        loss가 나빠지면 early stop step 업데이트\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            train_state['early_stopping_step']+=1\n",
    "            \n",
    "#        loss가 좋아지면   \n",
    "        else:\n",
    "#            early stop step 0으로 다시 초기화        \n",
    "            train_state['early_stopping_step']=0\n",
    "    \n",
    "#           최저 loss이면 모델 저장 \n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "                torch.save(model.state_dict(),train_state['model_filename'])\n",
    "\n",
    "#       기준점 넘으면 early stop \n",
    "        if train_state['early_stopping_step'] >= train_state['early_stopping_criteria']:\n",
    "            train_state['stop_early'] = True\n",
    "        \n",
    "        return train_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66b6b449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop_early': False,\n",
       " 'early_stopping_step': 0,\n",
       " 'early_stopping_best_val': 100000000.0,\n",
       " 'early_stopping_criteria': 10,\n",
       " 'epoch_index': 0,\n",
       " 'train_loss': [],\n",
       " 'train_acc': [],\n",
       " 'val_loss': [],\n",
       " 'val_acc': [],\n",
       " 'test_loss': [],\n",
       " 'test_acc': [],\n",
       " 'model_filename': 'model.pth'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 진행 상황 함수 초기화\n",
    "train_state = make_train_state()\n",
    "train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ef8576c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                          | 1/100 [00:07<13:02,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 7.651246015842145\n",
      "val_acc 6.865985576923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▊                                          | 2/100 [00:14<11:39,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 7.0485306703127355\n",
      "val_acc 9.983473557692308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|█▎                                         | 3/100 [00:21<11:09,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.78089948800894\n",
      "val_acc 11.20042067307692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|█▋                                         | 4/100 [00:27<10:53,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.65202104128324\n",
      "val_acc 12.454927884615383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|██▏                                        | 5/100 [00:34<10:56,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.576415575467623\n",
      "val_acc 13.025841346153845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|██▌                                        | 6/100 [00:41<10:43,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.516040600263156\n",
      "val_acc 13.341346153846153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|███                                        | 7/100 [00:48<10:33,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.4738259865687455\n",
      "val_acc 13.551682692307692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|███▍                                       | 8/100 [00:54<10:19,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.448409869120671\n",
      "val_acc 13.694411057692307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███▊                                       | 9/100 [01:01<10:09,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.4282675706423245\n",
      "val_acc 13.987379807692307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▏                                     | 10/100 [01:08<10:00,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.419538589624257\n",
      "val_acc 14.115084134615383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████▌                                     | 11/100 [01:14<09:51,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.429953593474168\n",
      "val_acc 13.949819711538463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████                                     | 12/100 [01:21<09:47,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.435051624591534\n",
      "val_acc 14.43810096153846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█████▍                                    | 13/100 [01:28<09:39,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.436679418270405\n",
      "val_acc 14.25030048076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█████▉                                    | 14/100 [01:34<09:38,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.42897947017963\n",
      "val_acc 14.51322115384615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|██████▎                                   | 15/100 [01:41<09:30,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.430670866599449\n",
      "val_acc 14.415564903846153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|██████▋                                   | 16/100 [01:48<09:19,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.454676756492028\n",
      "val_acc 14.197716346153847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████▏                                  | 17/100 [01:54<09:14,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.468279893581684\n",
      "val_acc 14.235276442307692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|███████▌                                  | 18/100 [02:01<09:11,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.464673794232881\n",
      "val_acc 14.505709134615385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████▉                                  | 19/100 [02:08<09:04,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.4765980427081775\n",
      "val_acc 14.520733173076922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████▉                                  | 19/100 [02:15<09:35,  7.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.495692601570716\n",
      "val_acc 14.077524038461537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "# 에포크만큼\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "\n",
    "#     print('epoch',epoch)\n",
    "#     print(train_state['epoch_index']) \n",
    "    train_state['epoch_index'] +=1 \n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "\n",
    "#     모델을 학습 모드로 설정 -> 드롭아웃 및 배치 정규화와 같은 학습 중에만 적용되는 기법들이 활성화\n",
    "#     모델을 평가 모드로 전환하려면 classifier.eval()을 사용\n",
    "    classifier.train()\n",
    "# 배치 만큼\n",
    "    for batch_idx, batch_data in enumerate(Traindataloader):\n",
    "\n",
    "        \n",
    "\n",
    "#       1. 옵티마이저 그레디언트 0으로 초기화\n",
    "        optimizer.zero_grad()\n",
    "#       2. 모델에 데이터 넣어서 출력받기\n",
    "        y_pred = classifier(x_in=batch_data['x_data'])\n",
    "#       3. loss 계산하기\n",
    "        loss =  loss_func(y_pred, batch_data['y_target'])\n",
    "    \n",
    "#       tensor(0.3190) -> 0.3190, item()으로 스칼라 값만 추출\n",
    "        loss_t = loss.item()\n",
    "\n",
    "#       배치에서의 평균 loss 구하기\n",
    "        running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "\n",
    "#       4. gradient 계산하기\n",
    "        loss.backward()\n",
    "\n",
    "#       5. 옵티마이저 가중치 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "#       Accuracy 계산\n",
    "        acc_t = compute_accuracy(y_pred, batch_data['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "\n",
    "\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "\n",
    "\n",
    "#   valid에 대한 계산\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "    classifier.eval() # 모델 파라미터를 수정하지 못 하게 비활성화\n",
    "\n",
    "    for batch_idx, batch_data in enumerate(Validdataloader):\n",
    "\n",
    "#       1. 모델의 출력값(y_pred)계산\n",
    "        y_pred = classifier(x_in=batch_data['x_data'])\n",
    "\n",
    "#       2. loss 계산\n",
    "        loss = loss_func(y_pred,batch_data['y_target'])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "\n",
    "#       3. Accuracy 계산\n",
    "        acc_t = compute_accuracy(y_pred,batch_data['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "    \n",
    "    print(\"val_loss\",running_loss)\n",
    "    print(\"val_acc\",running_acc)\n",
    "\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "    \n",
    "\n",
    "#   전체 loss, acc 저장\n",
    "    train_state = update_train_state(model=classifier,\n",
    "                                     train_state=train_state)\n",
    "#   early stop해라고 했으면 학습 멈추기    \n",
    "    if train_state['stop_early']:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037c61f7",
   "metadata": {},
   "source": [
    "### Test 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "331de996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 좋은 모델을 사용해 테스트 세트의 손실과 정확도를 계산합니다\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "\n",
    "# 가중치 업데이트 하지 못 하게\n",
    "classifier.eval()\n",
    "\n",
    "for batch_idx, batch_data in enumerate(Testdataloader):\n",
    "    \n",
    "    y_pred = classifier(x_in=batch_data['x_data'])\n",
    "    loss = loss_func(y_pred,batch_data['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "    \n",
    "    acc_t = compute_accuracy(y_pred, batch_data['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74df48a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 7.267\n",
      "테스트 정확도: 12.97\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 손실: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {:.2f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "342a2c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop_early': True,\n",
       " 'early_stopping_step': 10,\n",
       " 'early_stopping_best_val': 6.419538589624257,\n",
       " 'early_stopping_criteria': 10,\n",
       " 'epoch_index': 20,\n",
       " 'train_loss': [8.67147666023624,\n",
       "  7.120615939940177,\n",
       "  6.559917034641385,\n",
       "  6.242215045036807,\n",
       "  6.003153427954644,\n",
       "  5.81934589724387,\n",
       "  5.665385280886002,\n",
       "  5.540672867528853,\n",
       "  5.432552322264639,\n",
       "  5.332696237871722,\n",
       "  5.24340657264956,\n",
       "  5.161360375342832,\n",
       "  5.083277917677356,\n",
       "  5.013531242647476,\n",
       "  4.955493523228553,\n",
       "  4.896792207994769,\n",
       "  4.838583561681934,\n",
       "  4.792905761349587,\n",
       "  4.7379609231025945,\n",
       "  4.693015256235678],\n",
       " 'train_acc': [3.0572706653225814,\n",
       "  9.088331653225808,\n",
       "  11.143838205645162,\n",
       "  12.375567036290326,\n",
       "  13.20407006048387,\n",
       "  13.730153729838714,\n",
       "  14.201108870967744,\n",
       "  14.544480846774198,\n",
       "  14.764994959677422,\n",
       "  15.141444052419361,\n",
       "  15.39188508064516,\n",
       "  15.615549395161285,\n",
       "  15.940020161290317,\n",
       "  16.37317288306451,\n",
       "  16.70236895161289,\n",
       "  16.9465095766129,\n",
       "  17.412739415322573,\n",
       "  17.61750252016129,\n",
       "  18.13886088709676,\n",
       "  18.520035282258064],\n",
       " 'val_loss': [7.651246015842145,\n",
       "  7.0485306703127355,\n",
       "  6.78089948800894,\n",
       "  6.65202104128324,\n",
       "  6.576415575467623,\n",
       "  6.516040600263156,\n",
       "  6.4738259865687455,\n",
       "  6.448409869120671,\n",
       "  6.4282675706423245,\n",
       "  6.419538589624257,\n",
       "  6.429953593474168,\n",
       "  6.435051624591534,\n",
       "  6.436679418270405,\n",
       "  6.42897947017963,\n",
       "  6.430670866599449,\n",
       "  6.454676756492028,\n",
       "  6.468279893581684,\n",
       "  6.464673794232881,\n",
       "  6.4765980427081775,\n",
       "  6.495692601570716],\n",
       " 'val_acc': [6.865985576923077,\n",
       "  9.983473557692308,\n",
       "  11.20042067307692,\n",
       "  12.454927884615383,\n",
       "  13.025841346153845,\n",
       "  13.341346153846153,\n",
       "  13.551682692307692,\n",
       "  13.694411057692307,\n",
       "  13.987379807692307,\n",
       "  14.115084134615383,\n",
       "  13.949819711538463,\n",
       "  14.43810096153846,\n",
       "  14.25030048076923,\n",
       "  14.51322115384615,\n",
       "  14.415564903846153,\n",
       "  14.197716346153847,\n",
       "  14.235276442307692,\n",
       "  14.505709134615385,\n",
       "  14.520733173076922,\n",
       "  14.077524038461537],\n",
       " 'test_loss': 7.266975146073562,\n",
       " 'test_acc': 12.965745192307692,\n",
       " 'model_filename': 'model.pth'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9403cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(results):\n",
    "    \"\"\"\n",
    "    임베딩 결과를 출력합니다\n",
    "    \"\"\"\n",
    "    for item in results:\n",
    "        print (\"...[%.2f] - %s\"%(item[1], item[0]))\n",
    "\n",
    "def get_closest(target_word, word_to_idx, embeddings, n=5):\n",
    "    \"\"\"\n",
    "    n개의 최근접 단어를 찾습니다.\n",
    "    \"\"\"\n",
    "\n",
    "    # 다른 모든 단어까지 거리를 계산합니다\n",
    "    word_embedding = embeddings[word_to_idx[target_word.lower()]]\n",
    "    distances = []\n",
    "    for word, index in word_to_idx.items():\n",
    "        if word == \"<MASK>\" or word == target_word:\n",
    "            continue\n",
    "        distances.append((word, torch.dist(word_embedding, embeddings[index])))\n",
    "    \n",
    "    results = sorted(distances, key=lambda x: x[1])[1:n+2]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "00dcde86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어를 입력해 주세요: good\n",
      "...[6.79] - ascii\n",
      "...[6.80] - bestowing\n",
      "...[6.84] - court\n",
      "...[6.86] - furs\n",
      "...[6.90] - delicate\n",
      "...[6.91] - parts\n"
     ]
    }
   ],
   "source": [
    "word = input('단어를 입력해 주세요: ')\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = cbow_vocab.token_to_idx\n",
    "pretty_print(get_closest(word, word_to_idx, embeddings, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d908444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======frankenstein=======\n",
      "...[7.17] - computers\n",
      "...[7.32] - submit\n",
      "...[7.50] - youngest\n",
      "...[7.55] - played\n",
      "...[7.56] - status\n",
      "...[7.60] - tranquillize\n",
      "=======monster=======\n",
      "...[7.23] - luxuriances\n",
      "...[7.36] - golden\n",
      "...[7.43] - frightened\n",
      "...[7.45] - machines\n",
      "...[7.47] - passages\n",
      "...[7.47] - damp\n",
      "=======science=======\n",
      "...[6.68] - considered\n",
      "...[6.78] - wreaked\n",
      "...[6.83] - image\n",
      "...[6.88] - changed\n",
      "...[6.93] - gigantic\n",
      "...[6.93] - inconceivable\n",
      "=======sickness=======\n",
      "...[7.12] - cursed\n",
      "...[7.19] - bernard\n",
      "...[7.20] - dash\n",
      "...[7.21] - blow\n",
      "...[7.25] - construct\n",
      "...[7.28] - angelic\n",
      "=======lonely=======\n",
      "...[7.14] - ramble\n",
      "...[7.34] - deprived\n",
      "...[7.42] - the\n",
      "...[7.42] - diabolically\n",
      "...[7.48] - daring\n",
      "...[7.54] - modulated\n",
      "=======happy=======\n",
      "...[6.75] - air\n",
      "...[6.83] - trials\n",
      "...[6.87] - precisely\n",
      "...[6.90] - deprecate\n",
      "...[6.92] - might\n",
      "...[6.95] - animated\n"
     ]
    }
   ],
   "source": [
    "target_words = ['frankenstein', 'monster', 'science', 'sickness', 'lonely', 'happy']\n",
    "\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = cbow_vocab.token_to_idx\n",
    "\n",
    "for target_word in target_words: \n",
    "    print(f\"======={target_word}=======\")\n",
    "    if target_word not in word_to_idx:\n",
    "        print(\"Not in vocabulary\")\n",
    "        continue\n",
    "    pretty_print(get_closest(target_word, word_to_idx, embeddings, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d2c1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a12dab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
