{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "684546a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "316b7ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('glove.6B.100d.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f267e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "with open('glove.6B.100d.txt') as f:\n",
    "    print(len(f.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f5fcffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\n",
      "\n",
      "{'the': 0}\n",
      "[array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
      "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
      "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
      "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
      "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
      "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
      "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
      "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
      "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
      "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
      "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
      "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
      "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
      "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
      "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
      "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
      "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ])]\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "word_vectors = []\n",
    "\n",
    "with open('glove.6B.100d.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        print(line)\n",
    "        line = line.split(\" \") # ex) the -0.038194 -0.24487 0.72812 -0.39961 .....\n",
    "        word = line[0] # ex) the\n",
    "        vec = np.array([float(x) for x in line[1:]]) # ex) -0.038194 -0.24487 0.72812 .....\n",
    "\n",
    "        word_to_index[word] = len(word_to_index) # 딕셔너리에 할당, ex) {'the': 0}\n",
    "        word_vectors.append(vec) # ex) [-0.038194 -0.24487 0.72812 ..... ]\n",
    "        \n",
    "        print(word_to_index)\n",
    "        print(word_vectors)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "625b680c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delayering 0.24859 0.2975 -0.053222 0.1037 -0.28096 0.18186 -0.00012976 -0.1191 0.47487 -0.2665 0.13091 0.2656 0.21277 -0.22553 -0.30563 0.17431 -0.4806 0.56025 0.48418 0.32567 -0.076064 0.49372 -0.31858 -0.52617 -0.32896 0.40217 -0.2874 0.027399 -0.25369 -0.59299 0.46551 -0.29286 0.15582 -0.0515 0.04039 -0.45929 -0.12762 -0.91255 -0.020881 0.21753 -0.064762 0.57053 0.062286 0.25849 -0.22679 0.034007 0.17225 0.32765 0.27137 0.038728 -0.12899 -0.088138 -0.80079 -0.17171 0.56097 0.94171 0.56148 0.52388 -0.23956 0.066374 0.14039 -0.027012 0.30229 -0.42897 -0.27329 0.11149 -0.045983 -0.17291 -0.13729 -0.068946 0.17973 -0.063772 -0.25256 0.32057 -0.47198 -0.046441 0.10799 0.22005 0.31429 0.30552 -0.022177 0.22133 0.37605 -0.0027247 -0.18582 0.024003 -0.019587 -0.082303 -0.70812 -0.17766 0.034959 -0.040964 -0.26912 -0.47983 0.08808 -0.18436 0.3538 0.1121 -0.46207 0.51087\n",
      "\n",
      "wilchcombe -0.1953 -0.33153 -0.21843 -0.049225 0.48725 -0.37651 0.076745 -0.19462 0.47653 -0.20441 -0.2409 0.011004 -0.085083 -0.16333 -0.15969 0.2588 -0.29716 0.2718 0.22613 0.38337 -0.25714 -0.012848 -0.44327 -0.55464 -0.034959 0.19468 0.11577 0.10917 0.22932 0.3254 -0.14084 -0.077283 0.054828 0.17466 0.11871 -0.17873 -0.10224 0.18864 -0.33085 0.08203 0.055497 0.36996 0.25004 0.37812 0.081496 -0.22398 -0.28277 0.079677 -0.078178 0.19644 0.12233 -0.3604 0.11761 -0.24837 0.36274 0.50433 -0.052883 0.39382 -0.78511 -0.10607 0.049087 -0.28529 0.066626 -0.22869 -0.38559 0.16809 0.068536 -0.1721 -0.13524 0.013627 0.39603 0.43538 0.064971 0.42558 0.013532 0.022421 -0.059128 -0.056668 0.46119 0.08967 -0.33702 -0.32732 0.44715 -0.65365 0.36397 0.0017284 -0.013692 -0.042055 -0.25367 0.070942 0.20582 0.14958 0.47098 -0.24649 0.70903 0.15471 0.3664 0.47313 -0.74334 0.20896\n",
      "\n",
      "tarino 0.086117 -0.15972 -0.36481 -0.30893 0.00042484 -0.34121 0.3798 0.11524 0.026795 0.048982 -0.10654 0.40578 -0.036713 -0.483 0.071287 0.12328 -0.27847 0.22432 -0.17442 0.29711 -0.38617 0.14847 -0.27996 -0.31264 -0.60293 -0.55001 -0.16652 0.18829 -0.059169 -0.033542 0.32835 0.074909 -0.35979 -0.12873 0.12306 0.036758 -0.044418 -0.19755 0.31896 0.39271 0.44211 0.39772 0.22076 0.16369 -0.23809 0.73038 0.092143 0.62646 0.32003 0.63233 -0.024474 0.28155 -0.70186 -0.2731 0.71065 1.0977 0.050131 0.19971 -0.76377 -0.31419 0.3447 -0.56416 0.45471 -0.017183 -0.56559 0.080511 0.050298 -0.60262 -0.11198 -0.182 -0.29483 -0.056923 -0.11997 0.068868 0.15588 0.34654 0.63501 -0.023101 1.1733 -0.66838 -0.46216 0.27955 0.34477 -0.31414 0.23793 0.20057 -0.086766 0.20041 -0.25416 -0.10702 0.21543 0.097379 0.11313 -0.22559 0.0090517 0.069997 0.21001 0.47474 -0.39755 -0.14663\n",
      "\n",
      ",763 0.23005 0.036652 -0.60606 0.10154 -0.7387 -0.18513 0.50365 -0.1626 0.49043 -0.40111 0.65146 -0.36296 -0.62135 0.057379 -0.016051 0.40165 -0.015475 0.56722 0.40532 -0.22716 -0.15184 -0.017155 -0.6052 -0.041512 -0.11068 0.056777 0.62946 0.25343 0.29224 0.1857 -0.047627 0.14507 0.088204 -0.4734 0.19705 -0.25488 -0.30257 -0.66926 0.66634 -0.03402 0.27047 0.12429 -0.2423 0.5439 0.26268 -1.1578 -0.55362 0.42771 0.20422 0.62228 0.57402 0.40836 -0.091739 -0.6164 0.2388 1.0895 0.50563 0.19374 -0.73298 -0.59102 0.075358 -0.38056 0.22037 -0.15495 -0.31533 -0.077775 -0.63537 -0.44353 0.33475 0.087329 0.083112 0.040594 0.092782 0.40331 -0.11054 -0.48924 -0.018061 0.16015 0.32025 0.22928 -0.52068 -0.2398 0.48205 -0.56522 0.48089 0.6405 -0.32717 -0.034359 0.024043 0.31429 0.035479 -0.11861 -0.0071768 -0.598 0.48542 0.04029 0.2628 0.38139 -0.13481 -0.37488\n",
      "\n",
      "bager -0.013493 -0.25268 -0.5281 -0.054072 0.015935 -0.40687 0.11283 -0.0043359 0.16022 -0.28472 0.039302 0.34058 -0.36715 -0.092509 -0.062832 -0.025268 -0.54715 0.2994 0.071062 0.05531 -0.36054 -0.082989 -0.21344 0.064384 -0.39416 0.19237 0.55078 0.34498 0.11993 0.28075 0.12295 -0.30029 -0.0050168 -0.08921 0.032945 -0.072971 0.089987 -0.048317 0.10715 0.13536 0.24274 0.057653 0.046016 0.47581 0.090398 -0.24434 -0.10705 -0.12727 0.38142 0.24049 0.18834 0.026015 -0.40092 -0.082469 0.093455 0.64512 0.0039154 0.21266 -0.8182 -0.69271 0.17639 -0.2706 0.18098 -0.36952 -0.018216 -0.061909 -0.10715 -0.039621 -0.44678 0.048235 0.066875 0.28902 0.042357 0.28409 -0.076027 -0.080719 -0.082184 0.12919 0.548 0.073723 -0.26969 0.19775 0.28267 -0.17935 0.50475 0.0059327 -0.26273 0.11474 0.14112 0.2371 -0.088083 0.11849 0.12226 -0.35212 0.35919 0.31017 0.12372 -0.10441 -0.47526 -0.56902\n",
      "\n",
      "jarak 0.12448 -0.48942 -0.51368 0.42176 -0.30897 -0.14965 0.24774 0.085538 0.3911 -0.37552 -0.060985 -0.11335 -0.0051106 0.024758 -0.19488 -0.020947 -0.38854 0.45909 0.22849 -0.098006 -0.13736 0.13132 -0.37008 -0.16676 -0.40981 0.26563 0.046679 0.33288 0.049593 0.096424 0.043817 -0.29165 -0.010564 -0.033642 0.372 -0.11505 0.14308 -0.33746 0.23703 0.13855 0.36988 0.20405 -0.12357 0.30313 -0.24179 -0.15753 -0.057992 0.38848 0.035794 0.47362 0.077297 0.31515 -0.17908 -0.42558 0.10646 0.59573 0.11011 0.39184 -0.67632 -0.479 0.092771 -0.38391 -0.25383 -0.092942 -0.1165 0.24558 -0.33899 -0.74011 -0.15112 -0.043073 0.052855 0.2192 0.018624 0.36181 -0.20784 0.10194 0.27493 0.17604 0.62123 0.24033 -0.31455 -0.11047 0.34464 -0.29668 0.39633 0.19016 -0.01939 -0.11711 -0.10641 0.089791 -0.030372 0.18997 0.31334 -0.39207 0.28144 0.23014 0.21384 0.32871 -0.47845 -0.06843\n",
      "\n",
      "raic 0.56996 0.16392 0.21655 0.11175 -0.11299 -0.10875 -0.38276 -0.13522 -0.075241 -0.39435 0.1657 0.094816 -0.16503 0.34979 -0.48431 0.92148 -0.09542 -0.070131 0.18991 -0.22138 -0.53532 -0.02787 -0.41213 0.029356 -0.35808 -0.079042 0.1995 0.27766 -0.029036 -0.44514 -0.23842 -0.17698 -0.16329 -0.11612 0.038903 -0.60858 -0.21868 0.00079796 0.094822 0.096885 0.40949 -0.0056077 -0.18212 -0.080886 0.013681 -0.34428 -0.15444 0.4104 -0.11849 0.056047 0.40061 -0.12011 -0.0019657 -0.47857 0.0016805 0.61299 0.12009 0.45551 -0.088226 -0.43689 0.082873 -0.16063 0.091019 -0.036706 -0.24743 -0.013083 -0.68896 -0.17096 -0.23598 0.12226 0.0182 0.37146 -0.27095 0.80082 0.18008 -0.3211 0.11635 -0.11637 0.39779 -0.10505 -0.70247 -0.30277 0.47415 -0.22516 0.68436 0.14397 -0.034529 0.15578 0.21809 0.55322 0.23855 0.28156 0.44243 -0.45072 0.019372 -0.22368 -0.075884 0.062401 -0.26178 -0.049035\n",
      "\n",
      "shuu -0.30865 -0.2115 -0.3778 0.04152 -0.34699 -0.0011849 0.078108 -0.4179 0.58471 -0.32003 -0.12409 0.25506 -0.34215 0.14848 -0.19776 0.056214 -0.51302 0.3392 0.20179 -0.017135 -0.36973 0.22525 -0.39704 0.17714 -0.20605 0.18415 0.24458 0.12307 0.20041 -0.055644 -0.068886 -0.11265 -0.0032992 -0.18465 0.30765 -0.32634 -0.21186 -0.38539 0.26908 0.24741 0.32766 -0.01058 -0.36389 0.26479 -0.18732 0.13225 -0.22929 0.44739 0.022842 0.18419 -0.2989 -0.13489 -0.47281 -0.56273 0.34571 1.0991 0.17702 0.31043 -0.67222 -0.83646 0.099192 -0.22677 0.11198 -0.20455 -0.45945 0.075886 -0.27945 -0.15926 0.072181 -0.030137 -0.1058 0.48874 0.33076 0.096472 0.0011722 -0.17315 0.19654 -0.14086 0.85192 0.26411 -0.19387 -0.016416 0.50532 -0.076173 0.78677 0.076376 -0.28606 0.60612 -0.1411 0.30334 0.35246 -0.049467 0.10293 -0.036755 0.10206 0.061342 0.069043 0.24739 -0.51558 -0.32948\n",
      "\n",
      "hammels 0.19812 -0.60276 -0.40589 0.22544 -0.094543 -0.60487 0.23436 0.099412 0.28376 0.1378 0.16682 0.66985 -0.37259 -0.1951 -0.053267 -0.19966 -0.17813 0.021725 0.098032 -0.15422 -0.35025 0.024913 -0.20909 -0.51992 -0.59059 0.31794 -0.27527 -0.076417 0.14053 0.062526 0.24628 -0.082835 0.28497 0.19579 0.0053973 -0.13677 0.0067447 -0.64569 -0.17914 -0.16396 -0.030925 0.27783 -0.31426 0.41678 -0.36687 0.36171 0.18731 0.45528 0.19095 0.53141 0.22791 0.10707 -0.094951 -0.6364 -0.036204 0.66468 0.016352 -0.069105 -0.89596 -0.26598 0.22193 -0.16931 0.0050693 -0.32635 -0.21947 0.14455 -0.49701 -0.30337 -0.40443 -0.040393 0.09665 0.1428 0.22686 -0.11369 -0.39557 -0.028708 0.031139 0.081096 0.41331 -0.26981 -0.47088 0.24452 0.1528 0.088461 0.19513 0.31763 0.2676 0.44337 -0.072252 0.7548 -0.091232 -0.065811 0.075905 0.15702 0.19637 -0.16434 0.095519 0.63018 -0.11351 0.37462\n",
      "\n",
      "qazis 0.50782 -0.091502 -0.27381 -0.037782 -0.29676 -0.0083078 0.079292 0.14085 -0.28914 0.084043 0.20031 0.10125 -0.034947 0.11663 -0.62828 0.13325 -0.532 0.18948 0.040536 0.17965 -0.090024 -0.13614 -0.35632 -0.52038 -0.13723 -0.19397 0.57574 0.25374 0.090088 0.014859 0.66814 -0.41095 -0.15419 -0.26561 0.40711 -0.31051 0.048311 -0.52832 0.3558 0.041566 0.036518 0.2509 -0.13821 0.17625 0.0043082 -0.49541 -0.20451 0.3632 -0.10625 0.36185 -0.11571 -0.29315 -0.55799 -0.60227 0.09411 0.66343 0.58664 0.14551 -0.15635 0.00026339 -0.089677 -0.095275 0.15447 -0.21954 -0.22593 0.41694 -0.50493 -0.39797 -0.031071 -0.20301 0.26961 0.079719 -0.010376 0.37919 -0.23185 0.025191 0.22309 0.096337 0.62143 -0.15036 -0.12627 -0.26987 -0.1292 -0.064659 -0.018105 0.28383 -0.079668 0.39865 0.12012 0.14812 0.088526 0.26551 -0.040527 -0.24445 0.3399 0.01635 -0.10974 0.12674 -0.41216 0.11374\n",
      "\n",
      "dntel 0.03444 -0.32518 -0.10747 0.1178 0.24617 -0.012569 0.067531 0.12534 0.24381 -0.28745 0.29677 0.17844 -0.19987 -0.26659 0.024801 0.47411 0.13629 -0.18661 0.57116 0.15817 -0.027283 -0.22439 -0.11243 -0.062966 -0.18286 0.35068 -0.3104 0.4672 0.22938 0.22641 0.13103 -0.062919 -0.0036299 0.19629 0.13398 0.10129 0.044103 -0.46184 0.24982 -0.017678 0.046917 0.54728 0.51826 0.63747 -0.11175 0.1868 -0.27867 0.30235 0.47416 0.1843 -0.039822 0.44859 -0.49313 -0.43404 -0.028038 0.70729 0.34749 0.16896 -0.46276 -0.30056 0.096069 -0.53129 0.20218 -0.31294 -0.19974 0.059288 -0.41189 -0.21886 -0.052952 -0.37887 -0.10463 0.17786 0.0028885 0.28349 -0.13278 0.16174 0.40661 0.04088 0.64595 0.052855 -0.33616 0.25985 0.41843 0.1349 0.29963 -0.06004 -0.09244 0.17744 -0.30419 0.25203 0.31323 0.27273 -0.13444 -0.20037 0.13462 -0.28217 0.27327 0.19238 -0.37265 -0.40271\n",
      "\n",
      "bja 0.024493 -0.11407 -0.035735 -0.099638 -0.29872 -0.23933 -0.40451 0.3202 0.16911 -0.46079 0.5905 -0.24541 -0.030673 0.34854 -0.42644 0.31337 -0.0020512 -0.12499 -0.04259 0.12861 -0.36892 -0.088112 -0.34513 -0.49065 -0.30635 0.072749 0.021246 0.081922 -0.075872 0.17837 -0.077578 0.067309 -0.065232 0.033169 0.027155 -0.35453 -0.19357 0.08276 0.22151 0.074057 -0.0096065 0.26257 -0.010453 0.023923 0.37186 -0.39755 -0.2466 0.26133 -0.29009 0.27195 0.37309 0.31261 -0.029744 -0.25554 0.20647 1.3379 -0.17039 0.20743 -0.19414 -0.30128 0.18249 -0.46506 0.18793 -0.08525 -0.38212 -0.13728 -0.86045 -0.17405 0.073678 -0.046665 0.65241 0.42689 -0.25463 0.34973 0.21836 -0.057385 -0.2468 0.038675 0.25313 0.15951 -0.83838 -0.046292 0.16773 -0.06538 0.13987 0.57554 -0.18082 0.053813 0.20518 0.49068 0.19585 0.16586 0.069619 -0.16287 0.34183 -0.28887 0.20858 0.16761 -0.38481 -0.14724\n",
      "\n",
      "punzalan 0.059433 -0.49847 -0.22751 0.4892 0.10183 -0.031291 0.14421 0.076702 0.30177 0.076958 -0.024055 0.26587 -0.16887 -0.24642 0.03311 0.046497 -0.48815 0.19047 -0.032453 -0.070068 -0.41974 0.012956 -0.31071 0.02668 -0.24915 0.34261 -0.17773 0.376 0.1828 0.12043 0.36273 -0.2617 0.22228 -0.014984 0.3419 -0.31995 0.1231 0.13306 -0.24391 0.20842 0.18998 0.11766 0.13369 -0.011148 -0.0015913 -0.081337 -0.16492 0.51549 0.6375 0.14422 0.2532 -0.083633 -0.076926 -0.52544 0.26426 0.89751 -0.014598 0.29934 -0.63909 -0.28466 0.20332 -0.13693 0.5152 -0.026879 -0.60735 -0.26842 -0.13047 -0.028971 -0.057248 -0.03694 -0.15396 0.25382 -0.0032219 0.15972 0.20262 -0.098623 0.30878 -0.20447 0.59438 0.093145 -0.23031 -0.42702 0.56937 -0.060115 0.23432 0.046149 -0.28409 0.24152 -0.02581 0.43312 -0.078923 0.015373 0.18734 -0.3765 0.18531 0.19629 0.16565 -0.16837 -0.2716 -0.56566\n",
      "\n",
      "coki 0.10451 -0.12407 -0.77333 0.44254 -0.088576 -0.41689 0.10027 0.12219 0.35579 -0.26385 0.20194 0.48986 -0.61093 -0.52473 0.15522 0.25545 0.085995 -0.089453 0.26064 -0.053376 -0.15907 -0.089719 -0.23888 -0.06368 0.067041 0.25338 0.060435 0.038227 0.04226 0.23375 -0.043499 -0.30484 0.56135 0.075087 -0.028202 -0.12149 -0.32859 -0.28036 0.35342 -0.011849 0.29378 -0.062607 0.22427 0.30076 -0.3992 0.20848 0.17153 0.36131 -0.15001 0.60936 -0.086718 0.087188 -0.16628 -0.3343 0.2522 0.57152 -0.0039442 -0.098626 -0.7246 -0.25178 -0.19759 -0.04747 -0.34843 0.026756 -0.58274 0.012508 0.16476 -0.88803 -0.27376 0.1939 -0.44164 0.37359 0.32322 0.29133 -0.39521 -0.14058 0.12757 -0.15722 0.58483 0.1173 -0.6792 0.014548 -0.12758 -0.25289 0.47306 -0.15648 -0.23256 0.29695 0.14087 0.22901 -0.47585 0.49134 0.29919 -0.16019 0.052155 0.046301 0.096036 0.32513 -0.077633 0.46406\n",
      "\n",
      "lockbaum 0.26987 -0.30661 -0.94482 0.21104 0.10051 -0.26391 -0.073147 -0.61344 0.29865 -0.058311 0.12148 0.39428 0.114 -0.0061205 -0.15305 0.014748 -0.10344 0.014642 0.0036553 -0.088161 -0.27218 -0.024139 -0.13343 -0.047438 -0.19339 -0.10415 -0.36647 0.14535 0.010302 -0.098957 0.014799 -0.29952 0.46159 -0.19105 0.50016 -0.091464 -0.20529 -0.21516 -0.036258 -0.0014903 0.1774 0.48102 0.38592 0.075881 -0.12058 0.40076 0.35886 -0.11905 0.46103 0.15286 0.48878 0.17554 -0.30896 -0.67808 -0.027152 0.56961 -0.11998 0.51698 -0.76503 -0.10464 0.23669 -0.35584 0.46881 -0.06946 -0.29726 0.023248 -0.23953 -0.05229 -0.4154 0.098235 0.16116 0.36411 0.26328 0.072643 -0.031619 0.12286 -0.10372 -0.015497 0.39054 0.15773 -0.3984 -0.020538 0.22009 -0.077177 0.6555 0.15582 -0.21253 -0.023976 -0.023596 0.64899 0.019918 0.13875 0.096284 0.11476 0.40638 -0.39982 0.15862 -0.12463 -0.89177 -0.21103\n",
      "\n",
      "nakina 0.022679 -0.14254 -0.11053 0.089841 0.12426 -0.21554 -0.16116 -0.0052568 0.31048 -0.20595 -0.23921 -0.21869 -0.31856 -0.30102 -0.10738 2.497e-05 -0.20619 -0.10556 0.22936 0.085663 -0.46666 0.23822 -0.026347 -0.30141 -0.49793 0.042527 0.070554 0.24933 0.0073047 0.24182 0.14795 -0.17486 0.10487 -0.16002 -0.011896 -0.26409 0.034052 -0.26551 0.11679 0.072439 0.066987 -0.021803 -0.24823 0.11641 -0.23772 0.55646 0.23839 0.2942 0.25526 0.47543 0.040292 0.16568 -0.26016 -0.85371 0.16159 0.70291 0.14722 0.066624 -0.42174 -0.35951 0.20813 -0.37793 0.24458 -0.15606 -0.21934 0.28119 -0.17435 -0.50062 -0.22629 0.17494 0.36378 -0.11448 0.20052 0.2348 0.12324 0.16492 0.297 -0.12296 0.63631 0.094553 -0.17866 0.21814 0.16765 -0.048114 0.49534 0.073172 0.014037 0.10827 0.3173 0.51313 0.061587 0.016514 -0.056654 0.0054159 0.068785 0.27263 0.032186 0.18737 -0.24478 0.26628\n",
      "\n",
      "edwardsi 0.047512 -0.098981 -0.2684 0.12473 -0.40088 -0.2309 0.31875 -0.040068 0.4422 -0.46997 0.13947 0.047987 -0.20111 0.039814 -0.19128 0.19241 -0.13775 0.16432 0.28734 0.27587 0.077948 -0.21706 -0.70545 -0.24731 -0.45917 0.3518 0.4147 0.33697 0.3643 0.52595 -0.044058 -0.12767 0.15919 -0.03739 0.25315 -0.041482 0.0013498 -0.50372 0.18479 0.062025 -0.006902 0.32097 0.13948 0.57694 0.036507 -0.36907 -0.422 0.27814 -0.367 0.67827 -0.1618 -0.16455 -0.20746 -0.30866 0.13957 0.32722 0.29773 0.33266 -0.56194 -0.36022 0.043141 -0.25245 -0.18614 -0.39409 -0.31841 0.11676 -0.21553 -0.30604 -0.17647 -0.064314 0.21226 0.3579 0.10895 0.45818 0.14421 -0.17483 -0.065666 0.20788 0.10035 -0.16769 -0.53332 -0.31066 0.18421 -0.5446 0.74663 0.11016 -0.021919 -0.37007 -0.20071 0.29173 -0.3459 0.32835 0.34862 -0.074547 0.83659 -0.034245 0.43422 0.0032669 -0.52985 -0.0681\n",
      "\n",
      "usapa 0.27467 -0.40008 -0.015657 0.13015 -0.41735 -0.0025469 -0.099384 -0.23398 0.12599 -0.3118 0.12857 0.1197 0.054372 -0.27524 0.10797 -0.21837 -0.44373 -0.1686 0.35434 -0.0013263 -0.41705 -0.14548 -0.22529 0.14046 -0.053646 -0.11728 -0.0013576 0.23065 0.092588 -0.18308 0.1395 0.09073 0.011082 -0.098048 0.0028491 -0.245 -0.18718 -0.39765 0.01491 -0.090798 -0.22677 0.30203 0.0046819 0.09867 0.18926 -0.065085 0.14012 0.33217 0.23253 0.33111 -0.19846 0.2039 -0.13545 -0.22263 0.23617 1.0797 -0.043024 -0.048773 -0.56217 -0.26088 -0.045943 -0.25537 0.2033 0.03962 -0.53465 0.31069 -0.35599 -0.10236 -0.49126 0.21309 0.19632 -0.04312 -0.22434 0.43927 -0.028761 -0.13968 0.50254 0.049991 0.35206 0.26806 -0.46501 -0.46886 -0.054655 -0.34711 0.2782 -0.09575 -0.22021 0.32018 0.11529 0.43159 -0.0058679 -0.12818 -0.2576 0.026862 0.48034 -0.36783 -0.018206 0.67589 -0.34721 -0.40281\n",
      "\n",
      "jókai 0.44201 -0.3677 -0.16177 0.35996 -0.10613 0.018236 0.11803 -0.69579 0.4417 -0.28721 -0.094089 0.35237 -0.24086 -0.13236 -0.73825 -0.11093 -0.5292 0.069844 0.17362 0.213 -0.024378 0.2587 -0.2058 -0.31561 0.2803 -0.34178 -0.098173 0.38917 -0.25871 0.19573 -0.12247 -0.24881 0.19307 0.03078 0.40712 -0.18918 -0.07862 -0.22386 -0.0097109 -0.15789 0.20961 0.26852 0.18665 0.48068 0.081336 0.44477 0.08693 -0.080431 0.3233 0.13897 -0.32402 0.18735 -0.02134 -0.63695 0.30257 0.75325 0.21974 0.078481 -0.84819 -0.31024 0.23228 0.032461 0.24194 -0.069238 -0.34692 -0.0074763 -0.17891 -0.54348 0.014048 0.29129 -0.064578 0.07708 0.15873 0.35396 0.080441 0.043149 0.11729 0.067066 0.19706 -0.028196 -0.28853 0.069933 0.25377 -0.16524 0.21807 0.25233 -0.08667 -0.11411 -0.17844 0.39761 0.32919 0.01441 0.37066 -0.27389 0.41111 -0.087942 0.27098 0.35377 -0.2963 -0.12218\n",
      "\n",
      "exercice 0.07062 -0.35955 -0.4378 0.53535 -0.50708 -0.087279 0.0032011 0.011145 0.027548 0.15831 -0.38467 0.29259 0.081612 -0.50755 -0.46221 0.4085 0.056336 0.26112 0.12828 -0.20582 0.25193 -0.10738 -0.52984 -0.59464 -0.39323 0.15371 0.14585 0.26037 0.060646 -0.20177 -0.01002 -0.4203 0.15957 -0.24243 0.1702 -0.038842 -0.016649 -0.37726 0.2528 -0.093669 0.32359 0.38065 -0.23232 0.067849 -0.252 0.0018063 -0.18135 0.20419 0.35202 -0.015681 0.032731 0.40717 -0.36114 -0.18321 0.30847 0.26758 0.30196 0.04964 -0.45381 -0.33063 0.11267 -0.095897 -0.39331 -0.056964 -0.25657 0.25718 -0.50553 -0.77412 -0.43066 -0.20268 0.32953 -0.042363 0.34223 0.24472 -0.15467 0.16131 -0.029923 0.030148 0.71156 -0.0069856 -0.86456 -0.18408 -0.04169 -0.033 0.16725 0.24055 0.13812 -0.11769 -0.064007 0.39138 -0.45045 0.067812 -0.18108 -0.40388 0.64453 -0.0034858 0.27447 -0.16261 -0.61109 0.1023\n",
      "\n",
      "nightmask 0.28227 -0.2224 -0.18635 -0.0040821 0.0028202 0.03852 0.16468 -0.15172 0.44858 0.15142 -0.26355 -0.23079 -0.33825 -0.18988 -0.10821 0.10239 0.011814 0.16739 0.18013 0.037466 -0.36145 -0.060286 -0.51908 0.020315 0.097272 0.68789 0.3422 0.23229 0.059084 -0.16372 0.31777 -0.12503 -0.11612 -0.01791 0.22729 -0.24475 0.13344 -0.76457 -0.21842 -0.23255 0.32896 -0.028703 -0.015641 0.26641 -0.0060806 0.13015 -0.12862 0.45426 0.28282 0.37816 -0.15416 0.31514 -0.37371 -0.11011 0.69136 0.5585 -0.039816 0.4442 -0.49551 -0.10056 0.074204 -0.41243 -0.058199 -0.0017325 -0.09414 -0.056304 -0.11575 -0.15565 -0.37496 -0.093905 0.33139 0.15539 0.17836 0.34818 -0.080993 0.063419 0.090889 0.027331 0.69973 0.026764 -0.29021 -0.18866 0.32936 0.049781 0.57822 -0.31824 -0.18431 0.15763 0.14582 0.12256 -0.29638 0.078356 0.064358 -0.17546 0.59107 -0.076119 -0.022521 0.35394 -0.1271 0.15224\n",
      "\n",
      "luthe -0.038181 -0.22276 -0.70862 0.038671 -0.11481 -0.23252 0.15112 -0.20043 0.34074 -0.12219 0.14416 -0.033747 -0.049017 -0.49712 0.049752 0.17606 0.013214 0.54069 0.15605 0.035147 -0.61974 -0.29712 0.096073 -0.054513 -0.16509 -0.10276 -0.33591 0.15115 0.11666 -0.21253 -0.11457 -0.046098 0.16826 -0.013514 -0.059873 -0.015268 0.1311 -0.38911 0.14118 0.15948 0.084135 0.31645 -0.044016 0.46048 -0.070979 -0.060617 0.29539 0.23156 0.37534 0.21357 0.20445 -0.027585 -0.20639 -0.29636 0.18275 0.75335 0.26929 0.14395 -0.4729 -0.13497 0.33185 -0.11129 0.22484 0.15565 -0.44462 -0.04668 -0.26302 -0.11476 -0.17331 0.3027 -0.12992 0.21 -0.17346 0.25941 0.026344 0.19164 0.091903 -0.2315 0.75203 0.11286 -0.5647 -0.20504 0.18216 -0.234 0.35311 0.16596 -0.1681 -0.070215 -0.33764 0.06043 0.055366 -0.18373 0.29853 -0.33478 0.3849 -0.2009 0.18926 0.39858 -0.51551 -0.073537\n",
      "\n",
      "gyeongsan 0.13467 0.12089 0.36066 0.39759 0.14253 0.12672 -0.097883 0.39522 0.6142 0.54633 -0.041662 -0.29693 0.0052409 -0.09111 -0.0010817 0.41532 -0.12645 0.23059 0.26758 -0.1584 -0.048093 0.43305 -0.14544 -0.052129 -0.6174 0.21919 0.38258 0.41783 -0.36351 0.49933 -0.34341 -0.2533 0.28331 -0.13397 -0.2973 -0.40011 -0.085768 -0.16109 -0.09834 0.15997 0.19257 0.018778 -0.37251 0.0079805 -0.080518 0.30429 0.6998 0.26686 0.03087 0.62389 0.18063 0.12593 0.035344 -0.93362 0.26636 0.41312 0.069898 -0.11526 -0.86687 -0.68611 -0.11441 -0.27814 -0.041584 -0.11593 -0.20766 0.38729 -0.2491 -0.069764 -0.54323 0.17879 0.14524 -0.081519 -0.0057168 0.33876 -0.10168 0.044655 0.85344 -0.070945 0.5085 -0.091153 -0.34388 0.075876 0.12773 -0.13121 0.481 0.65774 -0.36662 0.47928 0.41633 0.13801 0.42645 0.055004 0.11371 -0.014677 0.051595 0.022747 0.075894 0.32711 -0.40559 0.17786\n",
      "\n",
      "winair 0.31774 -0.10979 -0.51416 0.07558 0.1852 -0.28251 -0.36381 0.17961 0.2099 -0.37237 -0.15036 0.22309 0.23725 0.38865 -0.31626 -0.036749 -0.56727 0.048961 0.3377 -0.018984 -0.33831 -0.018611 -0.020146 -0.055772 0.11463 0.26776 -0.062649 0.35375 -0.12948 0.18849 0.28931 -0.018192 -0.25191 -0.15829 0.13619 0.10922 0.22542 -0.52939 -0.1549 0.1112 -0.049314 0.29796 0.0095232 0.38522 -0.052215 0.30311 -0.17183 0.47505 0.34886 0.15794 -0.0021061 0.079284 -0.27089 -0.58207 0.21401 0.95023 0.40113 0.35678 0.012174 -0.057281 0.037933 -0.41324 0.26448 -0.40936 -0.58792 -0.028305 -0.46036 -0.064513 0.061803 0.077317 -0.018255 -0.23923 0.15985 0.36126 -0.054095 0.0021038 0.26343 -0.077783 0.46251 0.18601 -0.38485 -0.38495 0.20116 0.12998 0.33667 -0.089872 -0.025308 0.54674 -0.046732 0.31852 -0.24022 0.39295 -0.082988 -0.56745 0.14245 0.21791 0.1039 0.23373 -0.48721 -0.024379\n",
      "\n",
      "enping 0.36819 0.073209 0.068805 0.28243 0.42469 0.36103 -0.51845 0.0063942 0.53068 0.16994 -0.2937 -0.47924 -0.15899 -0.11967 -0.35287 0.076239 -0.18002 0.3779 0.40414 -0.20637 -0.39084 0.025741 -0.27357 -0.26871 -0.74889 0.030104 0.20978 0.72348 -0.31229 -0.025004 -0.0070337 -0.53419 0.43009 -0.21792 -0.12658 -0.35628 -0.09859 -0.17691 0.20629 -0.0037675 0.34657 0.12216 -0.15853 -0.15713 -0.13297 0.020167 0.5417 0.21142 0.27136 0.40978 0.10368 0.010227 -0.39709 -0.53026 0.26596 0.76723 -0.19132 -0.01867 -0.88063 -0.41046 -0.12889 -0.2244 0.28714 -0.016366 -0.7359 0.16041 -0.10466 -0.11545 -0.42291 0.022369 0.061075 0.0325 -0.011936 0.45198 -0.12642 0.042781 0.21957 -0.12572 0.42909 0.095621 -0.57673 0.18365 -0.16209 0.16043 0.69324 0.25383 -0.61125 0.39308 -0.14406 0.16464 0.26516 -0.23465 0.048941 -0.17059 -0.066527 0.014037 -0.11652 0.32216 -0.17371 -0.15998\n",
      "\n",
      "suski -0.22454 -0.32996 -0.28089 0.094348 -0.10674 0.072217 -0.14283 0.14668 0.35245 -0.53052 -0.059136 0.072408 -0.13897 -0.23378 -0.032518 0.4917 -0.20148 0.18604 -0.078189 -0.14329 -0.45349 -0.047785 -0.40976 -0.019905 -0.28307 0.14869 -0.05808 0.56064 -0.062574 0.23288 0.026001 -0.039935 -0.094496 0.13552 0.22756 -0.10035 0.016155 -0.24032 0.14623 0.12332 -0.03277 0.34415 -0.064641 0.22668 -0.27536 -0.058737 -0.19066 0.29154 0.15123 0.16407 0.20344 0.012237 -0.26509 -0.33223 0.17609 0.87055 -0.048432 0.18738 -0.76006 -0.18662 0.5699 -0.36922 0.23694 0.066013 -0.3926 0.20396 -0.22226 -0.46089 -0.37977 0.17803 -0.023254 0.30825 0.11603 0.23976 -0.038656 0.072838 -0.050093 -0.17721 0.5448 -0.015412 -0.37547 -0.081602 0.0063674 -0.52721 0.57433 0.14179 0.015606 0.20719 -0.12488 0.19347 -0.0066619 0.095257 -0.091523 -0.21296 0.22841 0.15396 0.29948 0.33227 -0.53151 -0.24102\n",
      "\n",
      "anangpal -0.1012 -0.17889 -0.29258 0.67071 -0.0090491 -0.079047 -0.0096101 0.18504 0.078431 -0.19818 -0.054367 0.34498 -0.18378 -0.033738 0.64865 -0.27806 -0.66089 0.40607 -0.12285 -0.26674 -0.95134 0.0053545 -0.33655 -0.46777 -0.16644 0.034879 0.11476 0.11478 0.47802 0.015038 0.4372 -0.47978 0.044671 0.087321 -0.19923 0.097804 -0.11285 -0.24294 0.13032 0.27654 -0.0083548 0.67473 0.2872 0.22887 0.23191 -0.1348 -0.0019946 0.31024 0.34716 0.13902 -0.035538 0.17939 -0.16084 -0.64291 0.085162 0.79726 -0.39265 0.56322 -0.12966 -0.66899 0.44032 -0.38922 0.12425 -0.13575 -0.28611 0.06781 -0.53658 -0.069048 -0.23486 0.25079 0.57748 0.011709 -0.060049 0.5217 -0.21111 0.014994 0.39279 -0.55368 0.38417 0.31797 0.073795 -0.32439 0.5227 0.35164 0.32081 0.49472 -0.69413 -0.68846 -0.31866 -0.31168 0.27146 -0.12169 0.2524 -0.35806 0.093177 0.23405 0.2736 0.37922 -0.42972 -0.15602\n",
      "\n",
      "singhana 0.17912 -0.064993 -0.12501 0.4698 -0.12391 -0.13619 0.060488 -0.21128 -0.17203 -0.60679 0.14508 0.3783 -0.32619 -0.046359 0.087649 -0.091178 -0.097075 -0.13223 0.31171 -0.078136 -0.66013 -0.086726 -0.21853 0.053766 -0.15817 -0.013724 -0.035846 0.33517 0.47809 0.052198 0.37808 -0.2209 0.29391 0.02983 0.10689 0.12007 -0.068787 -0.10489 -0.11197 0.075963 0.21555 0.10307 -0.037005 0.41066 -0.1572 0.073083 0.13854 0.3652 0.24516 0.11637 0.31915 -0.077677 -0.16109 -0.35732 0.45559 0.73702 -0.061179 0.22928 -0.37268 -0.30163 -0.12431 0.10012 0.51463 -0.096033 -0.067258 0.1839 -0.60891 -0.13752 -0.34762 0.21616 0.3546 -0.20817 3.1821e-05 0.42245 0.00076655 -0.42786 0.13695 0.16691 0.17629 0.30966 -0.29604 -0.028966 0.049758 0.11169 0.47917 0.27835 -0.53882 -0.63173 0.017823 0.24063 0.13259 0.43012 -0.10651 0.39296 -0.013153 -0.087604 0.2152 0.28557 -0.23021 -0.24352\n",
      "\n",
      "weisbuch -0.16758 -0.31783 -0.2128 -0.13485 0.12636 -0.35267 -0.097758 0.065924 0.23892 -0.29251 -0.29295 0.13926 -0.31907 -0.2988 0.082827 0.29679 -0.35757 0.32375 -0.15421 0.058113 -0.37603 0.20106 -0.010218 -0.36623 -0.47828 0.25137 0.21409 0.30555 -0.019566 0.15677 0.22208 -0.30429 0.041264 -0.32503 -0.15578 -0.21233 0.15897 -0.39854 0.26405 0.18367 0.34438 0.38804 -0.29731 0.6097 -0.14154 0.14875 -0.10438 0.10623 0.020902 0.31631 0.58022 -0.61076 -0.17183 -0.75194 0.48314 0.569 0.035927 0.3755 -0.68818 -0.4449 0.31452 -0.47436 0.54096 -0.44696 -0.35896 -0.11019 -0.25154 0.25622 -0.12919 -0.081139 0.34626 0.42549 0.21629 0.33648 0.34587 -0.12965 -0.099639 0.10081 0.19804 0.14818 -0.076631 -0.24351 -0.0035887 -0.71209 0.34591 0.15834 0.14717 0.17217 0.096156 0.16524 0.47309 -0.15324 0.29486 -0.17273 0.21203 -0.079216 0.13203 0.34562 -0.63108 0.28353\n",
      "\n",
      "aronin 0.12558 -0.13855 -0.47534 0.081972 0.016697 -0.1881 0.086859 -0.12484 0.37181 -0.27958 -0.13093 0.017208 -0.29338 -0.1564 0.021403 0.27261 -0.29193 0.48457 0.20747 -0.2353 -0.22925 0.14436 -0.24071 -0.22067 -0.26299 0.030659 0.040165 0.31028 -0.012345 0.16865 -0.095198 -0.06543 0.15513 -0.090853 0.20136 -0.20759 0.38827 -0.44227 -0.13086 0.13101 0.45338 -0.063283 0.049525 0.22636 -0.10988 0.24729 0.24964 0.012621 0.11463 0.16008 0.079609 -0.21784 -0.29833 -0.30796 0.23781 0.5703 -0.0057539 0.24665 -0.95781 0.038706 0.11676 -0.34107 0.11854 -0.21403 -0.4635 -0.0075296 -0.23604 0.06496 -0.36043 0.21174 0.1484 0.10013 0.19591 0.2702 0.15321 0.002729 -0.12414 -0.08781 0.42935 0.21357 -0.12598 -0.0070019 0.040071 -0.44624 0.57906 0.019635 0.07354 0.1021 -0.23696 0.1912 0.21964 0.12821 0.26702 -0.228 -0.02309 -0.10537 -0.043359 0.29627 -0.59979 0.0066042\n",
      "\n",
      "shuinan 0.34717 -0.27897 0.081754 0.096766 -0.076866 0.049962 -0.30722 0.12443 0.57938 -0.06795 -0.40304 0.36607 -0.11241 -0.23393 -0.14165 -0.0038639 0.29205 -0.2778 0.016976 -0.28312 -0.36071 0.012158 -0.29868 -0.14666 -0.34541 -0.11342 -0.0059134 0.15379 -0.15135 0.39776 -0.27507 -0.4053 0.56694 0.22263 -0.30763 -0.18013 0.20412 -0.17651 0.45932 0.074623 0.41882 0.075541 0.44361 0.90475 0.14519 0.43925 0.036817 0.37078 0.21181 0.55717 -0.024505 0.12396 -0.27061 -0.3537 0.16481 0.73092 -0.43228 0.20682 -0.47989 -0.61276 -0.31173 -0.32558 -0.11744 -0.36347 -0.67107 0.0060282 -0.45535 -0.53861 -0.4044 -0.038617 -0.07416 -0.093288 0.51692 -0.064576 -0.071442 -0.62961 0.28303 -0.016523 0.17363 0.34625 -0.20328 0.33581 0.37325 -0.20491 0.27645 0.12136 -0.037722 0.7261 0.3733 0.35799 0.17799 0.40695 -0.0047076 -0.25764 -0.044609 -0.048948 0.21197 0.77894 0.082028 0.4242\n",
      "\n",
      "jobey 0.14158 0.062041 -0.55304 0.030772 -0.12449 -0.2486 0.039364 -0.48024 -0.2126 -0.68013 -0.069663 0.040412 -0.24221 -0.35266 -0.28021 -0.23452 -0.51244 -0.035407 -0.55876 -0.051536 -0.32132 -0.20642 -0.2443 -0.3616 -0.14527 -0.3329 -0.23833 0.012386 -0.31611 0.19285 0.29022 -0.3792 0.18904 0.10542 0.35121 0.48353 -0.22516 -0.38944 -0.19649 0.16569 0.20574 0.24666 0.45103 0.54855 -0.099243 0.030317 -0.34152 -0.10896 0.37902 0.41546 0.69493 0.10351 -0.21438 -0.45852 0.52037 0.53551 -0.052743 -0.017251 -0.76473 -0.57737 0.35374 -0.39744 0.31998 0.1347 -0.22104 -0.21198 -0.53397 0.23679 -0.55602 0.31844 -0.167 0.20195 -0.042796 0.17985 -0.036159 -0.18863 0.20555 0.14185 0.4161 0.29474 0.048115 -0.14152 -0.020274 0.079317 0.49153 -0.21168 -0.25638 -0.25171 -0.038205 -0.20915 -0.014519 -0.39634 -0.24957 0.039755 0.082812 -0.25784 0.45774 -0.053938 -0.67854 -0.43727\n",
      "\n",
      "bardai -0.054359 -0.25945 -0.32113 0.28322 0.013213 0.45961 -0.034009 -0.23181 0.027532 -0.14108 -0.0036226 0.17512 -0.21238 -0.36967 0.1437 -0.26543 -0.24083 0.057064 0.22355 -0.024634 -0.46751 -0.032687 -0.38689 0.029549 -0.38432 -0.64942 0.25924 0.019404 -0.010034 0.20805 0.3695 -0.33804 0.18673 -0.12735 0.23943 -0.44909 0.31443 -0.56266 -0.0058918 -0.066482 -0.057949 0.52581 0.020972 0.11922 -0.021988 -0.18251 0.16783 0.18844 0.27995 0.21236 -0.2631 -0.25808 -0.24294 -0.8039 0.059859 0.6745 -0.1528 -0.083198 -0.55327 -0.020513 -0.019508 -0.071066 -0.017047 0.13492 -0.49545 0.042009 -0.12397 0.12579 -0.16823 -0.068809 -0.13654 0.19228 -0.048354 0.28914 -0.1279 0.37828 -0.13681 0.2805 0.41867 0.33995 -0.47579 -0.18812 0.22402 -0.1319 0.71925 0.16177 -0.11491 0.27967 -0.19957 0.10015 0.29386 -0.13166 -0.26669 0.029565 0.29797 0.12546 0.12112 0.14918 -0.32197 0.13075\n",
      "\n",
      "passagers 0.35269 -0.097465 -0.32183 -0.50325 0.10062 -0.095485 0.014758 0.22868 0.0025078 -0.42443 0.32609 0.28518 -0.040347 0.1228 -0.50224 -0.096843 -0.43839 -0.17336 0.17809 -0.061523 0.12285 0.15196 -0.82684 -0.20148 -0.20741 -0.26718 0.098268 0.43428 0.27338 0.053515 -0.0073961 -0.20221 -0.22621 0.17498 0.18191 0.26084 0.23306 -0.43506 -0.010994 0.21685 -0.040386 0.018109 -0.38641 0.27326 0.064034 0.27265 -0.46144 0.258 0.066314 0.27342 -0.23641 -0.13434 -0.6175 -0.24083 0.44092 0.71382 0.34265 -0.002267 -0.16098 -0.45483 0.37536 0.10942 -0.14123 0.058544 -0.48279 0.46634 -0.39141 -0.54028 -0.35058 -0.35516 -0.14236 -0.12177 0.28573 0.1133 -0.016354 0.29879 0.25066 0.4817 0.82186 0.44727 -0.31013 -0.17862 0.34372 -0.089276 0.4334 -0.20151 -0.13574 0.042543 -0.34532 0.32333 -0.14302 0.36782 0.15866 -0.29704 0.10783 0.11906 0.14585 0.54051 -0.45778 0.11109\n",
      "\n",
      "o'mahonys 0.090761 -0.25884 -0.24094 0.13924 -0.070854 -0.09596 0.20461 0.34854 0.24478 -0.1977 0.27767 0.33748 -0.21684 -0.096003 -0.30387 0.0021658 -0.098006 0.11448 0.072238 -0.28141 -0.028963 -0.14033 -0.27786 0.26393 -0.34098 -0.26368 -0.10627 0.30824 -0.076351 -0.11913 0.43823 -0.54328 0.078509 -0.23167 0.24734 -0.50087 -0.10814 -0.1554 0.18466 0.077391 0.15203 0.41241 -0.22687 0.28798 -0.074265 -0.0099861 0.58088 0.23168 0.24969 0.18517 0.015946 0.21532 -0.138 -0.24835 0.41185 1.1045 -0.027344 0.17385 -0.43222 -0.5591 -0.01019 -0.33625 -0.10512 -0.0029636 -0.33511 -0.090131 -0.48173 -0.065282 -0.51943 -0.25435 -0.085039 0.23416 -0.051023 0.088156 -0.25787 0.053227 0.084849 0.13255 0.4036 0.19779 -0.43154 -0.12619 0.085712 0.089037 0.36931 -0.085106 -0.13521 0.56077 -0.47713 0.66249 -0.21377 -0.19425 -0.13634 0.0058398 0.10384 -0.51525 -0.17436 0.54366 -0.098621 0.18777\n",
      "\n",
      "kiamba -0.22291 -0.35873 -0.31646 0.49168 0.17569 0.068313 0.058733 0.0027205 0.12495 -0.23291 -0.5237 -0.13552 -0.30798 -0.026261 -0.11756 -0.21431 0.0025837 0.28563 0.018864 -0.37275 -0.64125 0.022389 -0.21925 -0.3333 -0.66256 -0.12723 0.22985 0.22276 -0.52436 0.022671 0.24689 -0.57942 0.56582 -0.093118 0.081867 -0.30267 -0.17036 -0.11811 0.058451 0.26635 0.21028 -0.16244 0.02584 0.043157 -0.53183 0.38456 0.26244 0.015091 0.050691 0.37521 0.09334 -0.22115 -0.09609 -0.63461 0.20217 0.67792 -0.076304 -0.1124 -0.6211 -0.38874 0.418 -0.16711 0.32609 -0.33198 -0.47958 0.30583 0.045997 -0.19838 -0.4865 -0.17707 0.40916 0.14378 -0.068675 0.35991 -0.010319 0.089683 0.19615 -0.025177 0.50174 0.092352 -0.46981 -0.20218 0.0040184 -0.50363 0.74335 0.03673 -0.15098 -0.2722 0.34181 0.19331 0.46591 0.016941 0.18228 0.15646 0.089403 0.29829 -0.011834 0.60065 -0.34187 0.27588\n",
      "\n",
      "grotnes -0.13134 -0.12041 -0.28774 0.19245 -0.44872 -0.44011 0.10909 -0.023272 0.022809 -0.2556 -0.10364 0.54163 0.12611 -0.05773 0.055992 0.020179 -0.34987 0.15836 0.35392 0.20057 -0.59723 0.16804 -0.30334 0.061214 0.1177 -0.12268 -0.015605 0.4074 0.095572 0.052736 0.23277 0.32408 0.31351 0.022179 0.55738 -0.00048445 0.068804 -0.22861 0.15461 -0.37453 0.35434 0.24729 -0.024849 0.71517 0.021073 -0.02522 -0.27119 0.30615 0.57503 0.38516 -0.24385 0.35135 -0.23593 -0.071674 0.47453 0.72548 -0.21483 0.33486 -0.38086 -0.078776 0.53519 -0.39185 0.18592 -0.1734 -0.30236 -0.15626 -0.17184 0.38247 -0.40019 0.017522 0.049407 0.218 0.34063 0.56197 -0.056679 -0.12172 0.058426 0.35593 0.68487 0.28771 -0.50089 -0.56979 0.13304 -0.20042 0.050943 0.12286 -0.17285 -0.1449 -0.33793 0.35333 0.30412 0.4837 -0.055393 -0.25381 0.31857 -0.15578 0.30345 0.7963 -0.55631 -0.24073\n",
      "\n",
      "undiplomatically -0.17842 -0.54916 -0.3831 0.6114 -0.21962 0.1005 -0.083866 0.27116 0.34813 -0.23619 0.098029 0.45184 0.22521 0.12788 0.064047 0.21069 -0.84937 0.28596 0.50809 -0.21604 -0.02607 0.10164 -0.13939 -0.58102 -0.081768 -0.20395 -0.15587 0.29051 -0.061197 0.31239 0.14741 -0.12033 -0.13674 -0.04555 0.18631 0.015206 -0.31756 -0.73755 -0.15704 0.15146 0.77819 0.43763 0.35911 0.31357 0.0042165 -0.024934 0.21159 0.19345 0.26221 0.16432 0.090927 -0.25936 0.014724 -0.23347 0.45535 0.69237 0.43396 0.15043 -0.61974 -0.35272 0.12498 -0.24707 -0.16721 -0.056851 -0.35703 -0.47281 0.15876 0.16506 -0.30952 0.19837 -0.29753 0.24047 0.26341 -0.084337 0.06721 -0.1769 -0.58813 0.019964 0.72689 -0.16092 -0.30881 -0.10589 -0.17214 -0.72022 0.05645 -0.66277 0.56281 0.024343 -0.76181 0.012543 -0.11801 0.11233 0.10025 -0.1839 0.66902 -0.19327 -0.14506 0.66212 -0.35278 -0.0070347\n",
      "\n",
      "vasović 0.054003 -0.2155 -0.31412 0.23979 -0.11407 -0.22719 0.17191 -0.15464 0.41268 -0.29638 0.09218 0.097151 0.18541 -0.33426 0.058095 -0.075948 -0.59358 0.18923 0.039834 -0.1695 -0.52242 -0.16428 -0.33616 -0.17808 -0.34696 0.02566 0.039211 0.26787 -0.1235 0.022 0.0056696 0.023671 0.12636 -0.26291 0.026585 -0.1379 -0.16838 -0.22303 0.017566 0.027435 0.14825 0.16576 0.11776 -0.02478 -0.14182 0.045614 0.038053 0.31859 0.29813 0.29532 0.053888 0.13666 0.015448 -0.2797 0.32594 0.72847 0.10705 0.16374 -0.67757 -0.21937 0.11259 -0.23255 0.20245 -0.13518 -0.14298 0.030899 -0.03027 0.0055044 -0.28217 0.094169 0.2685 0.058845 -0.1067 0.34115 -0.038987 0.14328 0.13524 -0.19658 0.59727 0.17815 -0.33521 -0.046113 0.18225 -0.32563 0.37773 -0.11486 0.037951 0.12862 -0.34497 0.30441 -0.082842 0.25671 -0.021127 -0.47781 0.28242 -0.099892 0.21012 0.39341 -0.59972 -0.15854\n",
      "\n",
      "šaper 0.14417 -0.22519 -0.78202 0.1475 -0.021628 0.05976 -0.34108 0.028155 0.31316 -0.38324 0.40404 -0.082956 -0.091255 -0.05206 -0.19372 -0.044775 -0.51339 0.34661 -0.10858 -0.30169 -0.32 -0.39518 0.033744 -0.28312 -0.42603 0.13804 -0.20031 0.3715 0.065324 -0.11194 0.31831 0.11307 -0.21141 -0.08271 0.36328 -0.1194 0.10968 -0.32931 0.24935 0.22244 -0.073467 0.10157 -0.094349 0.59457 -0.16696 -0.029724 -0.14571 0.028038 0.49354 0.077362 0.17878 0.31862 -0.096307 -0.32933 0.31951 0.83426 0.45291 -0.010763 -1.2413 -0.15699 0.15136 -0.29607 0.13219 -0.10338 -0.21867 -0.26914 -0.051298 -0.22416 0.11164 0.19192 0.11236 0.35471 0.31402 0.37557 0.066146 0.074004 0.099036 -0.10606 0.79586 -0.16785 -0.48426 -0.018968 0.39889 -0.52829 0.25736 -0.11904 0.08574 0.25155 -0.60377 0.33553 -0.23093 -0.42507 -0.11934 -0.091719 0.37384 0.10149 0.10039 0.35012 -0.26671 0.047213\n",
      "\n",
      "longyou 0.29164 -0.12327 -0.064166 0.45369 0.46883 -0.21082 -0.087939 0.14956 0.019793 0.21349 0.1268 -0.029976 -0.3139 -0.025573 -0.10699 0.15255 -0.081048 0.14962 0.088789 -0.18904 -0.13074 0.08315 0.047447 -0.37001 -0.60634 -0.15858 0.32722 0.41877 0.022943 0.20974 -0.14445 -0.205 0.16808 -0.33444 0.30764 -0.2913 -0.27874 -0.43254 0.34989 -0.080833 0.10381 -0.044214 -0.014194 -0.24055 -0.20422 -0.47103 0.35796 0.279 0.084318 0.11689 0.17176 0.13682 -0.073717 -0.66679 0.08484 0.74262 0.13307 -0.089836 -0.53449 -0.12964 0.20616 -0.0052602 0.2392 0.13937 -0.18804 0.026653 -0.63195 -0.46039 -0.4084 0.25761 -0.013736 0.17043 -0.039356 0.40643 0.21622 -0.020799 0.27522 -0.012255 0.80694 0.38469 -0.52649 0.22857 0.21256 0.33473 0.41183 0.61432 -0.031823 0.027816 0.38752 -0.32025 0.095869 0.20376 0.062426 -0.38886 0.29273 0.46178 0.13904 -0.088235 -0.14967 -0.30047\n",
      "\n",
      "lcia 0.52798 -0.00066707 0.17757 -0.027364 -0.43951 -0.12078 -0.29237 0.1811 -0.083255 -0.32708 0.24174 -0.22563 -0.28778 0.185 -0.23391 0.57517 -0.2604 -0.20574 0.34355 -0.14749 -0.52701 -0.36597 0.11192 0.0019546 0.042376 0.25869 0.12114 -0.01811 -0.15287 0.2242 0.074599 -0.38775 -0.3543 -0.27098 0.23921 -0.29326 0.29184 -0.21505 0.11808 0.038863 0.11839 0.13245 0.11149 -0.36866 0.16636 -0.065813 -0.019877 -0.085386 0.084377 0.58116 0.44812 0.2703 -0.14671 -0.45441 0.027497 0.78898 -0.18163 0.31584 -0.15842 -0.31109 0.020312 -0.24831 -0.087688 0.0068462 -0.40054 0.029473 -0.85007 -0.2675 -0.35102 0.20364 0.089756 0.028231 0.1197 0.38333 -0.02234 -0.64164 0.3889 -0.38821 0.21176 -0.096114 -0.3752 -0.25418 0.57119 -0.20015 0.43428 0.40568 -0.12015 0.29205 0.30843 0.56254 -0.012361 -0.17458 -0.16989 -0.31786 0.53023 0.0941 0.2409 0.3452 -0.28433 0.15865\n",
      "\n",
      "vuoi -0.1815 -0.044194 -0.10838 0.08803 -0.49277 -0.17925 0.039785 -0.13891 0.4143 -0.21474 0.085278 0.2497 0.32048 -0.18221 0.19315 0.12776 -0.51092 0.47048 0.029659 -0.15518 0.73695 0.33641 -0.67561 -0.14358 -0.50664 0.72087 0.1214 0.30232 0.25654 -0.22751 -0.36874 -0.6236 0.2001 -0.33723 0.18121 0.18937 0.27187 -0.31046 -0.19963 0.082752 -0.22471 -0.10912 -0.31831 0.43172 0.070813 -0.21711 -0.17695 1.0236 -0.29492 0.07446 -0.13363 0.1334 -0.1927 -0.595 0.30338 1.0583 0.62836 0.40179 0.055116 -0.43423 0.32461 -0.50852 -1.0101 -0.42048 -0.42176 0.68852 -0.13923 -0.267 0.11015 0.27602 0.59255 0.46815 -0.024659 -0.51808 0.1429 0.46004 -0.13376 0.22021 0.74982 -0.21502 -0.36887 -0.094098 -0.12599 -0.20145 0.16997 0.34256 -0.42091 -0.12013 0.14591 0.21231 -0.072858 0.24474 0.47577 -0.45154 0.64902 -0.34802 0.057541 0.0023648 -0.27239 -0.0040136\n",
      "\n",
      "tavčar 0.23915 -0.21888 -0.42727 0.051068 0.033966 -0.18155 0.26908 -0.078669 0.29958 -0.5036 0.17719 0.22982 -0.25474 -0.34986 0.1138 0.1054 -0.57491 0.26944 0.30667 0.15554 -0.42302 0.097088 0.068859 0.034122 -0.036483 -0.060956 -0.19966 0.32106 0.52035 0.22638 0.28522 -0.24932 -0.028529 0.053674 0.13824 -0.3581 0.12969 -0.27107 -0.077404 0.59921 -0.060937 -0.10597 0.22276 0.28885 -0.023051 0.10455 0.082202 0.26415 0.52419 0.35639 -0.16079 0.14967 0.08008 -0.3134 0.41203 0.78926 0.17006 0.41406 -0.66777 0.11844 0.2836 -0.16676 0.12887 -0.017199 -0.3993 0.02126 -0.072179 -0.12307 -0.12775 0.050411 0.16874 0.34717 0.43244 0.28849 0.10587 -0.076772 0.35126 -0.068656 0.586 0.086206 -0.18885 0.14871 -0.13148 -0.43495 0.24897 0.036576 -0.30802 -0.082091 -0.4649 -0.046731 0.23348 0.073716 0.087672 -0.10754 0.33692 0.081486 0.22006 0.13537 -0.76853 -0.2637\n",
      "\n",
      "fardell -0.071754 -0.15909 -0.50823 0.32146 -0.29568 -0.2944 0.30057 -0.35822 0.38732 -0.047929 -0.12684 -0.055966 0.045002 -0.46265 -0.055857 -0.074212 -0.39072 0.31503 0.11522 -0.0088287 -0.35187 -0.45304 -0.12284 -0.29717 -0.27612 -0.01018 0.011082 0.45823 0.10889 0.063327 0.2438 -0.28653 0.15159 0.19797 0.31898 -0.24382 -0.12856 -0.19569 0.14736 0.12845 -0.17726 0.35327 0.08576 0.58881 -0.31502 -0.0037934 -0.12346 -0.083993 0.40721 0.35818 0.21611 0.14286 0.017395 -0.31196 0.39077 0.57174 0.12148 0.062777 -0.54883 -0.19904 0.28959 -0.27737 0.31113 0.050295 -0.49306 0.30045 -0.40166 -0.42296 -0.23353 0.097082 0.18893 0.19393 0.14809 0.51932 0.052466 0.17596 -0.005771 -0.21063 0.41226 0.10342 -0.032749 -0.13918 0.15831 -0.21441 0.86182 -0.14372 -0.29536 -0.20529 -0.28685 0.29683 0.28334 -0.22648 0.19101 -0.0082034 0.12085 -0.074668 0.35868 0.39034 -0.65466 -0.14478\n",
      "\n",
      "nurc 0.1312 0.73353 -0.23846 -0.18042 -0.1142 -0.24371 -0.23973 -0.047679 0.18534 -0.49774 0.46379 -0.014095 0.013813 -0.1327 -0.10399 0.39789 -0.12263 -0.20584 0.21088 0.20825 -0.51216 0.027103 -0.61756 -0.28918 -0.24709 -0.05586 -0.041182 0.73723 0.27168 -0.036423 0.049675 0.10452 -0.19821 0.24367 -0.066709 -0.35322 0.18987 0.31577 0.20685 0.51219 0.22539 0.1049 -0.0080251 -0.056487 0.023417 -0.46813 -0.53525 0.19287 0.0058133 0.1267 0.19016 0.099303 -0.11783 -0.39363 0.63017 0.7034 0.17006 -0.287 -0.22091 -0.38605 -0.044857 -0.35637 0.1623 0.10627 -0.4474 0.32711 -0.92708 -0.17912 0.0054091 0.085302 0.41714 0.36775 0.0043819 0.19983 -0.13094 -0.27111 0.14424 0.14891 0.24272 -0.21688 -1.155 -0.26243 0.64517 -0.092207 0.34177 0.32519 0.49452 0.56168 -0.13026 0.52214 0.040242 0.14363 0.2143 -0.045412 0.63783 0.015111 0.30444 -0.074175 -0.1648 0.044867\n",
      "\n",
      "sauzal -0.15903 -0.25461 -0.128 0.55041 -0.054895 0.10398 0.17771 0.23856 0.3497 0.10003 -0.065161 -0.17485 -0.2773 -0.22472 -0.11443 0.17094 -0.68281 0.10299 0.38716 -0.17789 -0.00075091 0.21118 -0.29862 -0.12371 -0.25028 0.19936 0.048929 0.27793 -0.27375 -0.039603 0.092806 -0.4289 0.2042 0.17095 -0.39206 -0.37865 0.25555 -0.38114 0.47801 0.17062 0.11915 0.61871 -0.15814 0.22805 -0.30425 -0.17119 0.15647 0.47271 -0.073474 0.12865 -0.26042 0.0075127 -0.016707 -0.45366 -0.093581 0.43099 -0.12296 0.071281 -0.21272 -0.42911 -0.13547 -0.3236 0.32165 -0.78479 -0.43619 -0.050281 -0.07478 -0.69122 -0.72057 0.097062 0.33347 0.18126 0.24144 0.097412 -0.15441 0.23714 0.62485 -0.369 0.61862 0.10028 -0.15961 0.40793 -0.052073 -0.15596 0.83906 0.14941 0.094133 0.002513 0.0046531 0.41054 0.15168 0.30048 0.58048 -0.35161 0.33225 0.19625 -0.051844 0.30771 0.017367 0.22444\n",
      "\n",
      "gousmi 0.0085708 -0.56995 -0.3916 0.42052 0.26577 -0.29704 0.19569 -0.42878 0.28135 -0.021138 -0.26884 0.13692 0.16236 -0.49033 0.071329 0.34479 0.037886 -0.089752 0.10755 -0.086456 -0.058372 -0.34791 0.060171 0.1766 -0.39632 0.0090152 -0.27999 0.4566 0.20597 0.11757 0.29428 -0.29527 0.029799 0.27819 0.12331 -0.14344 0.23717 -0.14364 -0.13675 -0.13714 0.16372 0.44507 0.32348 0.35329 0.081452 0.35366 0.14654 0.63048 0.2318 0.70614 0.12037 0.030888 -0.28692 -0.22753 0.42057 0.81305 -0.30886 0.31172 -0.92934 -0.25532 0.43201 -0.36954 0.51283 -0.0046784 -0.44135 0.11551 -0.015297 -0.029272 -0.21869 0.31418 0.029281 0.045881 0.048344 0.6272 -0.13168 -0.38839 0.16411 -0.11074 0.12865 -0.11999 -0.32117 -0.2969 0.030065 -0.34196 0.47419 0.30844 0.37347 0.2959 0.0019657 0.24533 0.83591 0.33953 -0.052758 0.051524 0.21612 0.031666 0.12211 0.12701 -0.47181 -0.22847\n",
      "\n",
      "numbskulls 0.13629 0.10216 -0.24341 -0.011395 -0.3381 -0.049863 -0.15678 -0.32078 -0.19217 -0.15316 -0.067498 0.24157 0.095693 -0.33165 0.12921 0.18727 -0.3812 0.53224 0.26678 -0.03456 -0.21886 0.22697 -0.57167 -0.3993 -0.13769 -0.35815 0.24291 0.30247 -0.085706 -0.040061 0.4872 -0.53052 -0.029073 -0.42028 0.27945 -0.048711 -0.18602 -0.42564 0.22988 0.15267 -0.15856 0.53667 -0.029918 0.13537 -0.010186 -0.0012468 -0.077424 0.33305 -0.0092968 0.26352 -0.035085 -0.070003 -0.29777 -0.47029 0.20563 1.0676 0.14032 0.33885 -0.58385 0.12072 0.13758 -0.0067361 0.29073 -0.34903 -0.16511 0.1964 0.037968 -0.39553 -0.1728 -0.22804 -0.081673 0.10779 -0.082821 0.34801 -0.40658 0.30242 0.021013 -0.077285 0.61939 -0.056265 0.031644 0.13619 0.15965 -0.040588 0.054372 -0.033604 -0.2006 0.0051837 -0.52347 0.10482 -0.1697 0.23848 0.11046 -0.41638 -0.056464 -0.3279 0.10549 0.17333 -0.42844 0.15565\n",
      "\n",
      "visalakshi -0.112 0.067561 -0.56766 0.14787 -0.59989 -0.10204 -0.095224 0.059051 0.13718 0.11633 0.040321 0.41244 0.5069 0.32772 -0.5144 -0.050934 -0.61601 0.21104 0.17121 0.34733 -0.53981 -0.058693 0.069688 0.19933 -0.15244 0.45688 0.1257 0.017669 -0.037093 -0.12155 -0.015399 -0.3217 0.14297 0.12754 0.36461 -0.18747 -0.27211 -0.13464 0.063322 0.12782 -0.0033965 0.34438 0.31278 0.0095789 0.045099 0.035061 -0.098201 0.72258 0.41806 0.059238 0.10223 0.18094 -0.12264 -0.30401 0.49257 0.71779 -0.15776 -0.075117 -0.28923 -0.72061 0.031889 -0.57431 0.058568 -0.30907 -0.82437 0.0023612 -0.59145 0.048841 -0.57225 -0.0028475 0.1246 0.02495 -0.021306 0.41162 -0.0044901 0.15214 -0.19391 -0.019426 0.56588 0.23673 -0.39077 0.25027 0.2819 -0.51079 0.66204 0.096721 0.16711 -0.21125 0.04064 0.11969 0.085109 0.26372 0.36724 -0.42784 0.13838 -0.0049061 -0.018635 0.20073 -0.27416 -0.23371\n",
      "\n",
      "lieserl 0.37279 -0.154 -0.18013 0.40005 -0.22629 -0.36403 0.44806 -0.11548 0.30585 -0.079886 -0.31281 0.40715 -0.11573 -0.10307 0.0055611 0.058639 0.060056 0.29433 0.34752 0.15203 -0.41035 -0.1748 -0.20899 -0.34475 -0.19553 0.054939 -0.27873 0.080349 0.067647 0.1676 0.17201 -0.3851 0.17962 0.068223 0.17222 -0.029263 -0.10386 -0.22971 0.23125 -0.28681 0.12692 0.45718 0.34021 0.12674 -0.013981 0.32693 0.020211 0.50067 0.0051726 0.3781 0.29243 -0.21693 0.0018977 -0.24874 0.51042 0.61452 -0.043616 0.20003 -0.82207 -0.49931 0.048454 -0.33212 -0.10318 -0.27138 -0.23929 0.29511 -0.11887 -0.16404 -0.25328 0.24674 0.21983 0.12753 0.11317 0.068245 0.044075 0.048043 0.11307 -0.39658 0.05167 0.034685 -0.18126 -0.11552 0.40309 -0.11476 0.54134 -0.18075 0.052466 -0.19647 -0.24199 -0.036417 0.20837 0.15331 0.40271 -0.07066 0.29095 -0.14457 -0.010193 0.082802 -0.58852 -0.016435\n",
      "\n",
      "ulundurpet 0.036956 0.10527 0.16964 0.18798 0.024262 -0.0863 -0.019642 0.063283 0.32633 0.19334 -0.025111 0.22606 -0.11523 -0.30742 -0.014269 -0.1161 -0.5155 0.13203 0.60932 -0.39669 -0.39365 0.40205 -0.064883 -0.24949 -0.85196 -0.19275 0.10333 0.48747 -0.24039 -0.023411 0.25458 -0.1186 0.20268 -0.22339 0.27484 0.11682 -0.24388 -0.23261 -0.53122 0.087986 0.30975 0.36606 -0.1112 0.25355 -0.068922 0.048418 0.30839 0.030124 0.11191 0.52079 -0.15096 -0.041904 0.022588 -0.51066 0.17647 0.56044 -0.037168 -0.19295 -0.10155 -0.44109 0.03882 -0.30299 -0.041906 -0.11903 -0.31388 0.29382 -0.16145 -0.36212 -0.34858 0.0097719 0.17041 0.27765 0.23333 0.32023 0.079913 -0.2452 0.56134 -0.2385 0.17514 0.19863 -0.37421 0.035109 0.16473 -0.12385 0.34944 0.15709 0.15908 -0.12549 -0.11107 0.36235 -0.10104 0.46874 -0.18294 -0.12741 0.6286 0.0049949 0.42429 0.25672 0.095462 -0.16287\n",
      "\n",
      "arpita 0.10979 -0.27421 -0.44754 0.17022 -0.14662 -0.20881 -0.14004 0.13893 0.32283 -0.0233 0.011078 0.15743 -0.039366 0.025298 -0.041871 -0.091701 -0.22276 -0.16297 -0.1121 0.28843 -0.52748 0.1297 -0.20015 0.10943 -0.12479 0.16085 0.022974 0.042715 0.042659 0.16477 0.00068464 -0.20913 0.40003 -0.15432 0.042854 -0.071808 -0.050064 -0.30216 0.0020988 -0.19424 0.17109 0.15614 0.27184 0.2637 -0.23319 -0.38661 -0.33695 0.39535 -0.025517 0.30924 0.042733 0.24108 -0.052441 -0.23894 0.38661 0.55698 -0.24369 0.1361 -0.70464 -0.16968 0.01035 -0.14276 -0.068437 -0.37342 -0.16057 0.058205 -0.34176 0.031631 -0.051556 -0.0043193 0.045111 0.14305 0.079941 0.060788 -0.034772 0.24014 0.053066 0.33724 0.42294 0.0051789 -0.45056 -0.31117 0.39964 -0.3648 0.64475 0.097604 -0.022747 0.21064 -0.36551 0.28463 0.056302 0.31139 0.151 -0.082998 0.38012 -0.070696 0.048522 0.055083 -0.64378 -0.22268\n",
      "\n",
      "ineta 0.032967 0.050103 -0.15245 0.44043 -0.068466 -0.24083 0.071651 0.061726 0.44326 -0.31218 0.19108 -0.10845 -0.56464 -0.14068 0.0079561 0.011855 -0.53128 0.1639 -0.062715 0.25148 -0.1496 0.24026 -0.012656 0.33024 0.14696 0.019482 0.16876 0.48335 -0.072213 0.043855 0.072617 -0.2586 -0.11899 -0.42683 -0.019076 -0.42349 0.27305 -0.22816 0.30425 -0.54627 0.28378 -0.26645 0.32956 0.39617 -0.18474 -0.40671 0.083343 0.036909 0.067708 0.31322 -0.010447 -0.094414 -0.37335 -0.41659 0.38815 0.55382 0.23703 0.21042 -0.83976 -0.060063 -0.0090289 -0.48362 0.20063 -0.056136 -0.45584 0.23212 -0.38709 0.010522 0.097592 -0.018227 -0.18939 0.10316 0.069705 0.6006 -0.25257 0.24376 0.021812 0.10994 0.51153 -0.47307 -0.24657 -0.011247 0.09571 -0.19687 0.73565 -0.2702 0.035312 0.4004 0.0092406 0.42267 0.032935 -0.048035 0.18018 -0.27073 0.02686 0.02187 0.034935 0.40789 -0.60571 -0.18245\n",
      "\n",
      "whirlow 0.092527 -0.12334 -0.11662 0.034722 -0.17542 -0.17962 -0.18305 -0.034791 0.33107 -0.26803 -0.35159 0.22696 0.013926 -0.36785 -0.23785 0.080429 -0.0067036 0.13685 0.21662 -0.16072 -0.37848 0.32047 -0.40944 -0.62896 -0.012445 -0.1642 -0.056541 0.095369 -0.18852 0.38203 0.11787 -0.09865 0.29355 0.1437 0.087888 -0.027399 -0.060951 -0.008954 -0.26561 -0.16869 0.10894 -0.026629 -0.087661 0.36285 -0.0035087 0.057667 -0.038171 0.63116 0.16331 0.23365 0.15542 -0.14579 -0.30468 -0.5266 0.11253 0.39533 -0.12585 0.19914 -0.56402 -0.082246 -0.052308 -0.0013925 0.19433 0.1258 -0.38755 -0.032408 -0.16228 -0.40184 -0.47694 -0.044281 -0.19184 0.028705 0.073016 0.31306 -0.041552 0.2775 0.57881 -0.21091 0.66959 0.19096 -0.57033 0.032327 0.31073 -0.38626 0.60633 0.24484 0.41698 -0.28151 -0.031059 0.43214 0.092707 0.17244 0.051503 0.082733 0.48954 -0.43659 0.1313 0.53015 0.14034 0.14779\n",
      "\n",
      "wanz 0.16999 -0.039345 -1.0095 0.25571 -0.38235 -0.45853 0.017753 -0.28369 -0.090597 -0.069137 -0.29828 -0.20298 -0.57658 0.09988 -0.32143 0.36657 0.22993 -0.07068 0.044458 -0.23173 -0.17395 -0.29189 -0.024501 -0.46952 -0.67321 0.325 -0.61559 0.41712 0.28706 0.17226 0.06231 -0.35759 0.062408 -0.22734 0.11783 0.16852 0.17829 -0.38603 -0.073931 0.078589 0.24095 0.16442 -0.31212 0.35747 -0.22755 0.1238 0.13177 0.22142 0.5543 0.37714 -0.14715 -0.055194 -0.43942 -0.36958 0.18764 0.69257 -0.078449 0.29823 -0.50263 -0.47225 0.24766 -0.37516 0.06972 0.14569 -0.057344 0.011298 0.055206 0.017717 -0.57614 0.064494 0.23056 0.2728 0.066162 0.45268 0.37318 0.26936 -0.0039951 0.02435 0.33257 0.26923 -0.29004 0.17338 0.20864 -0.29162 0.30929 -0.029683 -0.37917 -0.43107 -0.14295 0.088137 0.41754 0.54769 0.35251 -0.3736 0.52773 -0.17894 0.28662 0.26913 0.17631 -0.0077713\n",
      "\n",
      "t22 -0.14533 0.34971 0.059825 0.33337 0.21734 -0.10062 0.20656 0.21411 0.14728 -0.12759 0.50772 0.32232 0.00067219 -0.24162 -0.43198 0.20716 -0.71956 0.242 0.062876 -0.018946 0.1089 0.46608 -0.13494 -0.33201 0.058483 -0.024842 0.3801 -0.045863 0.39703 -0.1156 -0.20927 -0.25329 -0.062156 -0.14226 0.23992 -0.37544 -0.48895 -0.61472 -0.24119 0.19438 0.19717 -0.25337 -0.43206 0.47349 0.51456 -0.13829 -0.32668 0.28314 -0.035849 -0.035195 -0.13396 0.16286 0.024417 -0.48103 -0.21946 0.69414 0.20923 0.83033 0.060047 -0.31677 0.30281 -0.12956 0.041977 -0.83334 -0.30829 -0.11067 -0.3971 -0.082879 -0.40505 0.10579 0.095485 0.41304 0.15515 0.27451 -0.38428 -0.072826 0.13904 0.29333 0.5047 -0.38081 0.053586 -0.31695 0.62972 -0.12759 0.2699 0.10295 0.12228 0.090233 0.15048 0.90558 -0.04019 0.036749 0.60321 0.017671 0.10592 0.020689 0.55921 0.090279 -0.18346 -0.11745\n",
      "\n",
      "slann 0.28094 -0.093316 -0.26453 0.095304 -0.2098 -0.10772 -0.021775 -0.30691 0.085582 -0.036737 -0.038006 0.19905 -0.088236 -0.34286 -0.35822 -0.27545 -0.48836 -0.020726 0.071885 -0.16275 -0.30408 0.01068 -0.21413 -0.15051 -0.29283 -0.54579 0.028523 0.0031672 0.2222 -0.090869 0.37888 -0.3649 0.21571 -0.16357 0.14751 -0.30051 -0.046425 -0.45759 0.22314 0.16259 0.045471 0.34112 0.12841 0.47069 0.09365 0.085477 -0.096442 0.40732 0.091688 0.14806 0.05075 -0.29825 -0.056994 -0.52887 0.51232 0.89145 0.14817 0.33094 -0.43669 -0.12164 0.12858 -0.34076 0.4669 -0.089554 -0.024031 0.15527 -0.49437 -0.11817 -0.41952 -0.2686 0.31304 0.2316 -0.11276 0.37865 -0.30128 0.3462 -0.17221 0.13344 0.50227 -0.20848 -0.024844 0.13334 0.4632 -0.096062 0.13268 0.089543 -0.33677 0.029303 0.22631 0.40316 -0.23126 0.0045931 0.13933 -0.044171 -0.16763 -0.23465 0.017115 0.19685 -0.10332 0.11686\n",
      "\n",
      "kunimoto 0.044197 -0.25056 -0.39112 0.07157 0.38909 -0.17348 0.069612 -0.24946 0.23398 -0.081217 0.0065373 -0.0063802 -0.11521 -0.16636 -0.052388 0.091575 -0.30658 0.50147 0.57256 -0.24135 -0.27366 0.094416 -0.081586 0.0099074 -0.18413 -0.13212 -0.28501 0.12311 0.16971 -0.093579 -0.11561 -0.10617 -0.092999 -0.090394 0.064105 -0.17271 0.21017 -0.23261 0.078118 0.20995 0.36896 -0.10428 0.16428 0.16573 -0.015306 0.01167 -0.0073994 0.26115 0.14902 0.28941 0.2519 0.056318 -0.10263 -0.35929 0.15102 0.57096 0.094059 0.22603 -0.66203 -0.32461 -0.0096707 -0.52674 0.43525 0.18721 -0.37761 0.058751 -0.14534 -0.036744 -0.29975 -0.22025 0.17624 0.34604 -0.093054 0.12772 -0.15602 0.11603 -0.15279 -0.040772 0.51726 -0.037535 -0.50369 -0.071048 0.066016 -0.12805 0.50742 -0.10055 -0.021719 0.42993 -0.32405 0.13724 0.05382 -0.055301 0.31614 -0.12249 0.26448 0.22243 0.26342 0.24159 -0.33181 0.07183\n",
      "\n",
      "altares 0.040448 -0.26831 -0.17026 0.13155 0.0031184 -0.090046 0.075502 0.033753 0.30945 -0.4065 0.065776 -0.0063695 0.10136 -0.067617 -0.043898 -0.15439 -0.19848 0.44232 0.022499 0.024071 -0.25625 0.047147 -0.7554 -0.25361 -0.41462 -0.10754 0.33536 -0.013744 -0.029511 0.40737 0.27287 -0.64688 0.083851 0.11089 0.2125 -0.23637 0.05732 -0.64148 0.023872 0.013369 0.0010291 0.24937 -0.19743 0.57335 0.24725 -0.40096 -0.13227 0.40566 -0.077616 0.69494 -0.14386 -0.19926 -0.40906 -0.42009 -0.032141 0.44163 0.22715 0.1825 -0.31749 -0.23536 -0.17101 -0.29757 -0.11567 -0.30011 -0.30751 0.031726 -0.26354 -0.4791 -0.25569 0.1793 0.28316 0.47205 -0.072882 0.45318 -0.096344 0.25422 0.24244 0.1476 0.29891 0.13644 -0.46897 0.049381 0.0362 0.055652 0.62037 0.24833 -0.29252 -0.005216 0.064036 0.50219 -0.041649 -0.0019268 -0.079156 -0.31722 0.57799 -0.042803 0.0687 0.23833 -0.75322 0.065772\n",
      "\n",
      "cipolletti 0.32256 -0.42925 -0.4239 0.58468 0.11893 0.028771 -0.18244 -0.11611 0.17085 0.039919 -0.10375 -0.16651 -0.043589 -0.10068 0.031676 -0.10218 -0.094052 0.070086 0.33921 -0.0076938 0.090463 0.26168 -0.075447 0.095631 -0.25008 0.038865 -0.04244 0.10837 -0.070896 -0.042916 -0.1643 -0.067949 0.24549 -0.048153 0.27167 -0.23709 0.098463 -0.18407 0.00080209 -0.13767 0.30847 0.022919 0.096475 0.10705 -0.28185 -0.013318 0.26934 0.031668 -0.05698 0.20648 0.15079 0.018998 -0.31563 -0.32173 -0.093014 0.75053 -0.2212 -0.0392 -0.77056 -0.19972 0.0096661 -0.24571 0.29856 -0.48925 -0.45671 -0.17426 -0.057636 -0.042566 -0.023598 0.019715 0.36406 0.49482 0.17662 0.34303 -0.0016153 0.14769 0.17223 -0.18336 0.4052 0.36737 -0.31295 0.10559 0.45194 -0.074243 0.60338 0.17676 -0.24686 -0.14147 -0.067262 0.7609 0.24938 0.022247 0.13455 -0.15849 0.38075 0.10951 0.21385 0.39433 -0.61916 0.1418\n",
      "\n",
      "sherwoods 0.29817 0.050683 -0.44973 0.022209 -0.17598 -0.10999 -0.17566 -0.071694 0.25714 -0.10303 0.072914 0.23938 0.19964 -0.21694 -0.39905 -0.14814 -0.25065 -0.0096439 -0.054094 -0.23711 -0.36822 -0.12432 -0.34374 -0.12725 -0.34881 -0.35476 -0.0050393 0.14932 0.0041334 -0.14938 0.42054 -0.061924 -0.042449 0.025873 -0.11448 -0.1276 0.15322 -0.17977 0.14253 0.078438 0.19796 0.29666 -0.20785 0.15597 0.084595 0.072365 -0.080986 0.50877 0.21441 0.30983 -0.19712 0.080355 -0.26687 -0.36168 0.17327 1.1079 0.13211 -0.00034555 -0.50604 -0.15112 -0.15493 -0.04196 0.12206 -0.28824 -0.10991 0.07312 -0.60429 -0.16364 -0.3633 -0.26724 0.071494 0.26556 0.042125 0.077213 -0.41077 0.14302 -0.2122 -0.17713 0.68566 0.012022 -0.15797 0.24942 0.18335 -0.31949 -0.043228 -0.075097 0.1906 0.031428 -0.10931 0.42679 -0.19198 0.13965 0.35369 -0.1308 0.10919 -0.45716 -0.12979 0.34426 -0.37951 0.19643\n",
      "\n",
      "sullana 0.14079 0.024554 0.0033749 0.16056 0.22334 0.12379 -0.14835 -0.017199 0.56483 0.42264 -0.21668 0.22938 0.10664 0.033516 -0.45973 -0.099689 -0.18621 -0.17796 0.48809 0.028066 -0.087963 -0.02771 -0.42261 -0.055998 -0.72153 0.050765 0.091119 0.32958 -0.18087 -0.084805 -0.173 -0.36581 0.16058 -0.12138 0.079026 -0.32194 -0.12388 -0.46568 -0.17945 -0.25586 0.14464 0.22621 0.27209 -0.56918 -0.22576 -0.042649 0.32272 0.10805 0.081055 0.24269 -0.030366 -0.051691 -0.31506 -0.35167 0.041663 0.83029 -0.30679 -0.18857 -0.46825 -0.34247 -0.050811 -0.076326 0.15678 0.057734 -0.2596 0.60592 -0.53677 -0.54545 -0.47865 0.064431 0.46666 -0.15182 0.1024 0.51227 0.023556 0.03608 -0.099971 -0.14052 0.44909 0.17448 -0.61281 0.15123 -0.0026391 -0.44766 0.46349 0.091276 -0.30127 0.21298 -0.05676 0.3144 0.44037 0.15967 -0.16223 -0.14385 0.1511 0.22484 -0.19778 0.207 -0.16529 -0.17769\n",
      "\n",
      "29km 0.088889 0.073361 -0.031134 0.7889 -0.25983 0.052001 0.23314 0.16684 -0.35771 0.061529 0.1996 0.81819 0.13432 -0.68898 0.24027 -0.035352 -0.41114 -0.2623 0.17155 -0.1922 -0.030624 0.30302 -0.46018 -0.59319 -0.13014 -0.049985 -0.17607 0.42472 0.11893 -0.024909 0.11117 -0.36993 0.16324 -0.52113 0.023219 0.028279 -0.19989 -0.54559 -0.32442 0.26782 0.31052 0.12912 -0.05597 0.65065 -0.34405 -0.20325 0.25841 0.022971 -0.10616 -0.16883 0.67064 -0.14181 -0.38317 -0.5031 0.26497 0.46379 -0.23218 -0.091124 -0.24058 -0.3451 0.032677 0.085954 -0.062111 -0.60422 -0.031266 -0.43523 -0.28772 -0.13801 -0.28683 -0.2233 -0.17418 0.17677 -0.30127 0.31046 -0.25228 -0.030488 0.0037518 -0.41117 0.73455 -0.21582 -0.59436 0.051313 0.18858 -0.3328 -0.21438 0.21513 -0.05163 0.21517 0.4065 0.14422 -0.34981 0.65943 0.061646 -0.48783 0.30774 -0.38138 0.47435 0.29641 -0.28225 0.029138\n",
      "\n",
      "sigarms 0.18917 -0.3181 -0.43749 0.61209 -0.031357 0.1568 0.30505 -0.39915 0.30101 -0.17353 -0.032278 -0.29338 -0.16598 -0.15038 -0.29364 -0.062909 -0.32706 0.13117 0.010177 0.078467 -0.32796 0.008198 -0.34072 0.1286 -0.27844 0.41588 -0.045799 0.45221 -0.065252 -0.055483 0.50665 -0.067356 -0.29428 -0.12657 -0.024669 -0.37284 0.23507 -0.48212 0.041255 -0.081911 0.59964 0.2047 -0.23401 -0.34091 -0.13306 0.36 -0.00697 0.6013 0.50416 0.099127 -0.031218 0.40532 -0.16901 -0.4664 0.14094 0.74539 0.15076 0.18017 -0.51756 -0.59651 -0.20991 -0.5506 -0.023214 -0.4166 -0.33079 -0.048234 -0.16065 0.36674 0.0072976 0.051298 -0.38274 0.57222 -0.34032 0.16816 -0.40298 -0.12421 0.16196 -0.15993 0.43126 -0.4145 -0.48541 -0.28735 0.34193 -0.029869 0.70822 0.01381 -0.034878 -0.26084 0.20595 0.64408 -0.056202 -0.1277 0.28507 -0.16824 -0.018927 -0.24652 -0.10403 0.32856 -0.43073 -0.10308\n",
      "\n",
      "katuna -0.21887 -0.30785 -0.28557 0.36186 -0.20892 -0.011439 -0.39597 0.27104 0.51042 -0.29147 -0.18927 0.167 -0.043189 -0.0076813 0.079556 0.28642 -0.12039 -0.79228 0.403 -0.37501 0.28638 0.43372 -0.23248 -0.28213 -0.17521 0.011767 -0.0056339 -0.056547 0.20587 -0.11068 -0.075307 -0.11201 0.81534 0.056497 0.5012 -0.44869 -0.28734 -0.67028 0.086008 -0.0032898 0.25806 0.67041 -0.18061 0.21005 -0.22464 0.28244 -0.12104 0.36402 0.10356 0.29394 0.19594 0.31164 -0.24877 -0.20947 -0.27186 0.66153 -0.1452 0.10883 -0.28422 -0.39554 -0.11977 -0.12497 0.15946 -0.36298 -0.40915 0.086626 -0.31137 -0.39377 -0.48603 0.14716 0.39346 0.0041725 0.2165 0.33835 -0.16381 -0.069797 0.37364 0.29598 0.35004 0.29113 0.012292 0.18861 0.45024 -0.26104 0.40511 0.47224 0.33026 0.23671 -0.17786 0.1739 -0.17542 0.41623 -0.04723 0.23439 0.22071 -0.079093 0.49752 0.4401 -0.25509 0.44544\n",
      "\n",
      "aqm 0.37372 0.21216 -0.61747 0.48158 -0.022374 -0.21698 -0.14605 0.17147 -0.29506 -0.49333 0.51952 0.18384 -0.018236 -0.026255 -0.15424 0.55934 -0.032471 -0.18166 0.45563 -0.30715 -0.58712 0.15795 -0.1111 -0.28791 -0.82979 0.20936 -0.37071 0.30697 -0.11828 0.26039 0.24629 0.31701 -0.29463 -0.039123 -0.044956 -0.19887 -0.31857 -0.065253 -0.48279 0.050349 0.019571 0.049414 -0.16211 0.13625 -0.34824 0.25767 -0.55942 0.21233 -0.018795 0.72442 0.33323 -0.25728 -0.27043 -0.13375 0.76019 1.0646 -0.11868 0.32608 -0.24438 -0.72694 0.091176 -0.40616 0.28378 0.085243 -0.67119 0.23653 -0.31338 -0.56921 -0.022608 -0.034588 -0.016904 0.2896 -0.01704 0.13038 -0.22882 -0.53274 0.18616 0.43042 0.65397 -0.51894 -0.095175 -0.41508 -0.23548 -0.61867 0.8877 0.5468 -0.36682 0.076571 0.18696 0.01932 0.46644 0.62652 0.041762 0.1549 0.20668 0.0035888 0.056255 0.035766 -0.34405 -0.32486\n",
      "\n",
      "1.3775 0.45771 0.1271 -0.23676 0.058859 0.17576 0.22606 -0.13308 -0.19668 0.016876 0.19004 0.055791 0.34109 -0.31568 -0.0941 0.32461 -0.13585 -0.13675 -0.42524 0.040012 0.17911 -0.062705 0.032896 -0.54879 -0.37689 -0.0025686 -0.036752 -0.1297 0.50618 0.21003 -0.40258 0.45895 -0.1308 0.31515 -0.031106 0.03072 -0.18423 -0.54498 -0.59592 -0.36091 0.1945 0.39025 0.10958 -0.2726 0.48004 -0.27752 -0.11313 -0.13584 0.42531 -0.24104 0.17879 0.4797 -0.14747 -0.36208 -0.41269 0.16858 0.28535 -0.38502 -0.050096 -0.096347 -0.6829 0.12246 0.31126 -0.10906 -0.079203 -0.85002 -0.028526 -0.38133 -0.31596 -0.62216 -0.43796 -0.12496 0.050087 -0.22498 0.38359 -0.028541 0.053482 0.14193 -0.16055 0.57861 0.47895 -0.016293 0.15554 0.36762 -0.45868 0.22228 -0.065948 0.13141 0.30238 -0.29245 0.28341 -0.63989 0.10335 0.096162 -0.10495 0.1979 -0.058495 0.25698 0.71917 -0.13331 -0.1474\n",
      "\n",
      "corythosaurus 0.083099 -0.25751 -0.49652 0.16322 -0.22434 -0.31205 0.055092 -0.22147 0.29359 -0.12412 -0.28137 0.27464 -0.069968 -0.15021 -0.22195 0.091413 -0.10794 0.32411 0.10797 0.24409 0.19349 -0.052584 -0.58191 -0.34929 -0.4247 0.56021 0.066951 0.44487 -0.017402 0.33296 0.16782 -0.60781 0.058356 0.044058 0.15465 0.18498 0.2521 -0.62418 -0.039028 -0.18849 0.12551 0.32745 -0.14099 0.31894 -0.095142 -0.07607 -0.17728 0.37005 -0.11972 0.32802 -0.074114 0.15685 -0.22622 -0.36427 0.23268 0.66983 0.4231 0.39486 -0.52441 -0.0040785 0.05352 -0.36026 -0.50696 -0.50728 0.10685 0.20806 -0.28306 -0.25915 -0.011765 -0.18617 0.19399 0.20752 0.14626 0.30932 -0.064216 0.25371 0.019049 0.43659 0.44512 0.070055 -0.22787 -0.15639 0.35225 -0.097078 0.37719 -0.023959 -0.2724 -0.2691 -0.11842 0.12733 -0.25782 0.079936 0.28886 -0.26477 0.51567 0.037181 -0.015981 -0.049755 -0.39768 0.0038855\n",
      "\n",
      "chanty -0.15577 -0.049188 -0.064377 0.2236 -0.20146 -0.038963 0.12971 -0.29451 0.0035897 -0.098377 -0.30939 0.050878 0.24574 -0.25382 -0.048145 0.15506 -0.39446 0.086549 0.22096 0.010005 -0.029974 -0.16488 -0.51622 -0.00016491 -0.11421 0.26008 -0.19419 0.36296 0.0099712 0.1731 -0.1477 -0.78212 0.19243 -0.14533 0.41308 0.0048941 -0.33375 -0.20914 0.26039 0.10949 0.49339 0.089623 -0.020955 0.15683 0.3137 -0.11759 -0.31317 0.69917 -0.13166 0.64363 -0.23016 0.20046 0.14912 -0.075741 -0.0029152 0.52797 0.18252 0.091756 -0.34982 -0.082495 0.0635 -0.30766 -0.13771 -0.55364 -3.6811e-05 0.11055 -0.42719 -0.21429 0.12669 -0.08624 0.23952 -0.037189 0.273 0.26172 -0.44486 0.10141 0.23421 0.0083713 0.71138 0.31115 -0.39297 -0.26296 0.40601 0.20615 0.20524 -0.11601 0.0101 0.15099 0.13692 0.18864 0.093324 0.094486 -0.023469 -0.48099 0.62332 0.024318 -0.27587 0.075044 -0.5638 0.14501\n",
      "\n",
      "kronik -0.094426 0.14725 -0.15739 0.071966 -0.29845 0.039432 0.02187 0.0080409 -0.18682 -0.31101 0.043422 0.16147 -0.012647 0.050696 -0.050954 0.013533 -0.20035 0.3019 -0.010799 -0.19664 -0.26712 -0.38311 -0.08666 -0.10954 0.0042728 -0.15433 0.15416 0.22333 0.13355 0.076866 0.045246 -0.00021332 0.0067774 0.047134 0.32453 -0.31853 -0.35445 -0.21979 -0.12723 0.10492 0.26715 0.058452 0.16751 0.31884 0.046914 -0.16315 -0.078414 0.50551 0.32689 0.046858 0.041268 0.49351 -0.44075 -0.15669 0.62512 0.59474 0.084773 0.017492 -0.89279 -0.28656 0.39685 -0.35591 -0.15007 -0.12261 -0.2569 0.08311 0.013643 0.16162 -0.006617 0.015909 -0.16797 0.11139 0.09692 -0.006589 -0.26508 -0.11282 0.034702 -0.022573 0.44549 0.30833 -0.20067 -0.033317 0.10966 -0.18257 0.54201 -0.11415 -0.19819 0.28277 -0.34232 0.3163 -0.30545 -0.011082 0.11855 -0.11312 0.33951 -0.22449 0.25743 0.63143 -0.2009 -0.10542\n",
      "\n",
      "rolonda 0.36088 -0.16919 -0.32704 0.098332 -0.4297 -0.18874 0.45556 0.28529 0.3034 -0.36683 -0.13923 0.10053 -0.52026 -0.30629 -0.18236 0.23908 -0.45987 0.35561 0.067856 0.069954 -0.044425 -0.19452 -0.30248 -0.31011 0.43554 0.24623 0.05153 0.31476 0.094553 0.32482 0.38447 -0.099659 0.414 0.25902 0.08238 0.096832 0.22163 -0.47655 0.13628 0.12927 0.26019 0.45182 -0.079522 0.72982 -0.56586 -0.18653 -0.6082 0.16635 -0.52492 0.14167 -0.26089 -0.046457 -0.060964 -0.48269 0.32158 0.7501 0.52608 0.2913 -0.46036 -0.39314 0.17445 -0.25116 -0.2416 -0.33391 0.086837 0.1027 0.074325 -0.29071 0.3768 0.0079988 0.42131 -0.30349 0.11643 0.38284 0.030575 0.11889 0.42949 -0.0054543 0.83973 0.16628 0.087226 -0.2906 0.16843 -0.19309 0.35477 0.24789 0.14577 0.31387 -0.084938 0.21647 -0.044082 0.14003 0.30007 -0.12731 -0.14304 -0.069396 0.2816 0.27139 -0.29188 0.16109\n",
      "\n",
      "zsombor -0.10461 -0.5047 -0.49331 0.13516 -0.36371 -0.4475 0.18429 -0.05651 0.40474 -0.72583 0.31079 -0.31763 -0.019824 -0.29765 0.16847 -0.029003 -0.42048 0.039778 0.10003 0.14749 -0.24683 0.040093 -0.0938 0.32488 -0.22667 -0.039094 0.21616 0.4959 0.069714 0.16686 -0.026112 0.096436 0.18843 -0.3967 0.082282 -0.38073 0.086211 -0.40775 -0.1726 -0.29836 0.29015 -0.0774 0.017294 0.32361 -0.22261 -0.72733 -0.070333 0.17454 0.021926 0.37076 0.37268 0.037672 -0.29863 -0.9022 0.28775 0.60194 0.028256 -0.15408 -0.39262 -0.22826 0.10673 -0.3631 0.35778 0.034102 -0.29885 0.42406 -0.57664 -0.40484 -0.15435 0.23217 -0.014499 0.2932 -0.030599 0.62079 -0.02442 -0.22534 0.13813 -0.21491 0.61883 0.047665 -0.2661 -0.35747 0.32165 -0.53815 0.63114 0.10025 0.22458 0.28004 -0.048782 0.72537 0.15153 -0.10842 0.34064 -0.40916 -0.081263 0.095315 0.15018 0.42527 -0.5125 -0.17054\n",
      "\n",
      "sandberger 0.28365 -0.6263 -0.44351 0.2177 -0.087421 -0.17062 0.29266 -0.024899 0.26414 -0.17023 0.25817 0.097484 -0.33103 -0.43859 0.0095799 0.095624 -0.17777 0.38886 0.27151 0.14742 -0.43973 -0.26588 -0.024271 0.27186 -0.36761 -0.24827 -0.20815 0.22128 -0.044409 0.021373 0.24594 0.26143 0.29303 0.13281 0.082232 -0.12869 0.1622 -0.22567 -0.060348 0.28703 0.11381 0.34839 0.3419 0.36996 -0.13592 0.0062694 0.080317 0.0036251 0.43093 0.01882 0.31008 0.16722 0.074112 -0.37745 0.47363 0.41284 0.24471 0.075965 -0.51725 -0.49481 0.526 -0.074645 0.41434 -0.1956 -0.16544 -0.045649 -0.40153 -0.13136 -0.4672 0.18825 0.2612 0.16854 0.22615 0.62992 -0.1288 0.055841 0.01928 0.024572 0.46875 0.2582 -0.31672 0.048591 0.3277 -0.50141 0.30855 0.11997 -0.25768 -0.039867 -0.059672 0.5525 0.13885 -0.22862 0.071792 -0.43208 0.5398 -0.085806 0.032651 0.43678 -0.82607 -0.15701\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "word_vectors = []\n",
    "\n",
    "with open('glove.6B.100d.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        print(line)\n",
    "        line = line.split(\" \") # ex) the -0.038194 -0.24487 0.72812 -0.39961 .....\n",
    "        word = line[0] # ex) the\n",
    "        vec = np.array([float(x) for x in line[1:]]) # ex) -0.038194 -0.24487 0.72812 .....\n",
    "\n",
    "        word_to_index[word] = len(word_to_index) # 딕셔너리에 할당, ex) {'the': 0}\n",
    "        word_vectors.append(vec) # ex) [-0.038194 -0.24487 0.72812 ..... ]\n",
    "        \n",
    "#         print(word_to_index)\n",
    "#         print(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f69bf6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.90715   0.064843  1.1566   -0.3465    0.14296  -0.27646  -0.017816\n",
      " -0.12121  -1.3285   -0.09812   0.87256  -0.16852   0.047454 -0.33343\n",
      " -0.27088   0.28921   0.33383  -0.05773   0.18968   0.53989   0.68687\n",
      "  0.56052   0.52556   0.53909   0.34341  -0.61371  -0.26548  -0.42634\n",
      "  0.49248  -0.54542  -0.29126  -0.092725  0.046827  0.84663  -0.48321\n",
      "  0.17015  -0.6567    0.23944  -0.55302   0.091274  0.25736  -0.27976\n",
      " -0.013608  0.065933  0.3542   -0.75957   0.51267  -1.1732    0.15933\n",
      " -1.6229    0.56459  -0.26746   0.45518   1.0818    0.47754  -2.7297\n",
      " -0.072848  0.25136   1.4372    1.0144   -0.36866  -0.44527  -0.35746\n",
      "  0.081572 -0.043148 -0.17133   0.57359   0.48109   0.64189  -0.35552\n",
      " -0.03176   0.8223   -0.44467   0.47329   0.028232  0.057876 -0.42354\n",
      " -0.71167  -0.60101  -0.12724   0.70369   0.5142   -0.058689  0.08078\n",
      " -0.7596   -0.31802   0.21919  -0.41005  -0.092171  0.80333  -0.37917\n",
      "  0.77842  -0.3825   -0.36406  -0.38988  -0.29918  -0.75252  -0.11745\n",
      "  0.61162  -0.30405 ]\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(word):\n",
    "    return word_vectors[word_to_index[word]]\n",
    "\n",
    "print(get_embedding('record'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9ec4321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 0,\n",
       " ',': 1,\n",
       " '.': 2,\n",
       " 'of': 3,\n",
       " 'to': 4,\n",
       " 'and': 5,\n",
       " 'in': 6,\n",
       " 'a': 7,\n",
       " '\"': 8,\n",
       " \"'s\": 9,\n",
       " 'for': 10,\n",
       " '-': 11,\n",
       " 'that': 12,\n",
       " 'on': 13,\n",
       " 'is': 14,\n",
       " 'was': 15,\n",
       " 'said': 16,\n",
       " 'with': 17,\n",
       " 'he': 18,\n",
       " 'as': 19,\n",
       " 'it': 20,\n",
       " 'by': 21,\n",
       " 'at': 22,\n",
       " '(': 23,\n",
       " ')': 24,\n",
       " 'from': 25,\n",
       " 'his': 26,\n",
       " \"''\": 27,\n",
       " '``': 28,\n",
       " 'an': 29,\n",
       " 'be': 30,\n",
       " 'has': 31,\n",
       " 'are': 32,\n",
       " 'have': 33,\n",
       " 'but': 34,\n",
       " 'were': 35,\n",
       " 'not': 36,\n",
       " 'this': 37,\n",
       " 'who': 38,\n",
       " 'they': 39,\n",
       " 'had': 40,\n",
       " 'i': 41,\n",
       " 'which': 42,\n",
       " 'will': 43,\n",
       " 'their': 44,\n",
       " ':': 45,\n",
       " 'or': 46,\n",
       " 'its': 47,\n",
       " 'one': 48,\n",
       " 'after': 49,\n",
       " 'new': 50,\n",
       " 'been': 51,\n",
       " 'also': 52,\n",
       " 'we': 53,\n",
       " 'would': 54,\n",
       " 'two': 55,\n",
       " 'more': 56,\n",
       " \"'\": 57,\n",
       " 'first': 58,\n",
       " 'about': 59,\n",
       " 'up': 60,\n",
       " 'when': 61,\n",
       " 'year': 62,\n",
       " 'there': 63,\n",
       " 'all': 64,\n",
       " '--': 65,\n",
       " 'out': 66,\n",
       " 'she': 67,\n",
       " 'other': 68,\n",
       " 'people': 69,\n",
       " \"n't\": 70,\n",
       " 'her': 71,\n",
       " 'percent': 72,\n",
       " 'than': 73,\n",
       " 'over': 74,\n",
       " 'into': 75,\n",
       " 'last': 76,\n",
       " 'some': 77,\n",
       " 'government': 78,\n",
       " 'time': 79,\n",
       " '$': 80,\n",
       " 'you': 81,\n",
       " 'years': 82,\n",
       " 'if': 83,\n",
       " 'no': 84,\n",
       " 'world': 85,\n",
       " 'can': 86,\n",
       " 'three': 87,\n",
       " 'do': 88,\n",
       " ';': 89,\n",
       " 'president': 90,\n",
       " 'only': 91,\n",
       " 'state': 92,\n",
       " 'million': 93,\n",
       " 'could': 94,\n",
       " 'us': 95,\n",
       " 'most': 96,\n",
       " '_': 97,\n",
       " 'against': 98,\n",
       " 'u.s.': 99,\n",
       " 'so': 100,\n",
       " 'them': 101,\n",
       " 'what': 102,\n",
       " 'him': 103,\n",
       " 'united': 104,\n",
       " 'during': 105,\n",
       " 'before': 106,\n",
       " 'may': 107,\n",
       " 'since': 108,\n",
       " 'many': 109,\n",
       " 'while': 110,\n",
       " 'where': 111,\n",
       " 'states': 112,\n",
       " 'because': 113,\n",
       " 'now': 114,\n",
       " 'city': 115,\n",
       " 'made': 116,\n",
       " 'like': 117,\n",
       " 'between': 118,\n",
       " 'did': 119,\n",
       " 'just': 120,\n",
       " 'national': 121,\n",
       " 'day': 122,\n",
       " 'country': 123,\n",
       " 'under': 124,\n",
       " 'such': 125,\n",
       " 'second': 126,\n",
       " 'then': 127,\n",
       " 'company': 128,\n",
       " 'group': 129,\n",
       " 'any': 130,\n",
       " 'through': 131,\n",
       " 'china': 132,\n",
       " 'four': 133,\n",
       " 'being': 134,\n",
       " 'down': 135,\n",
       " 'war': 136,\n",
       " 'back': 137,\n",
       " 'off': 138,\n",
       " 'south': 139,\n",
       " 'american': 140,\n",
       " 'minister': 141,\n",
       " 'police': 142,\n",
       " 'well': 143,\n",
       " 'including': 144,\n",
       " 'team': 145,\n",
       " 'international': 146,\n",
       " 'week': 147,\n",
       " 'officials': 148,\n",
       " 'still': 149,\n",
       " 'both': 150,\n",
       " 'even': 151,\n",
       " 'high': 152,\n",
       " 'part': 153,\n",
       " 'told': 154,\n",
       " 'those': 155,\n",
       " 'end': 156,\n",
       " 'former': 157,\n",
       " 'these': 158,\n",
       " 'make': 159,\n",
       " 'billion': 160,\n",
       " 'work': 161,\n",
       " 'our': 162,\n",
       " 'home': 163,\n",
       " 'school': 164,\n",
       " 'party': 165,\n",
       " 'house': 166,\n",
       " 'old': 167,\n",
       " 'later': 168,\n",
       " 'get': 169,\n",
       " 'another': 170,\n",
       " 'tuesday': 171,\n",
       " 'news': 172,\n",
       " 'long': 173,\n",
       " 'five': 174,\n",
       " 'called': 175,\n",
       " '1': 176,\n",
       " 'wednesday': 177,\n",
       " 'military': 178,\n",
       " 'way': 179,\n",
       " 'used': 180,\n",
       " 'much': 181,\n",
       " 'next': 182,\n",
       " 'monday': 183,\n",
       " 'thursday': 184,\n",
       " 'friday': 185,\n",
       " 'game': 186,\n",
       " 'here': 187,\n",
       " '?': 188,\n",
       " 'should': 189,\n",
       " 'take': 190,\n",
       " 'very': 191,\n",
       " 'my': 192,\n",
       " 'north': 193,\n",
       " 'security': 194,\n",
       " 'season': 195,\n",
       " 'york': 196,\n",
       " 'how': 197,\n",
       " 'public': 198,\n",
       " 'early': 199,\n",
       " 'according': 200,\n",
       " 'several': 201,\n",
       " 'court': 202,\n",
       " 'say': 203,\n",
       " 'around': 204,\n",
       " 'foreign': 205,\n",
       " '10': 206,\n",
       " 'until': 207,\n",
       " 'set': 208,\n",
       " 'political': 209,\n",
       " 'says': 210,\n",
       " 'market': 211,\n",
       " 'however': 212,\n",
       " 'family': 213,\n",
       " 'life': 214,\n",
       " 'same': 215,\n",
       " 'general': 216,\n",
       " '–': 217,\n",
       " 'left': 218,\n",
       " 'good': 219,\n",
       " 'top': 220,\n",
       " 'university': 221,\n",
       " 'going': 222,\n",
       " 'number': 223,\n",
       " 'major': 224,\n",
       " 'known': 225,\n",
       " 'points': 226,\n",
       " 'won': 227,\n",
       " 'six': 228,\n",
       " 'month': 229,\n",
       " 'dollars': 230,\n",
       " 'bank': 231,\n",
       " '2': 232,\n",
       " 'iraq': 233,\n",
       " 'use': 234,\n",
       " 'members': 235,\n",
       " 'each': 236,\n",
       " 'area': 237,\n",
       " 'found': 238,\n",
       " 'official': 239,\n",
       " 'sunday': 240,\n",
       " 'place': 241,\n",
       " 'go': 242,\n",
       " 'based': 243,\n",
       " 'among': 244,\n",
       " 'third': 245,\n",
       " 'times': 246,\n",
       " 'took': 247,\n",
       " 'right': 248,\n",
       " 'days': 249,\n",
       " 'local': 250,\n",
       " 'economic': 251,\n",
       " 'countries': 252,\n",
       " 'see': 253,\n",
       " 'best': 254,\n",
       " 'report': 255,\n",
       " 'killed': 256,\n",
       " 'held': 257,\n",
       " 'business': 258,\n",
       " 'west': 259,\n",
       " 'does': 260,\n",
       " 'own': 261,\n",
       " '%': 262,\n",
       " 'came': 263,\n",
       " 'law': 264,\n",
       " 'months': 265,\n",
       " 'women': 266,\n",
       " \"'re\": 267,\n",
       " 'power': 268,\n",
       " 'think': 269,\n",
       " 'service': 270,\n",
       " 'children': 271,\n",
       " 'bush': 272,\n",
       " 'show': 273,\n",
       " '/': 274,\n",
       " 'help': 275,\n",
       " 'chief': 276,\n",
       " 'saturday': 277,\n",
       " 'system': 278,\n",
       " 'john': 279,\n",
       " 'support': 280,\n",
       " 'series': 281,\n",
       " 'play': 282,\n",
       " 'office': 283,\n",
       " 'following': 284,\n",
       " 'me': 285,\n",
       " 'meeting': 286,\n",
       " 'expected': 287,\n",
       " 'late': 288,\n",
       " 'washington': 289,\n",
       " 'games': 290,\n",
       " 'european': 291,\n",
       " 'league': 292,\n",
       " 'reported': 293,\n",
       " 'final': 294,\n",
       " 'added': 295,\n",
       " 'without': 296,\n",
       " 'british': 297,\n",
       " 'white': 298,\n",
       " 'history': 299,\n",
       " 'man': 300,\n",
       " 'men': 301,\n",
       " 'became': 302,\n",
       " 'want': 303,\n",
       " 'march': 304,\n",
       " 'case': 305,\n",
       " 'few': 306,\n",
       " 'run': 307,\n",
       " 'money': 308,\n",
       " 'began': 309,\n",
       " 'open': 310,\n",
       " 'name': 311,\n",
       " 'trade': 312,\n",
       " 'center': 313,\n",
       " '3': 314,\n",
       " 'israel': 315,\n",
       " 'oil': 316,\n",
       " 'too': 317,\n",
       " 'al': 318,\n",
       " 'film': 319,\n",
       " 'win': 320,\n",
       " 'led': 321,\n",
       " 'east': 322,\n",
       " 'central': 323,\n",
       " '20': 324,\n",
       " 'air': 325,\n",
       " 'come': 326,\n",
       " 'chinese': 327,\n",
       " 'town': 328,\n",
       " 'leader': 329,\n",
       " 'army': 330,\n",
       " 'line': 331,\n",
       " 'never': 332,\n",
       " 'little': 333,\n",
       " 'played': 334,\n",
       " 'prime': 335,\n",
       " 'death': 336,\n",
       " 'companies': 337,\n",
       " 'least': 338,\n",
       " 'put': 339,\n",
       " 'forces': 340,\n",
       " 'past': 341,\n",
       " 'de': 342,\n",
       " 'half': 343,\n",
       " 'june': 344,\n",
       " 'saying': 345,\n",
       " 'know': 346,\n",
       " 'federal': 347,\n",
       " 'french': 348,\n",
       " 'peace': 349,\n",
       " 'earlier': 350,\n",
       " 'capital': 351,\n",
       " 'force': 352,\n",
       " 'great': 353,\n",
       " 'union': 354,\n",
       " 'near': 355,\n",
       " 'released': 356,\n",
       " 'small': 357,\n",
       " 'department': 358,\n",
       " 'every': 359,\n",
       " 'health': 360,\n",
       " 'japan': 361,\n",
       " 'head': 362,\n",
       " 'ago': 363,\n",
       " 'night': 364,\n",
       " 'big': 365,\n",
       " 'cup': 366,\n",
       " 'election': 367,\n",
       " 'region': 368,\n",
       " 'director': 369,\n",
       " 'talks': 370,\n",
       " 'program': 371,\n",
       " 'far': 372,\n",
       " 'today': 373,\n",
       " 'statement': 374,\n",
       " 'july': 375,\n",
       " 'although': 376,\n",
       " 'district': 377,\n",
       " 'again': 378,\n",
       " 'born': 379,\n",
       " 'development': 380,\n",
       " 'leaders': 381,\n",
       " 'council': 382,\n",
       " 'close': 383,\n",
       " 'record': 384,\n",
       " 'along': 385,\n",
       " 'county': 386,\n",
       " 'france': 387,\n",
       " 'went': 388,\n",
       " 'point': 389,\n",
       " 'must': 390,\n",
       " 'spokesman': 391,\n",
       " 'your': 392,\n",
       " 'member': 393,\n",
       " 'plan': 394,\n",
       " 'financial': 395,\n",
       " 'april': 396,\n",
       " 'recent': 397,\n",
       " 'campaign': 398,\n",
       " 'become': 399,\n",
       " 'troops': 400,\n",
       " 'whether': 401,\n",
       " 'lost': 402,\n",
       " 'music': 403,\n",
       " '15': 404,\n",
       " 'got': 405,\n",
       " 'israeli': 406,\n",
       " '30': 407,\n",
       " 'need': 408,\n",
       " '4': 409,\n",
       " 'lead': 410,\n",
       " 'already': 411,\n",
       " 'russia': 412,\n",
       " 'though': 413,\n",
       " 'might': 414,\n",
       " 'free': 415,\n",
       " 'hit': 416,\n",
       " 'rights': 417,\n",
       " '11': 418,\n",
       " 'information': 419,\n",
       " 'away': 420,\n",
       " '12': 421,\n",
       " '5': 422,\n",
       " 'others': 423,\n",
       " 'control': 424,\n",
       " 'within': 425,\n",
       " 'large': 426,\n",
       " 'economy': 427,\n",
       " 'press': 428,\n",
       " 'agency': 429,\n",
       " 'water': 430,\n",
       " 'died': 431,\n",
       " 'career': 432,\n",
       " 'making': 433,\n",
       " '...': 434,\n",
       " 'deal': 435,\n",
       " 'attack': 436,\n",
       " 'side': 437,\n",
       " 'seven': 438,\n",
       " 'better': 439,\n",
       " 'less': 440,\n",
       " 'september': 441,\n",
       " 'once': 442,\n",
       " 'clinton': 443,\n",
       " 'main': 444,\n",
       " 'due': 445,\n",
       " 'committee': 446,\n",
       " 'building': 447,\n",
       " 'conference': 448,\n",
       " 'club': 449,\n",
       " 'january': 450,\n",
       " 'decision': 451,\n",
       " 'stock': 452,\n",
       " 'america': 453,\n",
       " 'given': 454,\n",
       " 'give': 455,\n",
       " 'often': 456,\n",
       " 'announced': 457,\n",
       " 'television': 458,\n",
       " 'industry': 459,\n",
       " 'order': 460,\n",
       " 'young': 461,\n",
       " \"'ve\": 462,\n",
       " 'palestinian': 463,\n",
       " 'age': 464,\n",
       " 'start': 465,\n",
       " 'administration': 466,\n",
       " 'russian': 467,\n",
       " 'prices': 468,\n",
       " 'round': 469,\n",
       " 'december': 470,\n",
       " 'nations': 471,\n",
       " \"'m\": 472,\n",
       " 'human': 473,\n",
       " 'india': 474,\n",
       " 'defense': 475,\n",
       " 'asked': 476,\n",
       " 'total': 477,\n",
       " 'october': 478,\n",
       " 'players': 479,\n",
       " 'bill': 480,\n",
       " 'important': 481,\n",
       " 'southern': 482,\n",
       " 'move': 483,\n",
       " 'fire': 484,\n",
       " 'population': 485,\n",
       " 'rose': 486,\n",
       " 'november': 487,\n",
       " 'include': 488,\n",
       " 'further': 489,\n",
       " 'nuclear': 490,\n",
       " 'street': 491,\n",
       " 'taken': 492,\n",
       " 'media': 493,\n",
       " 'different': 494,\n",
       " 'issue': 495,\n",
       " 'received': 496,\n",
       " 'secretary': 497,\n",
       " 'return': 498,\n",
       " 'college': 499,\n",
       " 'working': 500,\n",
       " 'community': 501,\n",
       " 'eight': 502,\n",
       " 'groups': 503,\n",
       " 'despite': 504,\n",
       " 'level': 505,\n",
       " 'largest': 506,\n",
       " 'whose': 507,\n",
       " 'attacks': 508,\n",
       " 'germany': 509,\n",
       " 'august': 510,\n",
       " 'change': 511,\n",
       " 'church': 512,\n",
       " 'nation': 513,\n",
       " 'german': 514,\n",
       " 'station': 515,\n",
       " 'london': 516,\n",
       " 'weeks': 517,\n",
       " 'having': 518,\n",
       " '18': 519,\n",
       " 'research': 520,\n",
       " 'black': 521,\n",
       " 'services': 522,\n",
       " 'story': 523,\n",
       " '6': 524,\n",
       " 'europe': 525,\n",
       " 'sales': 526,\n",
       " 'policy': 527,\n",
       " 'visit': 528,\n",
       " 'northern': 529,\n",
       " 'lot': 530,\n",
       " 'across': 531,\n",
       " 'per': 532,\n",
       " 'current': 533,\n",
       " 'board': 534,\n",
       " 'football': 535,\n",
       " 'ministry': 536,\n",
       " 'workers': 537,\n",
       " 'vote': 538,\n",
       " 'book': 539,\n",
       " 'fell': 540,\n",
       " 'seen': 541,\n",
       " 'role': 542,\n",
       " 'students': 543,\n",
       " 'shares': 544,\n",
       " 'iran': 545,\n",
       " 'process': 546,\n",
       " 'agreement': 547,\n",
       " 'quarter': 548,\n",
       " 'full': 549,\n",
       " 'match': 550,\n",
       " 'started': 551,\n",
       " 'growth': 552,\n",
       " 'yet': 553,\n",
       " 'moved': 554,\n",
       " 'possible': 555,\n",
       " 'western': 556,\n",
       " 'special': 557,\n",
       " '100': 558,\n",
       " 'plans': 559,\n",
       " 'interest': 560,\n",
       " 'behind': 561,\n",
       " 'strong': 562,\n",
       " 'england': 563,\n",
       " 'named': 564,\n",
       " 'food': 565,\n",
       " 'period': 566,\n",
       " 'real': 567,\n",
       " 'authorities': 568,\n",
       " 'car': 569,\n",
       " 'term': 570,\n",
       " 'rate': 571,\n",
       " 'race': 572,\n",
       " 'nearly': 573,\n",
       " 'korea': 574,\n",
       " 'enough': 575,\n",
       " 'site': 576,\n",
       " 'opposition': 577,\n",
       " 'keep': 578,\n",
       " '25': 579,\n",
       " 'call': 580,\n",
       " 'future': 581,\n",
       " 'taking': 582,\n",
       " 'island': 583,\n",
       " '2008': 584,\n",
       " '2006': 585,\n",
       " 'road': 586,\n",
       " 'outside': 587,\n",
       " 'really': 588,\n",
       " 'century': 589,\n",
       " 'democratic': 590,\n",
       " 'almost': 591,\n",
       " 'single': 592,\n",
       " 'share': 593,\n",
       " 'leading': 594,\n",
       " 'trying': 595,\n",
       " 'find': 596,\n",
       " 'album': 597,\n",
       " 'senior': 598,\n",
       " 'minutes': 599,\n",
       " 'together': 600,\n",
       " 'congress': 601,\n",
       " 'index': 602,\n",
       " 'australia': 603,\n",
       " 'results': 604,\n",
       " 'hard': 605,\n",
       " 'hours': 606,\n",
       " 'land': 607,\n",
       " 'action': 608,\n",
       " 'higher': 609,\n",
       " 'field': 610,\n",
       " 'cut': 611,\n",
       " 'coach': 612,\n",
       " 'elections': 613,\n",
       " 'san': 614,\n",
       " 'issues': 615,\n",
       " 'executive': 616,\n",
       " 'february': 617,\n",
       " 'production': 618,\n",
       " 'areas': 619,\n",
       " 'river': 620,\n",
       " 'face': 621,\n",
       " 'using': 622,\n",
       " 'japanese': 623,\n",
       " 'province': 624,\n",
       " 'park': 625,\n",
       " 'price': 626,\n",
       " 'commission': 627,\n",
       " 'california': 628,\n",
       " 'father': 629,\n",
       " 'son': 630,\n",
       " 'education': 631,\n",
       " '7': 632,\n",
       " 'village': 633,\n",
       " 'energy': 634,\n",
       " 'shot': 635,\n",
       " 'short': 636,\n",
       " 'africa': 637,\n",
       " 'key': 638,\n",
       " 'red': 639,\n",
       " 'association': 640,\n",
       " 'average': 641,\n",
       " 'pay': 642,\n",
       " 'exchange': 643,\n",
       " 'eu': 644,\n",
       " 'something': 645,\n",
       " 'gave': 646,\n",
       " 'likely': 647,\n",
       " 'player': 648,\n",
       " 'george': 649,\n",
       " '2007': 650,\n",
       " 'victory': 651,\n",
       " '8': 652,\n",
       " 'low': 653,\n",
       " 'things': 654,\n",
       " '2010': 655,\n",
       " 'pakistan': 656,\n",
       " '14': 657,\n",
       " 'post': 658,\n",
       " 'social': 659,\n",
       " 'continue': 660,\n",
       " 'ever': 661,\n",
       " 'look': 662,\n",
       " 'chairman': 663,\n",
       " 'job': 664,\n",
       " '2000': 665,\n",
       " 'soldiers': 666,\n",
       " 'able': 667,\n",
       " 'parliament': 668,\n",
       " 'front': 669,\n",
       " 'himself': 670,\n",
       " 'problems': 671,\n",
       " 'private': 672,\n",
       " 'lower': 673,\n",
       " 'list': 674,\n",
       " 'built': 675,\n",
       " '13': 676,\n",
       " 'efforts': 677,\n",
       " 'dollar': 678,\n",
       " 'miles': 679,\n",
       " 'included': 680,\n",
       " 'radio': 681,\n",
       " 'live': 682,\n",
       " 'form': 683,\n",
       " 'david': 684,\n",
       " 'african': 685,\n",
       " 'increase': 686,\n",
       " 'reports': 687,\n",
       " 'sent': 688,\n",
       " 'fourth': 689,\n",
       " 'always': 690,\n",
       " 'king': 691,\n",
       " '50': 692,\n",
       " 'tax': 693,\n",
       " 'taiwan': 694,\n",
       " 'britain': 695,\n",
       " '16': 696,\n",
       " 'playing': 697,\n",
       " 'title': 698,\n",
       " 'middle': 699,\n",
       " 'meet': 700,\n",
       " 'global': 701,\n",
       " 'wife': 702,\n",
       " '2009': 703,\n",
       " 'position': 704,\n",
       " 'located': 705,\n",
       " 'clear': 706,\n",
       " 'ahead': 707,\n",
       " '2004': 708,\n",
       " '2005': 709,\n",
       " 'iraqi': 710,\n",
       " 'english': 711,\n",
       " 'result': 712,\n",
       " 'release': 713,\n",
       " 'violence': 714,\n",
       " 'goal': 715,\n",
       " 'project': 716,\n",
       " 'closed': 717,\n",
       " 'border': 718,\n",
       " 'body': 719,\n",
       " 'soon': 720,\n",
       " 'crisis': 721,\n",
       " 'division': 722,\n",
       " '&amp;': 723,\n",
       " 'served': 724,\n",
       " 'tour': 725,\n",
       " 'hospital': 726,\n",
       " 'kong': 727,\n",
       " 'test': 728,\n",
       " 'hong': 729,\n",
       " 'u.n.': 730,\n",
       " 'inc.': 731,\n",
       " 'technology': 732,\n",
       " 'believe': 733,\n",
       " 'organization': 734,\n",
       " 'published': 735,\n",
       " 'weapons': 736,\n",
       " 'agreed': 737,\n",
       " 'why': 738,\n",
       " 'nine': 739,\n",
       " 'summer': 740,\n",
       " 'wanted': 741,\n",
       " 'republican': 742,\n",
       " 'act': 743,\n",
       " 'recently': 744,\n",
       " 'texas': 745,\n",
       " 'course': 746,\n",
       " 'problem': 747,\n",
       " 'senate': 748,\n",
       " 'medical': 749,\n",
       " 'un': 750,\n",
       " 'done': 751,\n",
       " 'reached': 752,\n",
       " 'star': 753,\n",
       " 'continued': 754,\n",
       " 'investors': 755,\n",
       " 'living': 756,\n",
       " 'care': 757,\n",
       " 'signed': 758,\n",
       " '17': 759,\n",
       " 'art': 760,\n",
       " 'provide': 761,\n",
       " 'worked': 762,\n",
       " 'presidential': 763,\n",
       " 'gold': 764,\n",
       " 'obama': 765,\n",
       " 'morning': 766,\n",
       " 'dead': 767,\n",
       " 'opened': 768,\n",
       " \"'ll\": 769,\n",
       " 'event': 770,\n",
       " 'previous': 771,\n",
       " 'cost': 772,\n",
       " 'instead': 773,\n",
       " 'canada': 774,\n",
       " 'band': 775,\n",
       " 'teams': 776,\n",
       " 'daily': 777,\n",
       " '2001': 778,\n",
       " 'available': 779,\n",
       " 'drug': 780,\n",
       " 'coming': 781,\n",
       " '2003': 782,\n",
       " 'investment': 783,\n",
       " '’s': 784,\n",
       " 'michael': 785,\n",
       " 'civil': 786,\n",
       " 'woman': 787,\n",
       " 'training': 788,\n",
       " 'appeared': 789,\n",
       " '9': 790,\n",
       " 'involved': 791,\n",
       " 'indian': 792,\n",
       " 'similar': 793,\n",
       " 'situation': 794,\n",
       " '24': 795,\n",
       " 'los': 796,\n",
       " 'running': 797,\n",
       " 'fighting': 798,\n",
       " 'mark': 799,\n",
       " '40': 800,\n",
       " 'trial': 801,\n",
       " 'hold': 802,\n",
       " 'australian': 803,\n",
       " 'thought': 804,\n",
       " '!': 805,\n",
       " 'study': 806,\n",
       " 'fall': 807,\n",
       " 'mother': 808,\n",
       " 'met': 809,\n",
       " 'relations': 810,\n",
       " 'anti': 811,\n",
       " '2002': 812,\n",
       " 'song': 813,\n",
       " 'popular': 814,\n",
       " 'base': 815,\n",
       " 'tv': 816,\n",
       " 'ground': 817,\n",
       " 'markets': 818,\n",
       " 'ii': 819,\n",
       " 'newspaper': 820,\n",
       " 'staff': 821,\n",
       " 'saw': 822,\n",
       " 'hand': 823,\n",
       " 'hope': 824,\n",
       " 'operations': 825,\n",
       " 'pressure': 826,\n",
       " 'americans': 827,\n",
       " 'eastern': 828,\n",
       " 'st.': 829,\n",
       " 'legal': 830,\n",
       " 'asia': 831,\n",
       " 'budget': 832,\n",
       " 'returned': 833,\n",
       " 'considered': 834,\n",
       " 'love': 835,\n",
       " 'wrote': 836,\n",
       " 'stop': 837,\n",
       " 'fight': 838,\n",
       " 'currently': 839,\n",
       " 'charges': 840,\n",
       " 'try': 841,\n",
       " 'aid': 842,\n",
       " 'ended': 843,\n",
       " 'management': 844,\n",
       " 'brought': 845,\n",
       " 'cases': 846,\n",
       " 'decided': 847,\n",
       " 'failed': 848,\n",
       " 'network': 849,\n",
       " 'works': 850,\n",
       " 'gas': 851,\n",
       " 'turned': 852,\n",
       " 'fact': 853,\n",
       " 'vice': 854,\n",
       " 'ca': 855,\n",
       " 'mexico': 856,\n",
       " 'trading': 857,\n",
       " 'especially': 858,\n",
       " 'reporters': 859,\n",
       " 'afghanistan': 860,\n",
       " 'common': 861,\n",
       " 'looking': 862,\n",
       " 'space': 863,\n",
       " 'rates': 864,\n",
       " 'manager': 865,\n",
       " 'loss': 866,\n",
       " '2011': 867,\n",
       " 'justice': 868,\n",
       " 'thousands': 869,\n",
       " 'james': 870,\n",
       " 'rather': 871,\n",
       " 'fund': 872,\n",
       " 'thing': 873,\n",
       " 'republic': 874,\n",
       " 'opening': 875,\n",
       " 'accused': 876,\n",
       " 'winning': 877,\n",
       " 'scored': 878,\n",
       " 'championship': 879,\n",
       " 'example': 880,\n",
       " 'getting': 881,\n",
       " 'biggest': 882,\n",
       " 'performance': 883,\n",
       " 'sports': 884,\n",
       " '1998': 885,\n",
       " 'let': 886,\n",
       " 'allowed': 887,\n",
       " 'schools': 888,\n",
       " 'means': 889,\n",
       " 'turn': 890,\n",
       " 'leave': 891,\n",
       " 'no.': 892,\n",
       " 'robert': 893,\n",
       " 'personal': 894,\n",
       " 'stocks': 895,\n",
       " 'showed': 896,\n",
       " 'light': 897,\n",
       " 'arrested': 898,\n",
       " 'person': 899,\n",
       " 'either': 900,\n",
       " 'offer': 901,\n",
       " 'majority': 902,\n",
       " 'battle': 903,\n",
       " '19': 904,\n",
       " 'class': 905,\n",
       " 'evidence': 906,\n",
       " 'makes': 907,\n",
       " 'society': 908,\n",
       " 'products': 909,\n",
       " 'regional': 910,\n",
       " 'needed': 911,\n",
       " 'stage': 912,\n",
       " 'am': 913,\n",
       " 'doing': 914,\n",
       " 'families': 915,\n",
       " 'construction': 916,\n",
       " 'various': 917,\n",
       " '1996': 918,\n",
       " 'sold': 919,\n",
       " 'independent': 920,\n",
       " 'kind': 921,\n",
       " 'airport': 922,\n",
       " 'paul': 923,\n",
       " 'judge': 924,\n",
       " 'internet': 925,\n",
       " 'movement': 926,\n",
       " 'room': 927,\n",
       " 'followed': 928,\n",
       " 'original': 929,\n",
       " 'angeles': 930,\n",
       " 'italy': 931,\n",
       " '`': 932,\n",
       " 'data': 933,\n",
       " 'comes': 934,\n",
       " 'parties': 935,\n",
       " 'nothing': 936,\n",
       " 'sea': 937,\n",
       " 'bring': 938,\n",
       " '2012': 939,\n",
       " 'annual': 940,\n",
       " 'officer': 941,\n",
       " 'beijing': 942,\n",
       " 'present': 943,\n",
       " 'remain': 944,\n",
       " 'nato': 945,\n",
       " '1999': 946,\n",
       " '22': 947,\n",
       " 'remains': 948,\n",
       " 'allow': 949,\n",
       " 'florida': 950,\n",
       " 'computer': 951,\n",
       " '21': 952,\n",
       " 'contract': 953,\n",
       " 'coast': 954,\n",
       " 'created': 955,\n",
       " 'demand': 956,\n",
       " 'operation': 957,\n",
       " 'events': 958,\n",
       " 'islamic': 959,\n",
       " 'beat': 960,\n",
       " 'analysts': 961,\n",
       " 'interview': 962,\n",
       " 'helped': 963,\n",
       " 'child': 964,\n",
       " 'probably': 965,\n",
       " 'spent': 966,\n",
       " 'asian': 967,\n",
       " 'effort': 968,\n",
       " 'cooperation': 969,\n",
       " 'shows': 970,\n",
       " 'calls': 971,\n",
       " 'investigation': 972,\n",
       " 'lives': 973,\n",
       " 'video': 974,\n",
       " 'yen': 975,\n",
       " 'runs': 976,\n",
       " 'tried': 977,\n",
       " 'bad': 978,\n",
       " 'described': 979,\n",
       " '1994': 980,\n",
       " 'toward': 981,\n",
       " 'written': 982,\n",
       " 'throughout': 983,\n",
       " 'established': 984,\n",
       " 'mission': 985,\n",
       " 'associated': 986,\n",
       " 'buy': 987,\n",
       " 'growing': 988,\n",
       " 'green': 989,\n",
       " 'forward': 990,\n",
       " 'competition': 991,\n",
       " 'poor': 992,\n",
       " 'latest': 993,\n",
       " 'banks': 994,\n",
       " 'question': 995,\n",
       " '1997': 996,\n",
       " 'prison': 997,\n",
       " 'feel': 998,\n",
       " 'attention': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af1bd48a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 임베딩을 Annoy 인덱스에 추가하고 해당 인덱스를 구축하여 검색을 위한 준비\n",
    "\n",
    "index = AnnoyIndex(len(word_vectors[0]),metric='euclidean')\n",
    "\n",
    "for _, i in word_to_index.items():\n",
    "    index.add_item(i, word_vectors[i])\n",
    "index.build(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ffa84f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'the',\n",
       " 1: ',',\n",
       " 2: '.',\n",
       " 3: 'of',\n",
       " 4: 'to',\n",
       " 5: 'and',\n",
       " 6: 'in',\n",
       " 7: 'a',\n",
       " 8: '\"',\n",
       " 9: \"'s\",\n",
       " 10: 'for',\n",
       " 11: '-',\n",
       " 12: 'that',\n",
       " 13: 'on',\n",
       " 14: 'is',\n",
       " 15: 'was',\n",
       " 16: 'said',\n",
       " 17: 'with',\n",
       " 18: 'he',\n",
       " 19: 'as',\n",
       " 20: 'it',\n",
       " 21: 'by',\n",
       " 22: 'at',\n",
       " 23: '(',\n",
       " 24: ')',\n",
       " 25: 'from',\n",
       " 26: 'his',\n",
       " 27: \"''\",\n",
       " 28: '``',\n",
       " 29: 'an',\n",
       " 30: 'be',\n",
       " 31: 'has',\n",
       " 32: 'are',\n",
       " 33: 'have',\n",
       " 34: 'but',\n",
       " 35: 'were',\n",
       " 36: 'not',\n",
       " 37: 'this',\n",
       " 38: 'who',\n",
       " 39: 'they',\n",
       " 40: 'had',\n",
       " 41: 'i',\n",
       " 42: 'which',\n",
       " 43: 'will',\n",
       " 44: 'their',\n",
       " 45: ':',\n",
       " 46: 'or',\n",
       " 47: 'its',\n",
       " 48: 'one',\n",
       " 49: 'after',\n",
       " 50: 'new',\n",
       " 51: 'been',\n",
       " 52: 'also',\n",
       " 53: 'we',\n",
       " 54: 'would',\n",
       " 55: 'two',\n",
       " 56: 'more',\n",
       " 57: \"'\",\n",
       " 58: 'first',\n",
       " 59: 'about',\n",
       " 60: 'up',\n",
       " 61: 'when',\n",
       " 62: 'year',\n",
       " 63: 'there',\n",
       " 64: 'all',\n",
       " 65: '--',\n",
       " 66: 'out',\n",
       " 67: 'she',\n",
       " 68: 'other',\n",
       " 69: 'people',\n",
       " 70: \"n't\",\n",
       " 71: 'her',\n",
       " 72: 'percent',\n",
       " 73: 'than',\n",
       " 74: 'over',\n",
       " 75: 'into',\n",
       " 76: 'last',\n",
       " 77: 'some',\n",
       " 78: 'government',\n",
       " 79: 'time',\n",
       " 80: '$',\n",
       " 81: 'you',\n",
       " 82: 'years',\n",
       " 83: 'if',\n",
       " 84: 'no',\n",
       " 85: 'world',\n",
       " 86: 'can',\n",
       " 87: 'three',\n",
       " 88: 'do',\n",
       " 89: ';',\n",
       " 90: 'president',\n",
       " 91: 'only',\n",
       " 92: 'state',\n",
       " 93: 'million',\n",
       " 94: 'could',\n",
       " 95: 'us',\n",
       " 96: 'most',\n",
       " 97: '_',\n",
       " 98: 'against',\n",
       " 99: 'u.s.',\n",
       " 100: 'so',\n",
       " 101: 'them',\n",
       " 102: 'what',\n",
       " 103: 'him',\n",
       " 104: 'united',\n",
       " 105: 'during',\n",
       " 106: 'before',\n",
       " 107: 'may',\n",
       " 108: 'since',\n",
       " 109: 'many',\n",
       " 110: 'while',\n",
       " 111: 'where',\n",
       " 112: 'states',\n",
       " 113: 'because',\n",
       " 114: 'now',\n",
       " 115: 'city',\n",
       " 116: 'made',\n",
       " 117: 'like',\n",
       " 118: 'between',\n",
       " 119: 'did',\n",
       " 120: 'just',\n",
       " 121: 'national',\n",
       " 122: 'day',\n",
       " 123: 'country',\n",
       " 124: 'under',\n",
       " 125: 'such',\n",
       " 126: 'second',\n",
       " 127: 'then',\n",
       " 128: 'company',\n",
       " 129: 'group',\n",
       " 130: 'any',\n",
       " 131: 'through',\n",
       " 132: 'china',\n",
       " 133: 'four',\n",
       " 134: 'being',\n",
       " 135: 'down',\n",
       " 136: 'war',\n",
       " 137: 'back',\n",
       " 138: 'off',\n",
       " 139: 'south',\n",
       " 140: 'american',\n",
       " 141: 'minister',\n",
       " 142: 'police',\n",
       " 143: 'well',\n",
       " 144: 'including',\n",
       " 145: 'team',\n",
       " 146: 'international',\n",
       " 147: 'week',\n",
       " 148: 'officials',\n",
       " 149: 'still',\n",
       " 150: 'both',\n",
       " 151: 'even',\n",
       " 152: 'high',\n",
       " 153: 'part',\n",
       " 154: 'told',\n",
       " 155: 'those',\n",
       " 156: 'end',\n",
       " 157: 'former',\n",
       " 158: 'these',\n",
       " 159: 'make',\n",
       " 160: 'billion',\n",
       " 161: 'work',\n",
       " 162: 'our',\n",
       " 163: 'home',\n",
       " 164: 'school',\n",
       " 165: 'party',\n",
       " 166: 'house',\n",
       " 167: 'old',\n",
       " 168: 'later',\n",
       " 169: 'get',\n",
       " 170: 'another',\n",
       " 171: 'tuesday',\n",
       " 172: 'news',\n",
       " 173: 'long',\n",
       " 174: 'five',\n",
       " 175: 'called',\n",
       " 176: '1',\n",
       " 177: 'wednesday',\n",
       " 178: 'military',\n",
       " 179: 'way',\n",
       " 180: 'used',\n",
       " 181: 'much',\n",
       " 182: 'next',\n",
       " 183: 'monday',\n",
       " 184: 'thursday',\n",
       " 185: 'friday',\n",
       " 186: 'game',\n",
       " 187: 'here',\n",
       " 188: '?',\n",
       " 189: 'should',\n",
       " 190: 'take',\n",
       " 191: 'very',\n",
       " 192: 'my',\n",
       " 193: 'north',\n",
       " 194: 'security',\n",
       " 195: 'season',\n",
       " 196: 'york',\n",
       " 197: 'how',\n",
       " 198: 'public',\n",
       " 199: 'early',\n",
       " 200: 'according',\n",
       " 201: 'several',\n",
       " 202: 'court',\n",
       " 203: 'say',\n",
       " 204: 'around',\n",
       " 205: 'foreign',\n",
       " 206: '10',\n",
       " 207: 'until',\n",
       " 208: 'set',\n",
       " 209: 'political',\n",
       " 210: 'says',\n",
       " 211: 'market',\n",
       " 212: 'however',\n",
       " 213: 'family',\n",
       " 214: 'life',\n",
       " 215: 'same',\n",
       " 216: 'general',\n",
       " 217: '–',\n",
       " 218: 'left',\n",
       " 219: 'good',\n",
       " 220: 'top',\n",
       " 221: 'university',\n",
       " 222: 'going',\n",
       " 223: 'number',\n",
       " 224: 'major',\n",
       " 225: 'known',\n",
       " 226: 'points',\n",
       " 227: 'won',\n",
       " 228: 'six',\n",
       " 229: 'month',\n",
       " 230: 'dollars',\n",
       " 231: 'bank',\n",
       " 232: '2',\n",
       " 233: 'iraq',\n",
       " 234: 'use',\n",
       " 235: 'members',\n",
       " 236: 'each',\n",
       " 237: 'area',\n",
       " 238: 'found',\n",
       " 239: 'official',\n",
       " 240: 'sunday',\n",
       " 241: 'place',\n",
       " 242: 'go',\n",
       " 243: 'based',\n",
       " 244: 'among',\n",
       " 245: 'third',\n",
       " 246: 'times',\n",
       " 247: 'took',\n",
       " 248: 'right',\n",
       " 249: 'days',\n",
       " 250: 'local',\n",
       " 251: 'economic',\n",
       " 252: 'countries',\n",
       " 253: 'see',\n",
       " 254: 'best',\n",
       " 255: 'report',\n",
       " 256: 'killed',\n",
       " 257: 'held',\n",
       " 258: 'business',\n",
       " 259: 'west',\n",
       " 260: 'does',\n",
       " 261: 'own',\n",
       " 262: '%',\n",
       " 263: 'came',\n",
       " 264: 'law',\n",
       " 265: 'months',\n",
       " 266: 'women',\n",
       " 267: \"'re\",\n",
       " 268: 'power',\n",
       " 269: 'think',\n",
       " 270: 'service',\n",
       " 271: 'children',\n",
       " 272: 'bush',\n",
       " 273: 'show',\n",
       " 274: '/',\n",
       " 275: 'help',\n",
       " 276: 'chief',\n",
       " 277: 'saturday',\n",
       " 278: 'system',\n",
       " 279: 'john',\n",
       " 280: 'support',\n",
       " 281: 'series',\n",
       " 282: 'play',\n",
       " 283: 'office',\n",
       " 284: 'following',\n",
       " 285: 'me',\n",
       " 286: 'meeting',\n",
       " 287: 'expected',\n",
       " 288: 'late',\n",
       " 289: 'washington',\n",
       " 290: 'games',\n",
       " 291: 'european',\n",
       " 292: 'league',\n",
       " 293: 'reported',\n",
       " 294: 'final',\n",
       " 295: 'added',\n",
       " 296: 'without',\n",
       " 297: 'british',\n",
       " 298: 'white',\n",
       " 299: 'history',\n",
       " 300: 'man',\n",
       " 301: 'men',\n",
       " 302: 'became',\n",
       " 303: 'want',\n",
       " 304: 'march',\n",
       " 305: 'case',\n",
       " 306: 'few',\n",
       " 307: 'run',\n",
       " 308: 'money',\n",
       " 309: 'began',\n",
       " 310: 'open',\n",
       " 311: 'name',\n",
       " 312: 'trade',\n",
       " 313: 'center',\n",
       " 314: '3',\n",
       " 315: 'israel',\n",
       " 316: 'oil',\n",
       " 317: 'too',\n",
       " 318: 'al',\n",
       " 319: 'film',\n",
       " 320: 'win',\n",
       " 321: 'led',\n",
       " 322: 'east',\n",
       " 323: 'central',\n",
       " 324: '20',\n",
       " 325: 'air',\n",
       " 326: 'come',\n",
       " 327: 'chinese',\n",
       " 328: 'town',\n",
       " 329: 'leader',\n",
       " 330: 'army',\n",
       " 331: 'line',\n",
       " 332: 'never',\n",
       " 333: 'little',\n",
       " 334: 'played',\n",
       " 335: 'prime',\n",
       " 336: 'death',\n",
       " 337: 'companies',\n",
       " 338: 'least',\n",
       " 339: 'put',\n",
       " 340: 'forces',\n",
       " 341: 'past',\n",
       " 342: 'de',\n",
       " 343: 'half',\n",
       " 344: 'june',\n",
       " 345: 'saying',\n",
       " 346: 'know',\n",
       " 347: 'federal',\n",
       " 348: 'french',\n",
       " 349: 'peace',\n",
       " 350: 'earlier',\n",
       " 351: 'capital',\n",
       " 352: 'force',\n",
       " 353: 'great',\n",
       " 354: 'union',\n",
       " 355: 'near',\n",
       " 356: 'released',\n",
       " 357: 'small',\n",
       " 358: 'department',\n",
       " 359: 'every',\n",
       " 360: 'health',\n",
       " 361: 'japan',\n",
       " 362: 'head',\n",
       " 363: 'ago',\n",
       " 364: 'night',\n",
       " 365: 'big',\n",
       " 366: 'cup',\n",
       " 367: 'election',\n",
       " 368: 'region',\n",
       " 369: 'director',\n",
       " 370: 'talks',\n",
       " 371: 'program',\n",
       " 372: 'far',\n",
       " 373: 'today',\n",
       " 374: 'statement',\n",
       " 375: 'july',\n",
       " 376: 'although',\n",
       " 377: 'district',\n",
       " 378: 'again',\n",
       " 379: 'born',\n",
       " 380: 'development',\n",
       " 381: 'leaders',\n",
       " 382: 'council',\n",
       " 383: 'close',\n",
       " 384: 'record',\n",
       " 385: 'along',\n",
       " 386: 'county',\n",
       " 387: 'france',\n",
       " 388: 'went',\n",
       " 389: 'point',\n",
       " 390: 'must',\n",
       " 391: 'spokesman',\n",
       " 392: 'your',\n",
       " 393: 'member',\n",
       " 394: 'plan',\n",
       " 395: 'financial',\n",
       " 396: 'april',\n",
       " 397: 'recent',\n",
       " 398: 'campaign',\n",
       " 399: 'become',\n",
       " 400: 'troops',\n",
       " 401: 'whether',\n",
       " 402: 'lost',\n",
       " 403: 'music',\n",
       " 404: '15',\n",
       " 405: 'got',\n",
       " 406: 'israeli',\n",
       " 407: '30',\n",
       " 408: 'need',\n",
       " 409: '4',\n",
       " 410: 'lead',\n",
       " 411: 'already',\n",
       " 412: 'russia',\n",
       " 413: 'though',\n",
       " 414: 'might',\n",
       " 415: 'free',\n",
       " 416: 'hit',\n",
       " 417: 'rights',\n",
       " 418: '11',\n",
       " 419: 'information',\n",
       " 420: 'away',\n",
       " 421: '12',\n",
       " 422: '5',\n",
       " 423: 'others',\n",
       " 424: 'control',\n",
       " 425: 'within',\n",
       " 426: 'large',\n",
       " 427: 'economy',\n",
       " 428: 'press',\n",
       " 429: 'agency',\n",
       " 430: 'water',\n",
       " 431: 'died',\n",
       " 432: 'career',\n",
       " 433: 'making',\n",
       " 434: '...',\n",
       " 435: 'deal',\n",
       " 436: 'attack',\n",
       " 437: 'side',\n",
       " 438: 'seven',\n",
       " 439: 'better',\n",
       " 440: 'less',\n",
       " 441: 'september',\n",
       " 442: 'once',\n",
       " 443: 'clinton',\n",
       " 444: 'main',\n",
       " 445: 'due',\n",
       " 446: 'committee',\n",
       " 447: 'building',\n",
       " 448: 'conference',\n",
       " 449: 'club',\n",
       " 450: 'january',\n",
       " 451: 'decision',\n",
       " 452: 'stock',\n",
       " 453: 'america',\n",
       " 454: 'given',\n",
       " 455: 'give',\n",
       " 456: 'often',\n",
       " 457: 'announced',\n",
       " 458: 'television',\n",
       " 459: 'industry',\n",
       " 460: 'order',\n",
       " 461: 'young',\n",
       " 462: \"'ve\",\n",
       " 463: 'palestinian',\n",
       " 464: 'age',\n",
       " 465: 'start',\n",
       " 466: 'administration',\n",
       " 467: 'russian',\n",
       " 468: 'prices',\n",
       " 469: 'round',\n",
       " 470: 'december',\n",
       " 471: 'nations',\n",
       " 472: \"'m\",\n",
       " 473: 'human',\n",
       " 474: 'india',\n",
       " 475: 'defense',\n",
       " 476: 'asked',\n",
       " 477: 'total',\n",
       " 478: 'october',\n",
       " 479: 'players',\n",
       " 480: 'bill',\n",
       " 481: 'important',\n",
       " 482: 'southern',\n",
       " 483: 'move',\n",
       " 484: 'fire',\n",
       " 485: 'population',\n",
       " 486: 'rose',\n",
       " 487: 'november',\n",
       " 488: 'include',\n",
       " 489: 'further',\n",
       " 490: 'nuclear',\n",
       " 491: 'street',\n",
       " 492: 'taken',\n",
       " 493: 'media',\n",
       " 494: 'different',\n",
       " 495: 'issue',\n",
       " 496: 'received',\n",
       " 497: 'secretary',\n",
       " 498: 'return',\n",
       " 499: 'college',\n",
       " 500: 'working',\n",
       " 501: 'community',\n",
       " 502: 'eight',\n",
       " 503: 'groups',\n",
       " 504: 'despite',\n",
       " 505: 'level',\n",
       " 506: 'largest',\n",
       " 507: 'whose',\n",
       " 508: 'attacks',\n",
       " 509: 'germany',\n",
       " 510: 'august',\n",
       " 511: 'change',\n",
       " 512: 'church',\n",
       " 513: 'nation',\n",
       " 514: 'german',\n",
       " 515: 'station',\n",
       " 516: 'london',\n",
       " 517: 'weeks',\n",
       " 518: 'having',\n",
       " 519: '18',\n",
       " 520: 'research',\n",
       " 521: 'black',\n",
       " 522: 'services',\n",
       " 523: 'story',\n",
       " 524: '6',\n",
       " 525: 'europe',\n",
       " 526: 'sales',\n",
       " 527: 'policy',\n",
       " 528: 'visit',\n",
       " 529: 'northern',\n",
       " 530: 'lot',\n",
       " 531: 'across',\n",
       " 532: 'per',\n",
       " 533: 'current',\n",
       " 534: 'board',\n",
       " 535: 'football',\n",
       " 536: 'ministry',\n",
       " 537: 'workers',\n",
       " 538: 'vote',\n",
       " 539: 'book',\n",
       " 540: 'fell',\n",
       " 541: 'seen',\n",
       " 542: 'role',\n",
       " 543: 'students',\n",
       " 544: 'shares',\n",
       " 545: 'iran',\n",
       " 546: 'process',\n",
       " 547: 'agreement',\n",
       " 548: 'quarter',\n",
       " 549: 'full',\n",
       " 550: 'match',\n",
       " 551: 'started',\n",
       " 552: 'growth',\n",
       " 553: 'yet',\n",
       " 554: 'moved',\n",
       " 555: 'possible',\n",
       " 556: 'western',\n",
       " 557: 'special',\n",
       " 558: '100',\n",
       " 559: 'plans',\n",
       " 560: 'interest',\n",
       " 561: 'behind',\n",
       " 562: 'strong',\n",
       " 563: 'england',\n",
       " 564: 'named',\n",
       " 565: 'food',\n",
       " 566: 'period',\n",
       " 567: 'real',\n",
       " 568: 'authorities',\n",
       " 569: 'car',\n",
       " 570: 'term',\n",
       " 571: 'rate',\n",
       " 572: 'race',\n",
       " 573: 'nearly',\n",
       " 574: 'korea',\n",
       " 575: 'enough',\n",
       " 576: 'site',\n",
       " 577: 'opposition',\n",
       " 578: 'keep',\n",
       " 579: '25',\n",
       " 580: 'call',\n",
       " 581: 'future',\n",
       " 582: 'taking',\n",
       " 583: 'island',\n",
       " 584: '2008',\n",
       " 585: '2006',\n",
       " 586: 'road',\n",
       " 587: 'outside',\n",
       " 588: 'really',\n",
       " 589: 'century',\n",
       " 590: 'democratic',\n",
       " 591: 'almost',\n",
       " 592: 'single',\n",
       " 593: 'share',\n",
       " 594: 'leading',\n",
       " 595: 'trying',\n",
       " 596: 'find',\n",
       " 597: 'album',\n",
       " 598: 'senior',\n",
       " 599: 'minutes',\n",
       " 600: 'together',\n",
       " 601: 'congress',\n",
       " 602: 'index',\n",
       " 603: 'australia',\n",
       " 604: 'results',\n",
       " 605: 'hard',\n",
       " 606: 'hours',\n",
       " 607: 'land',\n",
       " 608: 'action',\n",
       " 609: 'higher',\n",
       " 610: 'field',\n",
       " 611: 'cut',\n",
       " 612: 'coach',\n",
       " 613: 'elections',\n",
       " 614: 'san',\n",
       " 615: 'issues',\n",
       " 616: 'executive',\n",
       " 617: 'february',\n",
       " 618: 'production',\n",
       " 619: 'areas',\n",
       " 620: 'river',\n",
       " 621: 'face',\n",
       " 622: 'using',\n",
       " 623: 'japanese',\n",
       " 624: 'province',\n",
       " 625: 'park',\n",
       " 626: 'price',\n",
       " 627: 'commission',\n",
       " 628: 'california',\n",
       " 629: 'father',\n",
       " 630: 'son',\n",
       " 631: 'education',\n",
       " 632: '7',\n",
       " 633: 'village',\n",
       " 634: 'energy',\n",
       " 635: 'shot',\n",
       " 636: 'short',\n",
       " 637: 'africa',\n",
       " 638: 'key',\n",
       " 639: 'red',\n",
       " 640: 'association',\n",
       " 641: 'average',\n",
       " 642: 'pay',\n",
       " 643: 'exchange',\n",
       " 644: 'eu',\n",
       " 645: 'something',\n",
       " 646: 'gave',\n",
       " 647: 'likely',\n",
       " 648: 'player',\n",
       " 649: 'george',\n",
       " 650: '2007',\n",
       " 651: 'victory',\n",
       " 652: '8',\n",
       " 653: 'low',\n",
       " 654: 'things',\n",
       " 655: '2010',\n",
       " 656: 'pakistan',\n",
       " 657: '14',\n",
       " 658: 'post',\n",
       " 659: 'social',\n",
       " 660: 'continue',\n",
       " 661: 'ever',\n",
       " 662: 'look',\n",
       " 663: 'chairman',\n",
       " 664: 'job',\n",
       " 665: '2000',\n",
       " 666: 'soldiers',\n",
       " 667: 'able',\n",
       " 668: 'parliament',\n",
       " 669: 'front',\n",
       " 670: 'himself',\n",
       " 671: 'problems',\n",
       " 672: 'private',\n",
       " 673: 'lower',\n",
       " 674: 'list',\n",
       " 675: 'built',\n",
       " 676: '13',\n",
       " 677: 'efforts',\n",
       " 678: 'dollar',\n",
       " 679: 'miles',\n",
       " 680: 'included',\n",
       " 681: 'radio',\n",
       " 682: 'live',\n",
       " 683: 'form',\n",
       " 684: 'david',\n",
       " 685: 'african',\n",
       " 686: 'increase',\n",
       " 687: 'reports',\n",
       " 688: 'sent',\n",
       " 689: 'fourth',\n",
       " 690: 'always',\n",
       " 691: 'king',\n",
       " 692: '50',\n",
       " 693: 'tax',\n",
       " 694: 'taiwan',\n",
       " 695: 'britain',\n",
       " 696: '16',\n",
       " 697: 'playing',\n",
       " 698: 'title',\n",
       " 699: 'middle',\n",
       " 700: 'meet',\n",
       " 701: 'global',\n",
       " 702: 'wife',\n",
       " 703: '2009',\n",
       " 704: 'position',\n",
       " 705: 'located',\n",
       " 706: 'clear',\n",
       " 707: 'ahead',\n",
       " 708: '2004',\n",
       " 709: '2005',\n",
       " 710: 'iraqi',\n",
       " 711: 'english',\n",
       " 712: 'result',\n",
       " 713: 'release',\n",
       " 714: 'violence',\n",
       " 715: 'goal',\n",
       " 716: 'project',\n",
       " 717: 'closed',\n",
       " 718: 'border',\n",
       " 719: 'body',\n",
       " 720: 'soon',\n",
       " 721: 'crisis',\n",
       " 722: 'division',\n",
       " 723: '&amp;',\n",
       " 724: 'served',\n",
       " 725: 'tour',\n",
       " 726: 'hospital',\n",
       " 727: 'kong',\n",
       " 728: 'test',\n",
       " 729: 'hong',\n",
       " 730: 'u.n.',\n",
       " 731: 'inc.',\n",
       " 732: 'technology',\n",
       " 733: 'believe',\n",
       " 734: 'organization',\n",
       " 735: 'published',\n",
       " 736: 'weapons',\n",
       " 737: 'agreed',\n",
       " 738: 'why',\n",
       " 739: 'nine',\n",
       " 740: 'summer',\n",
       " 741: 'wanted',\n",
       " 742: 'republican',\n",
       " 743: 'act',\n",
       " 744: 'recently',\n",
       " 745: 'texas',\n",
       " 746: 'course',\n",
       " 747: 'problem',\n",
       " 748: 'senate',\n",
       " 749: 'medical',\n",
       " 750: 'un',\n",
       " 751: 'done',\n",
       " 752: 'reached',\n",
       " 753: 'star',\n",
       " 754: 'continued',\n",
       " 755: 'investors',\n",
       " 756: 'living',\n",
       " 757: 'care',\n",
       " 758: 'signed',\n",
       " 759: '17',\n",
       " 760: 'art',\n",
       " 761: 'provide',\n",
       " 762: 'worked',\n",
       " 763: 'presidential',\n",
       " 764: 'gold',\n",
       " 765: 'obama',\n",
       " 766: 'morning',\n",
       " 767: 'dead',\n",
       " 768: 'opened',\n",
       " 769: \"'ll\",\n",
       " 770: 'event',\n",
       " 771: 'previous',\n",
       " 772: 'cost',\n",
       " 773: 'instead',\n",
       " 774: 'canada',\n",
       " 775: 'band',\n",
       " 776: 'teams',\n",
       " 777: 'daily',\n",
       " 778: '2001',\n",
       " 779: 'available',\n",
       " 780: 'drug',\n",
       " 781: 'coming',\n",
       " 782: '2003',\n",
       " 783: 'investment',\n",
       " 784: '’s',\n",
       " 785: 'michael',\n",
       " 786: 'civil',\n",
       " 787: 'woman',\n",
       " 788: 'training',\n",
       " 789: 'appeared',\n",
       " 790: '9',\n",
       " 791: 'involved',\n",
       " 792: 'indian',\n",
       " 793: 'similar',\n",
       " 794: 'situation',\n",
       " 795: '24',\n",
       " 796: 'los',\n",
       " 797: 'running',\n",
       " 798: 'fighting',\n",
       " 799: 'mark',\n",
       " 800: '40',\n",
       " 801: 'trial',\n",
       " 802: 'hold',\n",
       " 803: 'australian',\n",
       " 804: 'thought',\n",
       " 805: '!',\n",
       " 806: 'study',\n",
       " 807: 'fall',\n",
       " 808: 'mother',\n",
       " 809: 'met',\n",
       " 810: 'relations',\n",
       " 811: 'anti',\n",
       " 812: '2002',\n",
       " 813: 'song',\n",
       " 814: 'popular',\n",
       " 815: 'base',\n",
       " 816: 'tv',\n",
       " 817: 'ground',\n",
       " 818: 'markets',\n",
       " 819: 'ii',\n",
       " 820: 'newspaper',\n",
       " 821: 'staff',\n",
       " 822: 'saw',\n",
       " 823: 'hand',\n",
       " 824: 'hope',\n",
       " 825: 'operations',\n",
       " 826: 'pressure',\n",
       " 827: 'americans',\n",
       " 828: 'eastern',\n",
       " 829: 'st.',\n",
       " 830: 'legal',\n",
       " 831: 'asia',\n",
       " 832: 'budget',\n",
       " 833: 'returned',\n",
       " 834: 'considered',\n",
       " 835: 'love',\n",
       " 836: 'wrote',\n",
       " 837: 'stop',\n",
       " 838: 'fight',\n",
       " 839: 'currently',\n",
       " 840: 'charges',\n",
       " 841: 'try',\n",
       " 842: 'aid',\n",
       " 843: 'ended',\n",
       " 844: 'management',\n",
       " 845: 'brought',\n",
       " 846: 'cases',\n",
       " 847: 'decided',\n",
       " 848: 'failed',\n",
       " 849: 'network',\n",
       " 850: 'works',\n",
       " 851: 'gas',\n",
       " 852: 'turned',\n",
       " 853: 'fact',\n",
       " 854: 'vice',\n",
       " 855: 'ca',\n",
       " 856: 'mexico',\n",
       " 857: 'trading',\n",
       " 858: 'especially',\n",
       " 859: 'reporters',\n",
       " 860: 'afghanistan',\n",
       " 861: 'common',\n",
       " 862: 'looking',\n",
       " 863: 'space',\n",
       " 864: 'rates',\n",
       " 865: 'manager',\n",
       " 866: 'loss',\n",
       " 867: '2011',\n",
       " 868: 'justice',\n",
       " 869: 'thousands',\n",
       " 870: 'james',\n",
       " 871: 'rather',\n",
       " 872: 'fund',\n",
       " 873: 'thing',\n",
       " 874: 'republic',\n",
       " 875: 'opening',\n",
       " 876: 'accused',\n",
       " 877: 'winning',\n",
       " 878: 'scored',\n",
       " 879: 'championship',\n",
       " 880: 'example',\n",
       " 881: 'getting',\n",
       " 882: 'biggest',\n",
       " 883: 'performance',\n",
       " 884: 'sports',\n",
       " 885: '1998',\n",
       " 886: 'let',\n",
       " 887: 'allowed',\n",
       " 888: 'schools',\n",
       " 889: 'means',\n",
       " 890: 'turn',\n",
       " 891: 'leave',\n",
       " 892: 'no.',\n",
       " 893: 'robert',\n",
       " 894: 'personal',\n",
       " 895: 'stocks',\n",
       " 896: 'showed',\n",
       " 897: 'light',\n",
       " 898: 'arrested',\n",
       " 899: 'person',\n",
       " 900: 'either',\n",
       " 901: 'offer',\n",
       " 902: 'majority',\n",
       " 903: 'battle',\n",
       " 904: '19',\n",
       " 905: 'class',\n",
       " 906: 'evidence',\n",
       " 907: 'makes',\n",
       " 908: 'society',\n",
       " 909: 'products',\n",
       " 910: 'regional',\n",
       " 911: 'needed',\n",
       " 912: 'stage',\n",
       " 913: 'am',\n",
       " 914: 'doing',\n",
       " 915: 'families',\n",
       " 916: 'construction',\n",
       " 917: 'various',\n",
       " 918: '1996',\n",
       " 919: 'sold',\n",
       " 920: 'independent',\n",
       " 921: 'kind',\n",
       " 922: 'airport',\n",
       " 923: 'paul',\n",
       " 924: 'judge',\n",
       " 925: 'internet',\n",
       " 926: 'movement',\n",
       " 927: 'room',\n",
       " 928: 'followed',\n",
       " 929: 'original',\n",
       " 930: 'angeles',\n",
       " 931: 'italy',\n",
       " 932: '`',\n",
       " 933: 'data',\n",
       " 934: 'comes',\n",
       " 935: 'parties',\n",
       " 936: 'nothing',\n",
       " 937: 'sea',\n",
       " 938: 'bring',\n",
       " 939: '2012',\n",
       " 940: 'annual',\n",
       " 941: 'officer',\n",
       " 942: 'beijing',\n",
       " 943: 'present',\n",
       " 944: 'remain',\n",
       " 945: 'nato',\n",
       " 946: '1999',\n",
       " 947: '22',\n",
       " 948: 'remains',\n",
       " 949: 'allow',\n",
       " 950: 'florida',\n",
       " 951: 'computer',\n",
       " 952: '21',\n",
       " 953: 'contract',\n",
       " 954: 'coast',\n",
       " 955: 'created',\n",
       " 956: 'demand',\n",
       " 957: 'operation',\n",
       " 958: 'events',\n",
       " 959: 'islamic',\n",
       " 960: 'beat',\n",
       " 961: 'analysts',\n",
       " 962: 'interview',\n",
       " 963: 'helped',\n",
       " 964: 'child',\n",
       " 965: 'probably',\n",
       " 966: 'spent',\n",
       " 967: 'asian',\n",
       " 968: 'effort',\n",
       " 969: 'cooperation',\n",
       " 970: 'shows',\n",
       " 971: 'calls',\n",
       " 972: 'investigation',\n",
       " 973: 'lives',\n",
       " 974: 'video',\n",
       " 975: 'yen',\n",
       " 976: 'runs',\n",
       " 977: 'tried',\n",
       " 978: 'bad',\n",
       " 979: 'described',\n",
       " 980: '1994',\n",
       " 981: 'toward',\n",
       " 982: 'written',\n",
       " 983: 'throughout',\n",
       " 984: 'established',\n",
       " 985: 'mission',\n",
       " 986: 'associated',\n",
       " 987: 'buy',\n",
       " 988: 'growing',\n",
       " 989: 'green',\n",
       " 990: 'forward',\n",
       " 991: 'competition',\n",
       " 992: 'poor',\n",
       " 993: 'latest',\n",
       " 994: 'banks',\n",
       " 995: 'question',\n",
       " 996: '1997',\n",
       " 997: 'prison',\n",
       " 998: 'feel',\n",
       " 999: 'attention',\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word = {v:k for k,v in word_to_index.items()}\n",
    "index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9e1b798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['js03', 'bulletinyyy', '65stk', 'mo95', 'bdb94']\n"
     ]
    }
   ],
   "source": [
    "example_vector = [0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1,\n",
    "                 0.1, 0.2, -0.3, 0.4, -0.1]\n",
    "\n",
    "def get_closet_to_vector(vector, n=1):\n",
    "    \"\"\"벡터가 주어지면 최근접 이웃을 n개 반환한다. \"\"\"\n",
    "    nn_indices = index.get_nns_by_vector(vector, n)\n",
    "    return [index_to_word[neighbor] for neighbor in nn_indices]\n",
    "\n",
    "print(get_closet_to_vector(example_vector,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e4a0c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_print_analogy(word1,word2,word3):\n",
    "    \n",
    "    vec1 = get_embedding(word1)\n",
    "    vec2 = get_embedding(word2)\n",
    "    vec3 = get_embedding(word3)\n",
    "    \n",
    "#     단순 가정 : 이 유추는 공간적 관계입니다. \n",
    "    spatial_relationship = vec2 - vec1\n",
    "    vec4 = vec3 + spatial_relationship\n",
    "    \n",
    "    closest_words = get_closet_to_vector(vec4,n=4)\n",
    "    existing_words = ([word1,word2,word3])\n",
    "    closest_words = [ word for word in closest_words \n",
    "                             if word not in existing_words]\n",
    "    \n",
    "    if len(closest_words)==0:\n",
    "        print(\"계산된 벡터와 가장 가까운 이웃을 찾을 수 없습니다.\")\n",
    "        return \n",
    "    \n",
    "    for word4 in closest_words:\n",
    "        print(\"{} : {} :: {} : {}\".format(word1,word2,word3,word4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c864f131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : he :: woman : she\n",
      "man : he :: woman : never\n"
     ]
    }
   ],
   "source": [
    "# 관계 1 : 성별 명사와 대명사의 관계\n",
    "compute_and_print_analogy('man', 'he', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0849621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fly : plane :: sail : ship\n",
      "fly : plane :: sail : vessel\n"
     ]
    }
   ],
   "source": [
    "# 관계 2 : 동사-명사 관계 \n",
    "compute_and_print_analogy('fly', 'plane', 'sail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae583fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat : kitten :: dog : puppy\n",
      "cat : kitten :: dog : puppies\n",
      "cat : kitten :: dog : panting\n"
     ]
    }
   ],
   "source": [
    "# 관계 3 : 명사-명사 관계 \n",
    "compute_and_print_analogy('cat', 'kitten', 'dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a0269f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue : color :: dog : cat\n",
      "blue : color :: dog : animal\n",
      "blue : color :: dog : breed\n"
     ]
    }
   ],
   "source": [
    "# 관계 4 : 상위어 관계 \n",
    "compute_and_print_analogy('blue', 'color', 'dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ef7463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toe : foot :: finger : hand\n",
      "toe : foot :: finger : kept\n"
     ]
    }
   ],
   "source": [
    "# 관계 5 : 부분-전체 관계 \n",
    "compute_and_print_analogy('toe', 'foot', 'finger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba100d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "talk : communicate :: read : typed\n",
      "talk : communicate :: read : correctly\n"
     ]
    }
   ],
   "source": [
    "# 관계 6 : 방식 차이\n",
    "compute_and_print_analogy('talk', 'communicate', 'read')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea061ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blue : democrat :: red : republican\n",
      "blue : democrat :: red : congressman\n",
      "blue : democrat :: red : senator\n"
     ]
    }
   ],
   "source": [
    "# 관계 7 : 전체 의미 표현(관습 /인물)\n",
    "compute_and_print_analogy('blue', 'democrat', 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed0caaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast : fastest :: young : youngest\n",
      "fast : fastest :: young : sixth\n",
      "fast : fastest :: young : fifth\n",
      "fast : fastest :: young : fourth\n"
     ]
    }
   ],
   "source": [
    "# 관계 8 : 비교급\n",
    "compute_and_print_analogy('fast', 'fastest', 'young')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91717cc",
   "metadata": {},
   "source": [
    "### 잘못된 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c694c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : king :: woman : queen\n",
      "man : king :: woman : monarch\n",
      "man : king :: woman : throne\n"
     ]
    }
   ],
   "source": [
    "compute_and_print_analogy('man', 'king', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44915938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : doctor :: woman : nurse\n",
      "man : doctor :: woman : physician\n"
     ]
    }
   ],
   "source": [
    "compute_and_print_analogy('man', 'doctor', 'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe6618",
   "metadata": {},
   "source": [
    "## 예제: CBOW 임베딩 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ad1892f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>, or the</td>\n",
       "      <td>frankenstein</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frankenstein or the modern</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frankenstein , the modern prometheus</td>\n",
       "      <td>or</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frankenstein , or modern prometheus by</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>, or the prometheus by mary</td>\n",
       "      <td>modern</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90693</th>\n",
       "      <td>newsletter to hear new ebooks .</td>\n",
       "      <td>about</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90694</th>\n",
       "      <td>to hear about ebooks .</td>\n",
       "      <td>new</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90695</th>\n",
       "      <td>hear about new .</td>\n",
       "      <td>ebooks</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90696</th>\n",
       "      <td>about new ebooks</td>\n",
       "      <td>.</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90697</th>\n",
       "      <td>new ebooks .</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      context        target  split\n",
       "0                                    , or the  frankenstein  train\n",
       "1                  frankenstein or the modern             ,  train\n",
       "2        frankenstein , the modern prometheus            or  train\n",
       "3      frankenstein , or modern prometheus by           the  train\n",
       "4                 , or the prometheus by mary        modern  train\n",
       "...                                       ...           ...    ...\n",
       "90693         newsletter to hear new ebooks .         about   test\n",
       "90694                 to hear about ebooks .            new   test\n",
       "90695                       hear about new .         ebooks   test\n",
       "90696                       about new ebooks              .   test\n",
       "90697                            new ebooks .           NaN   test\n",
       "\n",
       "[90698 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"frankenstein_with_splits.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa05145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 다시 train/valid/test로 나눠줌\n",
    "\n",
    "# train 데이터 \n",
    "train_df = df[df.split=='train']\n",
    "train_size = len(train_df)\n",
    "\n",
    "# valid 데이터 \n",
    "val_df = df[df.split=='val']\n",
    "val_size = len(val_df)\n",
    "\n",
    "# test 데이터 \n",
    "test_df = df[df.split=='test']\n",
    "test_size = len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f78acf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dict = {'train': (train_df, train_size),\n",
    "                             'val': (val_df, val_size),\n",
    "                             'test': (test_df, test_size)}\n",
    "# lookup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5109dd35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>target</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>, or the</td>\n",
       "      <td>frankenstein</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frankenstein or the modern</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frankenstein , the modern prometheus</td>\n",
       "      <td>or</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>frankenstein , or modern prometheus by</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>, or the prometheus by mary</td>\n",
       "      <td>modern</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63484</th>\n",
       "      <td>i had feelings affection , and</td>\n",
       "      <td>of</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63485</th>\n",
       "      <td>had feelings of , and they</td>\n",
       "      <td>affection</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63486</th>\n",
       "      <td>feelings of affection and they were</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63487</th>\n",
       "      <td>of affection , they were requited</td>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63488</th>\n",
       "      <td>affection , and were requited by</td>\n",
       "      <td>they</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63489 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      context        target  split\n",
       "0                                    , or the  frankenstein  train\n",
       "1                  frankenstein or the modern             ,  train\n",
       "2        frankenstein , the modern prometheus            or  train\n",
       "3      frankenstein , or modern prometheus by           the  train\n",
       "4                 , or the prometheus by mary        modern  train\n",
       "...                                       ...           ...    ...\n",
       "63484          i had feelings affection , and            of  train\n",
       "63485              had feelings of , and they     affection  train\n",
       "63486     feelings of affection and they were             ,  train\n",
       "63487       of affection , they were requited           and  train\n",
       "63488        affection , and were requited by          they  train\n",
       "\n",
       "[63489 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d30142",
   "metadata": {},
   "source": [
    "### 2. Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b833a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Counter()를 통해 어떤 단어가 얼만큼의 횟수로 들어있는지를 알 수 있다.\n",
    "word_counts = Counter()\n",
    "for name_text in df.context:\n",
    "    for word in name_text.split(\" \"):\n",
    "        # word가 .(구두점,punctuation)이 아닐 경우 word에 추가\n",
    "        if word not in string.punctuation:\n",
    "            word_counts[word] += 1\n",
    "\n",
    "# word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fae986f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_unk=True를 하면 '<UNK>': 0 토큰을 추가해줌 !\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, mask_token=\"<MASK>\",add_unk=True):\n",
    "        self.token_to_idx = {}\n",
    "        self.idx_to_token = {}\n",
    "        \n",
    "        self.mask_index = self.add_token(mask_token)\n",
    "#         \"UNK\" 토큰이 추가되지 않는 경우에는 -1로 설정,\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "#         \"UNK\" 토큰이 추가될 경우에는 UNK에 해당하는 인덱스로 설정,\n",
    "            self.unk_index = self.add_token('<UNK>') \n",
    "\n",
    "    def add_token(self, token):\n",
    "        \n",
    "#       만약 해당 토큰이 있으면 토큰 idx만 return\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "            \n",
    "#       만약 해당 토큰이 없으면 새로운 토큰 만들어줌\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9daef709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary 객체 생성\n",
    "\n",
    "cbow_vocab = Vocabulary()\n",
    "for index, row in df.iterrows():\n",
    "    for token in row.context.split(' '):\n",
    "        cbow_vocab.add_token(token)\n",
    "    cbow_vocab.add_token(row.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "714ba3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<MASK>': 0, '<UNK>': 1, ',': 2, 'or': 3, 'the': 4}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(cbow_vocab.token_to_idx.items())[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "59bd61b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<MASK>', 1: '<UNK>', 2: ',', 3: 'or', 4: 'the'}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(cbow_vocab.idx_to_token.items())[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ee407f",
   "metadata": {},
   "source": [
    "## 3. Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "166fd3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 토큰에 대응하는 인덱스 반환\n",
    "\n",
    "def lookup_token(vocabulary_class,token):\n",
    "\n",
    "# UNK 토큰이 있을 경우\n",
    "    if vocabulary_class.unk_index >= 0:\n",
    "#           토큰을 찾아보고 없으면 unk_index 반환, 있으면 해당 토큰의 idx를 반환\n",
    "        return vocabulary_class.token_to_idx.get(token, vocabulary_class.unk_index)\n",
    "    else:\n",
    "        return vocabulary_class.token_to_idx[token]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db2d52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 인덱스에 대응하는 토큰 반환\n",
    "\n",
    "def lookup_index(vocabulary_class, index):\n",
    "        if index not in vocabulary_class.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return vocabulary_class.idx_to_token[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6853d180",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1764d004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[215   1   1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def vectorize(context, vector_length=-1):\n",
    "    \n",
    "    indices = [lookup_token(cbow_vocab,token) for token in context.split(' ')]\n",
    "    if vector_length < 0:\n",
    "        vector_length = len(indices)\n",
    "    \n",
    "#     전체 단어 사이즈만큼을 미리 0으로 채워둠\n",
    "    one_hot = np.zeros(len(cbow_vocab.token_to_idx), dtype=np.float32)\n",
    "    one_hot\n",
    "    \n",
    "    \n",
    "    out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "    out_vector[:len(indices)] = indices\n",
    "#     문장 길이가 작아서 padding 진행하면 해당 부분 mask 처리\n",
    "    out_vector[len(indices):] = cbow_vocab.mask_index\n",
    "    \n",
    "    \n",
    "    return out_vector\n",
    "\n",
    "print(vectorize(\"all dafs dfkdl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff427e2",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d828249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, cbow_df):\n",
    "        \n",
    "        self.cbow_df = cbow_df\n",
    "        \n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self.max_seq_length = max(map(measure_len, cbow_df.context))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cbow_df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        row = self.cbow_df.iloc[index]\n",
    "\n",
    "        context_vector = vectorize(row.context, self.max_seq_length)\n",
    "        target_index = lookup_token(cbow_vocab,row.target)\n",
    "\n",
    "        return {'x_data': context_vector,\n",
    "                'y_target': target_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70ee0b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.CBOWDataset at 0x7fb8f18f5940>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋을 인스턴스화 해주어야 로더에 넣어줄 수 있다. \n",
    "\n",
    "train_dataset = CBOWDataset(train_df)\n",
    "train_dataset\n",
    "\n",
    "valid_dataset = CBOWDataset(val_df)\n",
    "valid_dataset\n",
    "\n",
    "test_dataset = CBOWDataset(test_df)\n",
    "test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "313b3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더 설정\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# drop_last=True -> 배치 사이즈보다 over하면 drop\n",
    "\n",
    "Traindataloader = DataLoader(dataset=train_dataset, batch_size=512,\n",
    "                            shuffle=True, drop_last=True)\n",
    "\n",
    "Validdataloader = DataLoader(dataset=valid_dataset, batch_size=512,\n",
    "                            shuffle=False, drop_last=True)\n",
    "\n",
    "Testdataloader = DataLoader(dataset=test_dataset, batch_size=512,\n",
    "                            shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ca93111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63489 124\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset),len(Traindataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d2bb22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'x_data': tensor([[ 356, 5006,    2,  422, 1921,   49],\n",
      "        [3584,   53, 2545,    0,    0,    0],\n",
      "        [  37,  888,  859,   63,  125, 5637],\n",
      "        ...,\n",
      "        [ 962,   27,  226,   90,   85, 1934],\n",
      "        [  92,  211, 2049,   49,   37, 5644],\n",
      "        [3872,    8,   34,   33, 4306, 1354]]), 'y_target': tensor([   4,   44,   48,    2,   33,  255, 2130,  496, 1860,   44,    2, 1248,\n",
      "         213,  249,  830,  294,  592,   60, 1404,    2,  471,   44,    2,   60,\n",
      "         721,   41,   33, 1191,   72,   26,   49,   48,   44,   15,   85, 2038,\n",
      "         136,  212,   33,   36,   69,   19, 1584, 3710,   19,   50,   19,   33,\n",
      "         237,   60, 4868,   15, 1179, 2097, 3227,  137,  568, 1648, 3662, 1443,\n",
      "         463,   96,   44,    2,  682,   33, 1383,   33,    2,    4,  494, 2632,\n",
      "         136,    4,   50, 3998, 2532,   15,  542,    4,  766,   60,   48,   33,\n",
      "         420,  273,   44,   18,   60,   27,    8, 1897, 3507,   33,   33,    2,\n",
      "          33,  174,   48,  345,   15, 3041, 1132,  766,  104,   51,  365,   27,\n",
      "          48, 1854,   60,  358,   49,  788,  598,  160,   47, 2114,  334, 3908,\n",
      "        5371, 4360, 1802,   49,   82,  529,  173, 3055,   15,   48,    2,    2,\n",
      "          33,  780,  620,  983,  932,  151,   15,   36,  552, 3535,    8,   44,\n",
      "          90, 1466,    2,   60,  334,   15,   33,  358,  399,  326,   68,  239,\n",
      "           2,    2,    3,  412,  365,   82, 1710,  551,   44,   82,   19, 2057,\n",
      "           2,    4,  120,  376,  181,   50, 2278, 1443, 1192, 2733,  792,   90,\n",
      "           2, 1078,   15,   48, 2853, 3329, 1894,   49,   50,    4,    4,   90,\n",
      "          48,   36,   34,   36,    2,  577,  226,   23,    4,  497, 3485,   48,\n",
      "          49,   33,  908, 3239,    2,   48,   49, 1243,   60,  225,  797, 3174,\n",
      "        3754,  620, 3309,  319, 1533,   15, 4997,   65,    2,  768,  578,  605,\n",
      "          14,   48,   44, 3779,   19,    2,   15,   48,  756,    8,  239, 1762,\n",
      "        2426,   27,    4,  173,   49, 1072,   39,  598,   49,  474,   77, 4782,\n",
      "           4,  209,   48,   15,   33,   48,   33, 1033,  204,   49,  334,    2,\n",
      "          48,   60,   33, 5543, 2632,    4, 2709,  226,  230,    4,  992, 1369,\n",
      "         496,  909,  715,   50,   28,   49,   60,   33,    4, 5858,   50,    2,\n",
      "           2, 2107,  817,   33,    2,    2,   53,  319,   15, 2321,  104, 1192,\n",
      "         915, 1141,  239, 1564,  356,   15,   15, 3406,   15, 3230, 5831,   48,\n",
      "         396,   49,   49,  175,    4,  136,    4, 1725,    2,  583,  264, 3229,\n",
      "         278, 1154,   72,    4,   15,  792,  563, 1810, 4001,  151,   15,  334,\n",
      "          72,   50,  788,   15,   72,    2, 4958,    2,   33, 5478, 1307,   72,\n",
      "          33, 5081,    4, 1376, 4475,   33,   60,  830,   33,   48,   44,   27,\n",
      "          19,  178, 2738,   48, 3856,    2, 1775,   44,  395,  275, 2554,  173,\n",
      "          60,    2,   85, 3918,  319, 4466,    2,   33,    4,   48,    2, 1734,\n",
      "         766,   19,   36, 2391,    4,  255,   49,   33,  255, 4006,  173, 1884,\n",
      "          27, 1668,   44,  319, 3361,  774,  925, 1080,   49,    2,   15,   60,\n",
      "         389,  849,  603,   15, 5792, 1463, 2222, 4189,   33,   15, 1348,    4,\n",
      "         289,    2,  549,  605,   24, 4298,   96,   44,   48,  573,    2,   49,\n",
      "           2,  592,  179,   27,   48, 3781,   49,  226,    4,    4, 1973,   33,\n",
      "        2501,   37,   19, 1384,  394,   19,   49,  334,   56,  417,   60,   36,\n",
      "         127,   72,   19,    2,  529,   72,   44, 5123,   40,   44,  360,   49,\n",
      "          49,   48, 5939,   44, 1045,   19,   60, 5557,  226, 2627,   19,   49,\n",
      "         173,    2,  732,   48, 2555, 5169,  115, 2514,   44,  301,  337, 1743,\n",
      "         484,  321, 2024,   82,    2,   23, 1445, 2844,  492,   44,   36, 1082,\n",
      "        3616,   19,   27,   48, 2008, 1859,    2, 1126])}\n"
     ]
    }
   ],
   "source": [
    "for batch_index, batch_dict in enumerate(Traindataloader):\n",
    "    print(batch_index)\n",
    "    print(batch_dict)\n",
    "    \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ad0f0",
   "metadata": {},
   "source": [
    "### 모델정의 NameClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc70d742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn은 neural network로 torch의 신경망 모듈이다.\n",
    "# MLP사용 \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CBOWClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    매개변수:\n",
    "        vocabulary_size (int): 어휘 사전 크기, 임베딩 개수와 예측 벡터 크기를 결정합니다\n",
    "        embedding_size (int): 임베딩 크기\n",
    "        padding_idx (int): 기본값 0; 임베딩은 이 인덱스를 사용하지 않습니다\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocabulary_size, embedding_size, padding_idx=0):\n",
    "        \n",
    "#       torch.nn.Module의 초기화 메서드를 실행하여 해당 클래스의 기능을 상속받음\n",
    "        super(CBOWClassifier, self).__init__()\n",
    "        # Embedding layer를 이용해서 문맥의 단어를 나타내는 인덱스를 각 단어에 대한 벡터로 만듦\n",
    "        # 전반적인 문맥을 감지하도록 벡터를 결합함(sum)\n",
    "        self.embedding =  nn.Embedding(num_embeddings=vocabulary_size, \n",
    "                                       embedding_dim=embedding_size,\n",
    "                                       padding_idx=padding_idx)\n",
    "        self.fc1 = nn.Linear(in_features=embedding_size,\n",
    "                             out_features=vocabulary_size)\n",
    "    \n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        x_embedded_sum = F.dropout(self.embedding(x_in).sum(dim=1), 0.3)\n",
    "        y_out = self.fc1(x_embedded_sum)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "            \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84bbff40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOWClassifier(\n",
       "  (embedding): Embedding(7270, 50, padding_idx=0)\n",
       "  (fc1): Linear(in_features=50, out_features=7270, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size=50 \n",
    "\n",
    "classifier = CBOWClassifier(vocabulary_size=len(cbow_vocab.token_to_idx), \n",
    "                            embedding_size=embedding_size)\n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19041c2",
   "metadata": {},
   "source": [
    "### 옵티마이저, loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d3d0c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7f39ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 옵티마이저\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = lr)\n",
    "optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d18b666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "loss_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b010d",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6fe28b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "#      예측값과 타겟값을 비교하여 일치하는 개수를 계산\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f63b0979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train state 초기화 \n",
    "def make_train_state():\n",
    "    return {\n",
    "        'stop_early':False,\n",
    "        'early_stopping_step':0,\n",
    "        'early_stopping_best_val':1e8,\n",
    "        'early_stopping_criteria' : 10,\n",
    "        'epoch_index' : 0,\n",
    "        'train_loss': [], \n",
    "        'train_acc' :[], \n",
    "        'val_loss' : [],\n",
    "        'val_acc' : [], \n",
    "        'test_loss' : [],\n",
    "        'test_acc' : [],\n",
    "         \n",
    "#       모델 저장파일\n",
    "        'model_filename' : 'model.pth'\n",
    "    } \n",
    "\n",
    "\n",
    "# Train update \n",
    "def update_train_state(model, train_state):\n",
    "    \n",
    "#   학습시작하면 초기에 모델 저장하기 \n",
    "    \n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(),train_state['model_filename'])\n",
    "        \n",
    "#   모델 성능이 향상되면 모델 저장(valid loss가 더 낮아지면)\n",
    "    elif train_state['epoch_index'] >=1 :\n",
    "        loss_t = train_state['val_loss'][-1]\n",
    "#        loss가 나빠지면 early stop step 업데이트\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            train_state['early_stopping_step']+=1\n",
    "            \n",
    "#        loss가 좋아지면   \n",
    "        else:\n",
    "#            early stop step 0으로 다시 초기화        \n",
    "            train_state['early_stopping_step']=0\n",
    "    \n",
    "#           최저 loss이면 모델 저장 \n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "                torch.save(model.state_dict(),train_state['model_filename'])\n",
    "\n",
    "#       기준점 넘으면 early stop \n",
    "        if train_state['early_stopping_step'] >= train_state['early_stopping_criteria']:\n",
    "            train_state['stop_early'] = True\n",
    "        \n",
    "        return train_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2032c064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop_early': False,\n",
       " 'early_stopping_step': 0,\n",
       " 'early_stopping_best_val': 100000000.0,\n",
       " 'early_stopping_criteria': 10,\n",
       " 'epoch_index': 0,\n",
       " 'train_loss': [],\n",
       " 'train_acc': [],\n",
       " 'val_loss': [],\n",
       " 'val_acc': [],\n",
       " 'test_loss': [],\n",
       " 'test_acc': [],\n",
       " 'model_filename': 'model.pth'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 진행 상황 함수 초기화\n",
    "train_state = make_train_state()\n",
    "train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39f92d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                          | 1/100 [00:12<21:21, 12.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 7.004987111458411\n",
      "val_acc 10.073617788461538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▊                                          | 2/100 [00:25<20:54, 12.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.7591050404768716\n",
      "val_acc 11.523437499999996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|█▎                                         | 3/100 [00:38<20:45, 12.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.653256489680364\n",
      "val_acc 12.372295673076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|█▋                                         | 4/100 [00:51<20:56, 13.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.572822075623732\n",
      "val_acc 12.49248798076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|██▏                                        | 5/100 [01:04<20:28, 12.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.517270711752085\n",
      "val_acc 12.725360576923075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|██▌                                        | 6/100 [01:17<20:13, 12.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.476829748887282\n",
      "val_acc 13.326322115384615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|███                                        | 7/100 [01:30<19:57, 12.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.468164664048415\n",
      "val_acc 13.529146634615385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|███▍                                       | 8/100 [01:43<19:49, 12.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.439299528415387\n",
      "val_acc 13.574218750000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███▊                                       | 9/100 [01:57<20:09, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.437481366671049\n",
      "val_acc 13.837139423076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▏                                     | 10/100 [02:10<19:41, 13.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.421062671221219\n",
      "val_acc 13.979867788461538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████▌                                     | 11/100 [02:23<19:19, 13.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.44003750727727\n",
      "val_acc 14.107572115384615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████                                     | 12/100 [02:35<19:03, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.438934858028706\n",
      "val_acc 14.197716346153848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█████▍                                    | 13/100 [02:49<18:58, 13.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.446268540162307\n",
      "val_acc 14.085036057692308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█████▉                                    | 14/100 [03:02<18:38, 13.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.4550894407125625\n",
      "val_acc 14.27283653846154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|██████▎                                   | 15/100 [03:14<18:18, 12.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.459408980149491\n",
      "val_acc 14.25030048076923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|██████▋                                   | 16/100 [03:27<18:01, 12.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.47457896746122\n",
      "val_acc 14.212740384615383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████▏                                  | 17/100 [03:40<17:58, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.474959520193247\n",
      "val_acc 14.400540865384617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|███████▌                                  | 18/100 [03:54<17:55, 13.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.507918027731089\n",
      "val_acc 14.445612980769232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████▉                                  | 19/100 [04:07<17:38, 13.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.481477975845337\n",
      "val_acc 14.53575721153846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████▉                                  | 19/100 [04:19<18:28, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 6.511375830723689\n",
      "val_acc 14.242788461538462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "# 에포크만큼\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "\n",
    "#     print('epoch',epoch)\n",
    "#     print(train_state['epoch_index']) \n",
    "    train_state['epoch_index'] +=1 \n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "\n",
    "#     모델을 학습 모드로 설정 -> 드롭아웃 및 배치 정규화와 같은 학습 중에만 적용되는 기법들이 활성화\n",
    "#     모델을 평가 모드로 전환하려면 classifier.eval()을 사용\n",
    "    classifier.train()\n",
    "# 배치 만큼\n",
    "    for batch_idx, batch_data in enumerate(Traindataloader):\n",
    "\n",
    "        \n",
    "\n",
    "#       1. 옵티마이저 그레디언트 0으로 초기화\n",
    "        optimizer.zero_grad()\n",
    "#       2. 모델에 데이터 넣어서 출력받기\n",
    "        y_pred = classifier(x_in=batch_data['x_data'])\n",
    "#       3. loss 계산하기\n",
    "        loss =  loss_func(y_pred, batch_data['y_target'])\n",
    "    \n",
    "#       tensor(0.3190) -> 0.3190, item()으로 스칼라 값만 추출\n",
    "        loss_t = loss.item()\n",
    "\n",
    "#       배치에서의 평균 loss 구하기\n",
    "        running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "\n",
    "#       4. gradient 계산하기\n",
    "        loss.backward()\n",
    "\n",
    "#       5. 옵티마이저 가중치 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "#       Accuracy 계산\n",
    "        acc_t = compute_accuracy(y_pred, batch_data['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "\n",
    "\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "\n",
    "\n",
    "#   valid에 대한 계산\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "    classifier.eval() # 모델 파라미터를 수정하지 못 하게 비활성화\n",
    "\n",
    "    for batch_idx, batch_data in enumerate(Validdataloader):\n",
    "\n",
    "#       1. 모델의 출력값(y_pred)계산\n",
    "        y_pred = classifier(x_in=batch_data['x_data'])\n",
    "\n",
    "#       2. loss 계산\n",
    "        loss = loss_func(y_pred,batch_data['y_target'])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "\n",
    "#       3. Accuracy 계산\n",
    "        acc_t = compute_accuracy(y_pred,batch_data['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "    \n",
    "    print(\"val_loss\",running_loss)\n",
    "    print(\"val_acc\",running_acc)\n",
    "\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "    \n",
    "\n",
    "#   전체 loss, acc 저장\n",
    "    train_state = update_train_state(model=classifier,\n",
    "                                     train_state=train_state)\n",
    "#   early stop해라고 했으면 학습 멈추기    \n",
    "    if train_state['stop_early']:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029ccaa3",
   "metadata": {},
   "source": [
    "### Test 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "51c802ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 좋은 모델을 사용해 테스트 세트의 손실과 정확도를 계산합니다\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "\n",
    "# 가중치 업데이트 하지 못 하게\n",
    "classifier.eval()\n",
    "\n",
    "for batch_idx, batch_data in enumerate(Testdataloader):\n",
    "    \n",
    "    y_pred = classifier(x_in=batch_data['x_data'])\n",
    "    loss = loss_func(y_pred,batch_data['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "    \n",
    "    acc_t = compute_accuracy(y_pred, batch_data['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e6d1908b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 7.323\n",
      "테스트 정확도: 13.48\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 손실: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {:.2f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67e7d2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop_early': True,\n",
       " 'early_stopping_step': 10,\n",
       " 'early_stopping_best_val': 6.421062671221219,\n",
       " 'early_stopping_criteria': 10,\n",
       " 'epoch_index': 22,\n",
       " 'train_loss': [8.769133717783035,\n",
       "  7.03628062817358,\n",
       "  6.522220976891057,\n",
       "  6.20068036740826,\n",
       "  5.971037410920665,\n",
       "  5.795840478712515,\n",
       "  5.647560573393301,\n",
       "  5.522859184972702,\n",
       "  5.4152467443097025,\n",
       "  5.315567604957089,\n",
       "  5.231619077344095,\n",
       "  5.151370709942233,\n",
       "  5.076750001599714,\n",
       "  5.007491900074864,\n",
       "  4.949088873401769,\n",
       "  4.883807874494986,\n",
       "  4.835539360200204,\n",
       "  4.781407860017592,\n",
       "  4.724345587914989,\n",
       "  4.685608140883904,\n",
       "  4.6482952602448],\n",
       " 'train_acc': [2.3988785282258074,\n",
       "  9.308845766129037,\n",
       "  11.151713709677422,\n",
       "  12.34721522177419,\n",
       "  12.9914314516129,\n",
       "  13.545866935483872,\n",
       "  13.788432459677418,\n",
       "  14.175907258064516,\n",
       "  14.650012600806448,\n",
       "  14.903603830645155,\n",
       "  15.251701108870966,\n",
       "  15.500567036290326,\n",
       "  15.905367943548383,\n",
       "  16.204637096774185,\n",
       "  16.53855846774193,\n",
       "  17.097719254032256,\n",
       "  17.357610887096783,\n",
       "  17.80021421370969,\n",
       "  18.261718749999993,\n",
       "  18.502709173387093,\n",
       "  18.902784778225804],\n",
       " 'val_loss': [7.753033362902128,\n",
       "  7.004987111458411,\n",
       "  6.7591050404768716,\n",
       "  6.653256489680364,\n",
       "  6.572822075623732,\n",
       "  6.517270711752085,\n",
       "  6.476829748887282,\n",
       "  6.468164664048415,\n",
       "  6.439299528415387,\n",
       "  6.437481366671049,\n",
       "  6.421062671221219,\n",
       "  6.44003750727727,\n",
       "  6.438934858028706,\n",
       "  6.446268540162307,\n",
       "  6.4550894407125625,\n",
       "  6.459408980149491,\n",
       "  6.47457896746122,\n",
       "  6.474959520193247,\n",
       "  6.507918027731089,\n",
       "  6.481477975845337,\n",
       "  6.511375830723689],\n",
       " 'val_acc': [6.820913461538463,\n",
       "  10.073617788461538,\n",
       "  11.523437499999996,\n",
       "  12.372295673076923,\n",
       "  12.49248798076923,\n",
       "  12.725360576923075,\n",
       "  13.326322115384615,\n",
       "  13.529146634615385,\n",
       "  13.574218750000002,\n",
       "  13.837139423076923,\n",
       "  13.979867788461538,\n",
       "  14.107572115384615,\n",
       "  14.197716346153848,\n",
       "  14.085036057692308,\n",
       "  14.27283653846154,\n",
       "  14.25030048076923,\n",
       "  14.212740384615383,\n",
       "  14.400540865384617,\n",
       "  14.445612980769232,\n",
       "  14.53575721153846,\n",
       "  14.242788461538462],\n",
       " 'test_loss': 7.32296763933622,\n",
       " 'test_acc': 13.476562499999998,\n",
       " 'model_filename': 'model.pth'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0d3792a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(results):\n",
    "    \"\"\"\n",
    "    임베딩 결과를 출력합니다\n",
    "    \"\"\"\n",
    "    for item in results:\n",
    "        print (\"...[%.2f] - %s\"%(item[1], item[0]))\n",
    "\n",
    "def get_closest(target_word, word_to_idx, embeddings, n=5):\n",
    "    \"\"\"\n",
    "    n개의 최근접 단어를 찾습니다.\n",
    "    \"\"\"\n",
    "\n",
    "    # 다른 모든 단어까지 거리를 계산합니다\n",
    "    word_embedding = embeddings[word_to_idx[target_word.lower()]]\n",
    "    distances = []\n",
    "    for word, index in word_to_idx.items():\n",
    "        if word == \"<MASK>\" or word == target_word:\n",
    "            continue\n",
    "        distances.append((word, torch.dist(word_embedding, embeddings[index])))\n",
    "    \n",
    "    results = sorted(distances, key=lambda x: x[1])[1:n+2]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f16e8dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어를 입력해 주세요: monster\n",
      "...[6.43] - recourse\n",
      "...[6.44] - disclose\n",
      "...[6.50] - perceptibly\n",
      "...[6.54] - affectionate\n",
      "...[6.60] - began\n",
      "...[6.60] - survive\n"
     ]
    }
   ],
   "source": [
    "word = input('단어를 입력해 주세요: ')\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = cbow_vocab.token_to_idx\n",
    "pretty_print(get_closest(word, word_to_idx, embeddings, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b47d05e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======frankenstein=======\n",
      "...[6.68] - recurrence\n",
      "...[6.84] - robert\n",
      "...[6.88] - consoled\n",
      "...[7.02] - tolerably\n",
      "...[7.02] - catching\n",
      "...[7.05] - ideas\n",
      "=======monster=======\n",
      "...[6.43] - recourse\n",
      "...[6.44] - disclose\n",
      "...[6.50] - perceptibly\n",
      "...[6.54] - affectionate\n",
      "...[6.60] - began\n",
      "...[6.60] - survive\n",
      "=======science=======\n",
      "...[6.34] - orkneys\n",
      "...[6.38] - somewhat\n",
      "...[6.57] - sicken\n",
      "...[6.64] - spent\n",
      "...[6.70] - theory\n",
      "...[6.80] - entertained\n",
      "=======sickness=======\n",
      "...[6.56] - else\n",
      "...[6.61] - tolerably\n",
      "...[6.65] - sadness\n",
      "...[6.67] - stolen\n",
      "...[6.72] - collections\n",
      "...[6.77] - asleep\n",
      "=======lonely=======\n",
      "...[6.49] - early\n",
      "...[6.55] - muttered\n",
      "...[6.58] - large\n",
      "...[6.60] - d\n",
      "...[6.63] - cheerfulness\n",
      "...[6.64] - brink\n",
      "=======happy=======\n",
      "...[6.28] - surrounded\n",
      "...[6.49] - sledge\n",
      "...[6.50] - imagine\n",
      "...[6.59] - strangled\n",
      "...[6.62] - sharing\n",
      "...[6.63] - appointment\n"
     ]
    }
   ],
   "source": [
    "target_words = ['frankenstein', 'monster', 'science', 'sickness', 'lonely', 'happy']\n",
    "\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = cbow_vocab.token_to_idx\n",
    "\n",
    "for target_word in target_words: \n",
    "    print(f\"======={target_word}=======\")\n",
    "    if target_word not in word_to_idx:\n",
    "        print(\"Not in vocabulary\")\n",
    "        continue\n",
    "    pretty_print(get_closest(target_word, word_to_idx, embeddings, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d25f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
