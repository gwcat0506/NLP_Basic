{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a0c5d8d",
   "metadata": {},
   "source": [
    "### MLP 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1ed1dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super(MultilayerPerceptron,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \n",
    "#         relu 활성화 함수를 통해 비선형성을 추가\n",
    "        intermediate = F.relu(self.fc1(x_in))\n",
    "        output = self.fc2(intermediate)\n",
    "        \n",
    "#         softmax 적용여부 \n",
    "        if apply_softmax:\n",
    "#           dim = 행렬의 각 행에 대해 소프트맥스 함수를 적용\n",
    "            output = F.softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c3f39e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerPerceptron(\n",
      "  (fc1): Linear(in_features=3, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# example1\n",
    "\n",
    "input_dim = 3\n",
    "hidden_dim = 100\n",
    "output_dim = 4\n",
    "\n",
    "mlp = MultilayerPerceptron(input_dim,hidden_dim,output_dim)\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759e9b16",
   "metadata": {},
   "source": [
    "# 예제: MLP로 성씨 분류하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b451dde",
   "metadata": {},
   "source": [
    "### 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "962715ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nationality</th>\n",
       "      <th>nationality_index</th>\n",
       "      <th>split</th>\n",
       "      <th>surname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Totah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Abboud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Fakhoury</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Srour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>15</td>\n",
       "      <td>train</td>\n",
       "      <td>Sayegh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10975</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>11</td>\n",
       "      <td>test</td>\n",
       "      <td>Dinh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10976</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>11</td>\n",
       "      <td>test</td>\n",
       "      <td>Phung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10977</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>11</td>\n",
       "      <td>test</td>\n",
       "      <td>Quang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10978</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>11</td>\n",
       "      <td>test</td>\n",
       "      <td>Vu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10979</th>\n",
       "      <td>Vietnamese</td>\n",
       "      <td>11</td>\n",
       "      <td>test</td>\n",
       "      <td>Ha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10980 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      nationality  nationality_index  split   surname\n",
       "0          Arabic                 15  train     Totah\n",
       "1          Arabic                 15  train    Abboud\n",
       "2          Arabic                 15  train  Fakhoury\n",
       "3          Arabic                 15  train     Srour\n",
       "4          Arabic                 15  train    Sayegh\n",
       "...           ...                ...    ...       ...\n",
       "10975  Vietnamese                 11   test      Dinh\n",
       "10976  Vietnamese                 11   test     Phung\n",
       "10977  Vietnamese                 11   test     Quang\n",
       "10978  Vietnamese                 11   test        Vu\n",
       "10979  Vietnamese                 11   test        Ha\n",
       "\n",
       "[10980 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"surnames_with_splits.csv\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ac5f9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nationality\n",
       "English       2972\n",
       "Russian       2373\n",
       "Arabic        1603\n",
       "Japanese       775\n",
       "Italian        600\n",
       "German         576\n",
       "Czech          414\n",
       "Spanish        258\n",
       "Dutch          236\n",
       "French         229\n",
       "Chinese        220\n",
       "Irish          183\n",
       "Greek          156\n",
       "Polish         120\n",
       "Korean          77\n",
       "Scottish        75\n",
       "Vietnamese      58\n",
       "Portuguese      55\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['nationality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4de82687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split\n",
       "train    7680\n",
       "test     1660\n",
       "val      1640\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['split'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1765de80",
   "metadata": {},
   "source": [
    "### 데이터 split(train/valid/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8800055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 다시 train/valid/test로 나눠줌\n",
    "\n",
    "# train 데이터 \n",
    "train_df = df[df.split=='train']\n",
    "train_size = len(train_df)\n",
    "\n",
    "# valid 데이터 \n",
    "val_df = df[df.split=='val']\n",
    "val_size = len(val_df)\n",
    "\n",
    "# test 데이터 \n",
    "test_df = df[df.split=='test']\n",
    "test_size = len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe44af",
   "metadata": {},
   "source": [
    "### 2. Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "754eccbf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "# Counter()를 통해 어떤 단어가 얼만큼의 횟수로 들어있는지를 알 수 있다.\n",
    "word_counts = Counter()\n",
    "for name_text in df.surname:\n",
    "    for word in name_text.split(\" \"):\n",
    "        # word가 .(구두점,punctuation)이 아닐 경우 word에 추가\n",
    "        if word not in string.punctuation:\n",
    "            word_counts[word] += 1\n",
    "\n",
    "# word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af19c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_unk=True를 하면 '<UNK>': 0 토큰을 추가해줌 !\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, add_unk=False):\n",
    "        self.token_to_idx = {}\n",
    "        self.idx_to_token = {}\n",
    "    \n",
    "#         \"UNK\" 토큰이 추가되지 않는 경우에는 -1로 설정,\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "#         \"UNK\" 토큰이 추가될 경우에는 UNK에 해당하는 인덱스로 설정,\n",
    "            self.unk_index = self.add_token('<UNK>') \n",
    "\n",
    "    def add_token(self, token):\n",
    "        \n",
    "#       만약 해당 토큰이 있으면 토큰 idx만 return\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "            \n",
    "#       만약 해당 토큰이 없으면 새로운 토큰 만들어줌\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db8e8fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutoff 보다 수가 많은 단어만 vocab에 추가\n",
    "cutoff = 0\n",
    "\n",
    "# Vocabulary 객체 생성\n",
    "# cutoff보다 작으면 unk토큰으로 지정해줄 것이기 때문에 True\n",
    "name_vocab = Vocabulary(add_unk=True)\n",
    "\n",
    "# word_counts.items() -> ex) ('all', 24160)\n",
    "for word, count in word_counts.items():\n",
    "    if count > cutoff:\n",
    "        name_vocab.add_token(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09ed7e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<UNK>': 0, 'Totah': 1, 'Abboud': 2, 'Fakhoury': 3, 'Srour': 4}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(name_vocab.token_to_idx.items())[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0834a47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<UNK>', 1: 'Totah', 2: 'Abboud', 3: 'Fakhoury', 4: 'Srour'}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(name_vocab.idx_to_token.items())[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe1c62",
   "metadata": {},
   "source": [
    "### 국적 Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8374bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Vocabulary at 0x7f8801c7fd60>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nation_vocab = Vocabulary(add_unk=False)\n",
    "nation_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ff6c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 국적 Vocabulary 추가 \n",
    "\n",
    "for nation in sorted(set(df.nationality)):\n",
    "    nation_vocab.add_token(nation)\n",
    "#     print(nation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63795027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arabic': 0,\n",
       " 'Chinese': 1,\n",
       " 'Czech': 2,\n",
       " 'Dutch': 3,\n",
       " 'English': 4,\n",
       " 'French': 5,\n",
       " 'German': 6,\n",
       " 'Greek': 7,\n",
       " 'Irish': 8,\n",
       " 'Italian': 9,\n",
       " 'Japanese': 10,\n",
       " 'Korean': 11,\n",
       " 'Polish': 12,\n",
       " 'Portuguese': 13,\n",
       " 'Russian': 14,\n",
       " 'Scottish': 15,\n",
       " 'Spanish': 16,\n",
       " 'Vietnamese': 17}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nation_vocab.token_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2bdc1da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Arabic', 1: 'Chinese', 2: 'Czech', 3: 'Dutch', 4: 'English'}\n"
     ]
    }
   ],
   "source": [
    "print(dict(list(nation_vocab.idx_to_token.items())[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e05a07",
   "metadata": {},
   "source": [
    "## 3. Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "891b418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 토큰에 대응하는 인덱스 반환\n",
    "\n",
    "def lookup_token(vocabulary_class,token):\n",
    "\n",
    "# UNK 토큰이 있을 경우\n",
    "    if vocabulary_class.unk_index >= 0:\n",
    "#           토큰을 찾아보고 없으면 unk_index 반환, 있으면 해당 토큰의 idx를 반환\n",
    "        return vocabulary_class.token_to_idx.get(token, vocabulary_class.unk_index)\n",
    "    else:\n",
    "        return vocabulary_class.token_to_idx[token]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70f7f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 인덱스에 대응하는 토큰 반환\n",
    "\n",
    "def lookup_index(vocabulary_class, index):\n",
    "        if index not in vocabulary_class.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return vocabulary_class.idx_to_token[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6e1c43",
   "metadata": {},
   "source": [
    "### 텍스트(surname)에 대한 원 핫 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e16e3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def vectorize(voca, column):\n",
    "\n",
    "#     전체 단어 사이즈만큼을 미리 0으로 채워둠\n",
    "    one_hot = np.zeros(len(voca.token_to_idx), dtype=np.float32)\n",
    "    one_hot\n",
    "    \n",
    "    for token in column.split(\" \"):\n",
    "        \n",
    "#         토큰이 .(구두점)이 아닐 경우 \n",
    "#         토큰에 해당되는 인덱스에 1를 부여한 one_hot encoding 만듦\n",
    "        if token not in string.punctuation:\n",
    "            one_hot[lookup_token(voca,token)] = 1\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "print(vectorize(name_vocab,\"all i can say is that a i had no other option\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9475b5b",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8593f397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NameDataset(Dataset):\n",
    "    def __init__(self, names, nations):\n",
    "        self.names = names\n",
    "        self.nations = nations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        name = self.names[index]\n",
    "        nation = self.nations[index]\n",
    "        \n",
    "#         여기다가 vectorize함수 사용해서 name return\n",
    "        vectorized_name = vectorize(name_vocab,name)\n",
    "#       nation 숫자로 return\n",
    "#         vectorized_nation = vectorize(nation_vocab,nation)\n",
    "        vectorized_nation = lookup_token(nation_vocab,nation)\n",
    "    \n",
    "        return {\n",
    "            'surname': vectorized_name,\n",
    "            'nationality': vectorized_nation\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41704e7f",
   "metadata": {},
   "source": [
    "### 데이터셋 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa7d357c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.NameDataset at 0x7f8820d8a400>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋을 인스턴스화 해주어야 로더에 넣어줄 수 있다. \n",
    "\n",
    "train_dataset = NameDataset(train_df[\"surname\"].values, train_df[\"nationality\"].values)\n",
    "train_dataset\n",
    "\n",
    "valid_dataset = NameDataset(val_df[\"surname\"].values, val_df[\"nationality\"].values)\n",
    "valid_dataset\n",
    "\n",
    "test_dataset = NameDataset(test_df[\"surname\"].values, test_df[\"nationality\"].values)\n",
    "test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d0eb916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더 설정\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# drop_last=True -> 배치 사이즈보다 over하면 drop\n",
    "\n",
    "Traindataloader = DataLoader(dataset=train_dataset, batch_size=512,\n",
    "                            shuffle=True, drop_last=True)\n",
    "\n",
    "Validdataloader = DataLoader(dataset=valid_dataset, batch_size=512,\n",
    "                            shuffle=False, drop_last=True)\n",
    "\n",
    "Testdataloader = DataLoader(dataset=test_dataset, batch_size=512,\n",
    "                            shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b34a114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7680 15\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset),len(Traindataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c2bbed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'surname': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'nationality': tensor([ 0, 14,  2,  1, 14,  4,  4,  7,  9, 16,  4,  2,  2,  5, 14, 12,  6,  0,\n",
      "         4, 10,  2,  4,  6, 14, 14,  0,  2, 10,  0,  4,  4,  8,  0,  4,  4,  0,\n",
      "         4,  4,  5, 14, 10,  4,  4,  6, 14,  6,  6,  4,  0,  4,  6,  9, 10,  4,\n",
      "         4,  0,  3,  4,  4, 14,  4, 14,  0,  9,  4, 14, 10,  4,  4, 10, 14,  4,\n",
      "         4,  9, 10,  8, 14, 14,  0, 16,  4,  4,  4, 14,  0,  0,  1, 14, 14,  2,\n",
      "        14, 14,  4,  0,  0,  0, 14,  0, 14, 14,  4,  0,  5, 10, 14, 10,  0,  8,\n",
      "         0,  0,  4,  4,  4,  4, 14,  0,  0, 10, 14, 14, 14,  4, 14,  0, 14,  4,\n",
      "         4,  0,  4,  5,  4,  4,  4, 10, 14, 10, 14,  2,  6, 10, 16, 14,  4, 16,\n",
      "         6,  5,  0,  9, 14, 14,  2,  4,  2,  4,  4,  4,  1,  4,  0, 14, 11,  4,\n",
      "         4, 14,  0,  0, 10, 10,  0,  4,  4, 14, 14,  4, 10, 16,  0,  0,  2,  4,\n",
      "         0, 14,  0,  4,  4,  0, 14,  4,  0,  4,  4,  2, 14,  9,  0,  4,  4,  2,\n",
      "         0,  8,  5,  4,  2, 14, 14, 14,  4, 14,  2, 16, 10, 14,  3, 14, 10,  4,\n",
      "         5,  2,  9,  4,  6,  2,  4, 10,  4,  0, 14,  3,  0, 10, 14,  4, 14,  4,\n",
      "        17, 13, 14,  6,  2,  0,  0, 14,  4, 12,  9,  3,  4,  3,  6, 15,  2,  4,\n",
      "         0,  0, 10,  4,  0,  4, 14, 14,  4, 14,  4, 11,  6,  4,  9,  0,  0,  0,\n",
      "         6, 14, 10, 14, 14,  4, 10, 14,  4,  0,  4,  4, 14,  5,  7, 10,  4,  0,\n",
      "        10,  4,  6, 14, 10, 16, 12,  6,  0, 14,  6,  4, 14,  4,  0,  7,  6,  4,\n",
      "         0,  5,  4,  0, 14,  4,  4, 14,  6,  4,  6,  0,  0,  2, 12,  4, 10,  4,\n",
      "        10, 15,  4,  4,  4,  4,  4,  4,  4,  3,  0, 14,  0,  4, 14,  5,  9,  4,\n",
      "        14, 14,  9, 14,  0,  4, 14,  9, 14,  4, 14,  4,  4,  2, 14,  3,  4,  6,\n",
      "         4,  4, 10,  6, 14, 14,  0,  6,  4,  4, 10,  4,  2,  4,  4,  4,  2, 14,\n",
      "         0, 17,  3, 14, 15, 10, 14,  4,  6, 15, 14,  0, 10,  4,  4,  0,  0,  5,\n",
      "         4, 14, 14,  6, 12,  0,  0,  4, 13, 10, 14,  9,  5,  1,  0,  0, 14,  4,\n",
      "         9, 14, 10, 14,  0,  4,  4,  2, 14, 15,  0, 14,  4,  2,  0,  8,  4,  0,\n",
      "        14, 16,  4,  4,  3, 14,  0,  4,  4, 10,  1, 14, 16,  0,  1, 14,  6,  0,\n",
      "         4,  6,  4, 10,  4, 16, 14, 14,  4,  4, 14,  6,  4, 14,  4,  8, 14, 14,\n",
      "         4,  4,  0, 12,  0, 14,  0,  3, 14,  4, 17, 13,  4,  1, 14, 16,  4, 16,\n",
      "         1, 10, 14,  4,  6,  3, 14,  0,  4,  0, 14,  4,  4, 10,  6, 14, 14, 14,\n",
      "         5,  0,  8, 15,  6,  4,  4,  6])}\n"
     ]
    }
   ],
   "source": [
    "for batch_index, batch_dict in enumerate(Traindataloader):\n",
    "    print(batch_index)\n",
    "    print(batch_dict)\n",
    "    \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7617df4",
   "metadata": {},
   "source": [
    "### 모델 정의 및 옵티마이저, loss func 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44115fd",
   "metadata": {},
   "source": [
    "### 모델정의 NameClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41b014bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn은 neural network로 torch의 신경망 모듈이다.\n",
    "# MLP사용 \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NameClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim, hidden_dim, output_dim):\n",
    "        \n",
    "#       torch.nn.Module의 초기화 메서드를 실행하여 해당 클래스의 기능을 상속받음\n",
    "        super(NameClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim,hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim,output_dim)\n",
    "    \n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        intermediate = F.relu(self.fc1(x_in))\n",
    "        output = self.fc2(intermediate)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            output = F.softmax(output,dim=1)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34a483e",
   "metadata": {},
   "source": [
    "### 드롭 아웃 적용한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "697099cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn은 neural network로 torch의 신경망 모듈이다.\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NameClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim, hidden_dim, output_dim):\n",
    "        \n",
    "#       torch.nn.Module의 초기화 메서드를 실행하여 해당 클래스의 기능을 상속받음\n",
    "        super(NameClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim,hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim,output_dim)\n",
    "    \n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        intermediate = F.relu(self.fc1(x_in))\n",
    "        output = self.fc2(F.dropout(intermediate,p=0.5))\n",
    "        \n",
    "        if apply_softmax:\n",
    "            output = F.softmax(output,dim=1)\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d16646a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9042"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(name_vocab.token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c92fc2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nation_vocab.token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "136e4705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NameClassifier(\n",
       "  (fc1): Linear(in_features=9042, out_features=300, bias=True)\n",
       "  (fc2): Linear(in_features=300, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim = 300\n",
    "\n",
    "classifier = NameClassifier(input_dim=len(name_vocab.token_to_idx),\n",
    "                            hidden_dim=hidden_dim,\n",
    "                           output_dim=len(nation_vocab.token_to_idx))\n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9930e963",
   "metadata": {},
   "source": [
    "### 옵티마이저, loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e48b2a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fc66d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 옵티마이저\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = lr)\n",
    "optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4022f23c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nationality\n",
       "English       2972\n",
       "Russian       2373\n",
       "Arabic        1603\n",
       "Japanese       775\n",
       "Italian        600\n",
       "German         576\n",
       "Czech          414\n",
       "Spanish        258\n",
       "Dutch          236\n",
       "French         229\n",
       "Chinese        220\n",
       "Irish          183\n",
       "Greek          156\n",
       "Polish         120\n",
       "Korean          77\n",
       "Scottish        75\n",
       "Vietnamese      58\n",
       "Portuguese      55\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['nationality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d93c0eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7293, 0.7839, 0.8540, 0.9294, 0.9454, 0.9475, 0.9623, 0.9765, 0.9785,\n",
       "        0.9791, 0.9800, 0.9833, 0.9858, 0.9891, 0.9930, 0.9932, 0.9947, 0.9950])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numSample_list = df['nationality'].value_counts().tolist()\n",
    "numSample_list\n",
    "# weights 계산\n",
    "weights = [1 - (x / sum(numSample_list)) for x in numSample_list]\n",
    "\n",
    "# weights를 torch.FloatTensor로 변환\n",
    "weights = torch.FloatTensor(weights)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e53071d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss function\n",
    "#  dataset.class_weights -> 각 클래스에 대해 다른 가중치를 적용할 수 있음(데이터 불균형시에)\n",
    "# 소수의 클래스가 다수의 클래스보다 훨씬 적은 수의 샘플을 가지고 있는 경우, \n",
    "# 소수 클래스에 더 높은 가중치를 부여하여 모델이 불균형한 데이터에 대해 더 잘 학습\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(weights)\n",
    "loss_func\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd96977",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "139b0360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "#      예측값과 타겟값을 비교하여 일치하는 개수를 계산\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1bcb9838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train state 초기화 \n",
    "def make_train_state():\n",
    "    return {\n",
    "        'stop_early':False,\n",
    "        'early_stopping_step':0,\n",
    "        'early_stopping_best_val':1e8,\n",
    "        'early_stopping_criteria' : 10,\n",
    "        'epoch_index' : 0,\n",
    "        'train_loss': [], \n",
    "        'train_acc' :[], \n",
    "        'val_loss' : [],\n",
    "        'val_acc' : [], \n",
    "        'test_loss' : [],\n",
    "        'test_acc' : [],\n",
    "         \n",
    "#       모델 저장파일\n",
    "        'model_filename' : 'model.pth'\n",
    "    } \n",
    "\n",
    "\n",
    "# Train update \n",
    "def update_train_state(model, train_state):\n",
    "    \n",
    "#   학습시작하면 초기에 모델 저장하기 \n",
    "    \n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(),train_state['model_filename'])\n",
    "        \n",
    "#   모델 성능이 향상되면 모델 저장(valid loss가 더 낮아지면)\n",
    "    elif train_state['epoch_index'] >=1 :\n",
    "        loss_t = train_state['val_loss'][-1]\n",
    "#        loss가 나빠지면 early stop step 업데이트\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            train_state['early_stopping_step']+=1\n",
    "            \n",
    "#        loss가 좋아지면   \n",
    "        else:\n",
    "#            early stop step 0으로 다시 초기화        \n",
    "            train_state['early_stopping_step']=0\n",
    "    \n",
    "#           최저 loss이면 모델 저장 \n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                train_state['early_stopping_best_val'] = loss_t\n",
    "                torch.save(model.state_dict(),train_state['model_filename'])\n",
    "\n",
    "#       기준점 넘으면 early stop \n",
    "        if train_state['early_stopping_step'] >= train_state['early_stopping_criteria']:\n",
    "            train_state['stop_early'] = True\n",
    "        \n",
    "        return train_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67ec9f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop_early': False,\n",
       " 'early_stopping_step': 0,\n",
       " 'early_stopping_best_val': 100000000.0,\n",
       " 'early_stopping_criteria': 10,\n",
       " 'epoch_index': 0,\n",
       " 'train_loss': [],\n",
       " 'train_acc': [],\n",
       " 'val_loss': [],\n",
       " 'val_acc': [],\n",
       " 'test_loss': [],\n",
       " 'test_acc': [],\n",
       " 'model_filename': 'model.pth'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 진행 상황 함수 초기화\n",
    "train_state = make_train_state()\n",
    "train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fdf42f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                          | 1/100 [00:01<01:47,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.8303771018981934\n",
      "val_acc 37.6953125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▊                                          | 2/100 [00:01<01:35,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.6598594983418784\n",
      "val_acc 40.75520833333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|█▎                                         | 3/100 [00:02<01:31,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.3965635299682617\n",
      "val_acc 30.794270833333332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|█▋                                         | 4/100 [00:03<01:26,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.1826672554016113\n",
      "val_acc 27.669270833333332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|██▏                                        | 5/100 [00:04<01:25,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.10233473777771\n",
      "val_acc 28.645833333333336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|██▌                                        | 6/100 [00:05<01:26,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 2.04198956489563\n",
      "val_acc 41.14583333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|███                                        | 7/100 [00:06<01:22,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 1.9870244264602661\n",
      "val_acc 44.27083333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|███▍                                       | 8/100 [00:07<01:22,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 1.9383618831634521\n",
      "val_acc 43.68489583333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|███▊                                       | 9/100 [00:08<01:20,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 1.8962418635686238\n",
      "val_acc 44.01041666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▏                                     | 10/100 [00:09<01:34,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 1.8595126867294312\n",
      "val_acc 43.48958333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|████▌                                     | 11/100 [00:10<01:34,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 1.8413151105244954\n",
      "val_acc 41.6015625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████                                     | 12/100 [00:11<01:26,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 1.8185444672902424\n",
      "val_acc 42.317708333333336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█████▍                                    | 13/100 [00:12<01:33,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 1.8192741473515828\n",
      "val_acc 43.098958333333336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|█████▉                                    | 14/100 [00:13<01:28,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 1.810646692911784\n",
      "val_acc 42.252604166666664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|██████▎                                   | 15/100 [00:14<01:26,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 1.8178335030873616\n",
      "val_acc 41.927083333333336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|██████▋                                   | 16/100 [00:15<01:20,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 1.82183043162028\n",
      "val_acc 42.96875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████▏                                  | 17/100 [00:16<01:16,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 1.8310314416885376\n",
      "val_acc 42.447916666666664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|███████▌                                  | 18/100 [00:17<01:12,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 1.8286909659703572\n",
      "val_acc 43.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|███████▉                                  | 19/100 [00:18<01:11,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 1.842700759569804\n",
      "val_acc 41.731770833333336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████▍                                 | 20/100 [00:18<01:08,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 1.8429938157399495\n",
      "val_acc 42.96875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|████████▊                                 | 21/100 [00:19<01:07,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 1.8504865169525146\n",
      "val_acc 43.5546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████████▏                                | 22/100 [00:20<01:05,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 1.847023884455363\n",
      "val_acc 43.098958333333336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|█████████▋                                | 23/100 [00:21<01:03,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 1.8597977956136067\n",
      "val_acc 42.447916666666664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|█████████▋                                | 23/100 [00:22<01:13,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss 1.8799113829930623\n",
      "val_acc 40.8203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "# 에포크만큼\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "\n",
    "#     print('epoch',epoch)\n",
    "#     print(train_state['epoch_index']) \n",
    "    train_state['epoch_index'] +=1 \n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "\n",
    "#     모델을 학습 모드로 설정 -> 드롭아웃 및 배치 정규화와 같은 학습 중에만 적용되는 기법들이 활성화\n",
    "#     모델을 평가 모드로 전환하려면 classifier.eval()을 사용\n",
    "    classifier.train()\n",
    "# 배치 만큼\n",
    "    for batch_idx, batch_data in enumerate(Traindataloader):\n",
    "\n",
    "        \n",
    "\n",
    "#       1. 옵티마이저 그레디언트 0으로 초기화\n",
    "        optimizer.zero_grad()\n",
    "#       2. 모델에 데이터 넣어서 출력받기\n",
    "        y_pred = classifier(x_in=batch_data['surname'])\n",
    "#       3. loss 계산하기\n",
    "        loss =  loss_func(y_pred, batch_data['nationality'])\n",
    "    \n",
    "#       tensor(0.3190) -> 0.3190, item()으로 스칼라 값만 추출\n",
    "        loss_t = loss.item()\n",
    "\n",
    "#       배치에서의 평균 loss 구하기\n",
    "        running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "\n",
    "#       4. gradient 계산하기\n",
    "        loss.backward()\n",
    "\n",
    "#       5. 옵티마이저 가중치 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "#       Accuracy 계산\n",
    "        acc_t = compute_accuracy(y_pred, batch_data['nationality'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "\n",
    "\n",
    "\n",
    "    train_state['train_loss'].append(running_loss)\n",
    "    train_state['train_acc'].append(running_acc)\n",
    "\n",
    "\n",
    "#   valid에 대한 계산\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "\n",
    "    classifier.eval() # 모델 파라미터를 수정하지 못 하게 비활성화\n",
    "\n",
    "    for batch_idx, batch_data in enumerate(Validdataloader):\n",
    "\n",
    "#       1. 모델의 출력값(y_pred)계산\n",
    "        y_pred = classifier(x_in=batch_data['surname'])\n",
    "\n",
    "#       2. loss 계산\n",
    "        loss = loss_func(y_pred,batch_data['nationality'])\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "\n",
    "#       3. Accuracy 계산\n",
    "        acc_t = compute_accuracy(y_pred,batch_data['nationality'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "    \n",
    "    print(\"val_loss\",running_loss)\n",
    "    print(\"val_acc\",running_acc)\n",
    "\n",
    "    train_state['val_loss'].append(running_loss)\n",
    "    train_state['val_acc'].append(running_acc)\n",
    "    \n",
    "\n",
    "#   전체 loss, acc 저장\n",
    "    train_state = update_train_state(model=classifier,\n",
    "                                     train_state=train_state)\n",
    "#   early stop해라고 했으면 학습 멈추기    \n",
    "    if train_state['stop_early']:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a456e",
   "metadata": {},
   "source": [
    "### Test 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a68070a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 좋은 모델을 사용해 테스트 세트의 손실과 정확도를 계산합니다\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "running_loss = 0.0\n",
    "running_acc = 0.0\n",
    "\n",
    "# 가중치 업데이트 하지 못 하게\n",
    "classifier.eval()\n",
    "\n",
    "for batch_idx, batch_data in enumerate(Testdataloader):\n",
    "    \n",
    "    y_pred = classifier(x_in=batch_data['surname'])\n",
    "    loss = loss_func(y_pred,batch_data['nationality'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_idx + 1)\n",
    "    \n",
    "    acc_t = compute_accuracy(y_pred, batch_data['nationality'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_idx + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efb9c556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 손실: 1.849\n",
      "테스트 정확도: 42.06\n"
     ]
    }
   ],
   "source": [
    "print(\"테스트 손실: {:.3f}\".format(train_state['test_loss']))\n",
    "print(\"테스트 정확도: {:.2f}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "466e28b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stop_early': True,\n",
       " 'early_stopping_step': 10,\n",
       " 'early_stopping_best_val': 1.810646692911784,\n",
       " 'early_stopping_criteria': 10,\n",
       " 'epoch_index': 24,\n",
       " 'train_loss': [2.8753815809885666,\n",
       "  2.758050107955932,\n",
       "  2.5297334353129064,\n",
       "  2.2607335567474367,\n",
       "  2.0937030315399165,\n",
       "  1.9954595883687338,\n",
       "  1.888177291552226,\n",
       "  1.7716776053110759,\n",
       "  1.6456313212712605,\n",
       "  1.5146377881368,\n",
       "  1.3768490552902222,\n",
       "  1.2391159057617185,\n",
       "  1.100442369778951,\n",
       "  0.9662196358044941,\n",
       "  0.847908357779185,\n",
       "  0.7403143843015035,\n",
       "  0.6467431187629701,\n",
       "  0.5703393757343291,\n",
       "  0.5004418730735779,\n",
       "  0.4429114580154419,\n",
       "  0.39431544542312624,\n",
       "  0.3511805991331736,\n",
       "  0.31422823468844097,\n",
       "  0.2831361959377925],\n",
       " 'train_acc': [15.520833333333334,\n",
       "  39.609375,\n",
       "  41.95312499999999,\n",
       "  37.552083333333336,\n",
       "  32.70833333333333,\n",
       "  33.333333333333336,\n",
       "  46.510416666666664,\n",
       "  51.236979166666664,\n",
       "  58.91927083333333,\n",
       "  62.09635416666667,\n",
       "  63.65885416666667,\n",
       "  69.140625,\n",
       "  77.89062499999999,\n",
       "  82.17447916666667,\n",
       "  84.8828125,\n",
       "  87.90364583333333,\n",
       "  90.70312500000001,\n",
       "  92.55208333333334,\n",
       "  94.34895833333333,\n",
       "  94.93489583333334,\n",
       "  95.33854166666666,\n",
       "  95.71614583333333,\n",
       "  95.78125,\n",
       "  95.98958333333333],\n",
       " 'val_loss': [2.8303771018981934,\n",
       "  2.6598594983418784,\n",
       "  2.3965635299682617,\n",
       "  2.1826672554016113,\n",
       "  2.10233473777771,\n",
       "  2.04198956489563,\n",
       "  1.9870244264602661,\n",
       "  1.9383618831634521,\n",
       "  1.8962418635686238,\n",
       "  1.8595126867294312,\n",
       "  1.8413151105244954,\n",
       "  1.8185444672902424,\n",
       "  1.8192741473515828,\n",
       "  1.810646692911784,\n",
       "  1.8178335030873616,\n",
       "  1.82183043162028,\n",
       "  1.8310314416885376,\n",
       "  1.8286909659703572,\n",
       "  1.842700759569804,\n",
       "  1.8429938157399495,\n",
       "  1.8504865169525146,\n",
       "  1.847023884455363,\n",
       "  1.8597977956136067,\n",
       "  1.8799113829930623],\n",
       " 'val_acc': [37.6953125,\n",
       "  40.75520833333333,\n",
       "  30.794270833333332,\n",
       "  27.669270833333332,\n",
       "  28.645833333333336,\n",
       "  41.14583333333333,\n",
       "  44.27083333333333,\n",
       "  43.68489583333333,\n",
       "  44.01041666666667,\n",
       "  43.48958333333333,\n",
       "  41.6015625,\n",
       "  42.317708333333336,\n",
       "  43.098958333333336,\n",
       "  42.252604166666664,\n",
       "  41.927083333333336,\n",
       "  42.96875,\n",
       "  42.447916666666664,\n",
       "  43.75,\n",
       "  41.731770833333336,\n",
       "  42.96875,\n",
       "  43.5546875,\n",
       "  43.098958333333336,\n",
       "  42.447916666666664,\n",
       "  40.8203125],\n",
       " 'test_loss': 1.8494670788447063,\n",
       " 'test_acc': 42.057291666666664,\n",
       " 'model_filename': 'model.pth'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe334c42",
   "metadata": {},
   "source": [
    "### 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "539d9584",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_surname = \"Kim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f7fe58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 벡터화 + 텐서화\n",
    "\n",
    "vectorized_surname = torch.tensor(vectorize(name_vocab,new_surname))\n",
    "vectorized_surname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34991997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9042])\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_surname.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37857417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9042])\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 차원을 1로 만들고, 나머지는 다른 차원으로 알아서 되도록 !\n",
    "x_data = vectorized_surname.view(1, -1)\n",
    "print(x_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8c7e4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0216, 0.0386, 0.0752, 0.0630, 0.0868, 0.0655, 0.1066, 0.0363, 0.0484,\n",
      "         0.0977, 0.1037, 0.0252, 0.0261, 0.0165, 0.0873, 0.0238, 0.0595, 0.0183]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 모델에 test 데이터 넣어주기\n",
    "result = classifier(x_data,apply_softmax=True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd6ed5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1066, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "82ccf84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability_value tensor([0.1066], grad_fn=<MaxBackward0>)\n",
      "indices tensor([6])\n"
     ]
    }
   ],
   "source": [
    "probability_value, indices = result.max(dim=1)\n",
    "\n",
    "print(\"probability_value\",probability_value)\n",
    "print(\"indices\",indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d59fd4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새로운 데이터에 대해 추론한 label 6\n"
     ]
    }
   ],
   "source": [
    "print(\"새로운 데이터에 대해 추론한 label\",indices.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64eabc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'German'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = lookup_index(nation_vocab,indices.item())\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "54ba6aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kim -> German\n"
     ]
    }
   ],
   "source": [
    "print(\"{} -> {}\".format(new_surname, prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
